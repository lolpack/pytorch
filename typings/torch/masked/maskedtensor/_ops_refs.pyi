"""
This type stub file was generated by pyright.
"""

import torch
from typing import Any, Callable, TYPE_CHECKING
from torch._ops import OpOverload

if TYPE_CHECKING:
    ...
__all__ = []
class _MaskedContiguous(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input): # -> MaskedTensor:
        ...
    
    @staticmethod
    def backward(ctx, grad_output):
        ...
    


class _MaskedToDense(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input): # -> MaskedTensor:
        ...
    
    @staticmethod
    def backward(ctx, grad_output):
        ...
    


class _MaskedToSparse(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input): # -> MaskedTensor:
        ...
    
    @staticmethod
    def backward(ctx, grad_output):
        ...
    


class _MaskedToSparseCsr(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input): # -> MaskedTensor:
        ...
    
    @staticmethod
    def backward(ctx, grad_output):
        ...
    


class _MaskedWhere(torch.autograd.Function):
    @staticmethod
    def forward(ctx, cond, self, other): # -> Any:
        ...
    
    @staticmethod
    def backward(ctx, grad_output): # -> tuple[None, Any, Any]:
        ...
    


_MASKEDTENSOR_FUNCTION_TABLE = ...
_function_fn_apply_map = ...
def register_function_func(ops): # -> Callable[..., None]:
    """
    Used for registering a new __torch_function__ function to MaskedTensor
    Called via _MASKEDTENSOR_FUNCTION_TABLE[func](*args, **kwargs)

    The code to register a new function looks like:

    @register_function_func(list_of_ops)
    def foo(func, *args, **kwargs):
        <implementation>
    """
    ...

_MASKEDTENSOR_DISPATCH_TABLE: dict[OpOverload, Callable[..., Any]] = ...
def register_dispatch_func(aten_ops): # -> Callable[..., None]:
    """
    Used for registering a new __torch_dispatch__ function to MaskedTensor
    Called via _MASKEDTENSOR_DISPATCH_TABLE[func](*args, **kwargs)

    The code to register a new function looks like:

    @register_dispatch_func(list_of_ops)
    def foo(func, *args, **kwargs):
        <implementation>
    """
    ...

@register_dispatch_func([torch.ops.aten.stride])
def stride(func, *args, **kwargs): # -> None:
    ...

@register_dispatch_func([torch.ops.aten.sym_stride])
def sym_stride(func, *args, **kwargs): # -> None:
    ...

@register_dispatch_func([torch.ops.prim.layout])
def layout(func, *args, **kwargs):
    ...

@register_dispatch_func([torch.ops.aten.is_contiguous])
def is_contiguous(func, *args, **kwargs):
    ...

@register_dispatch_func([torch.ops.aten.is_strides_like_format])
def is_strides_like_format(func, *args, **kwargs):
    ...

@register_dispatch_func([torch.ops.aten.is_non_overlapping_and_dense])
def is_non_overlapping_and_dense(func, *args, **kwargs):
    ...

@register_dispatch_func([torch.ops.aten.contiguous])
def contiguous(func, *args, **kwargs): # -> Any | None:
    ...

@register_dispatch_func([torch.ops.aten.new_empty_strided])
def new_empty_strided(func, *args, **kwargs): # -> MaskedTensor:
    ...

@register_dispatch_func([torch.ops.aten.ones_like])
def ones_like(func, *args, **kwargs): # -> MaskedTensor:
    ...

@register_dispatch_func([torch.ops.aten.copy_])
def copy_(func, *args, **kwargs):
    ...

@register_dispatch_func([torch.ops.aten.where])
def where(func, *args, **kwargs): # -> MaskedTensor:
    ...

@register_dispatch_func([torch.ops.aten.is_same_size])
def is_same_size(func, *args, **kwargs):
    ...

