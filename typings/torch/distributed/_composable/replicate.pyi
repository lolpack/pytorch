"""
This type stub file was generated by pyright.
"""

import weakref
import torch
import torch.nn as nn
from collections.abc import Iterable
from typing import Any, NoReturn, Optional
from torch.distributed._composable_state import _State
from .contract import contract

_ROOT_MODULE_PREFIX = ...
class _ReplicateState(_State):
    _ddp_weakref: weakref.ref
    def __init__(self) -> None:
        ...
    
    def lazy_init(self) -> None:
        ...
    
    def init(self, module: nn.Module, ignored_modules: set[nn.Module], **kwargs) -> None:
        ...
    
    def register_comm_hook(self) -> None:
        ...
    
    def record_init_args(self, *args, **kwargs) -> None:
        ...
    
    def forward_pre_hook(self, module: nn.Module, args: tuple[Any, ...], kwargs: dict[str, Any]) -> Any:
        ...
    
    def forward_post_hook(self, module: nn.Module, input: tuple[torch.Tensor], output: torch.Tensor) -> torch.Tensor:
        ...
    


def unimplemented_deepcopy(*args: Any, **kwargs: Any) -> NoReturn:
    ...

class DDP:
    def __new__(cls, *args, **kwargs):
        """
        Override ``__new__`` to remove the DDP class and directly construct
        the original class for cases like indexing into a container module.
        """
        ...
    
    def set_requires_gradient_sync(self, requires_gradient_sync: bool) -> None:
        """
        Sets if the module should sync gradients. This can be used to implement
        gradient accumulation without communication.

        Args:
            requires_gradient_sync (bool): Whether to reduce gradients for the
                module's parameters.
        """
        ...
    
    def register_comm_hook(self, *args, **kwargs) -> None:
        ...
    


@contract(state_cls=_ReplicateState)
def replicate(module: nn.Module, ignored_modules: Optional[Iterable[torch.nn.Module]] = ..., **kwargs) -> nn.Module:
    r"""Replicates a module

    Args:
        module (torch.nn.Module): module to replicate

    Example::
        >>> # xdoctest: +REQUIRES(module:torch._C._distributed_c10d)
        >>> module = nn.Linear(3, 3)
        >>> replicate(module)
    """
    ...

