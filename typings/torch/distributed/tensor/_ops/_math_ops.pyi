"""
This type stub file was generated by pyright.
"""

from dataclasses import dataclass
from enum import Enum
from typing import Union
from torch.distributed.tensor._op_schema import OpSchema, OpStrategy, RuntimeSchemaInfo, TupleStrategy
from torch.distributed.tensor._ops.utils import register_op_strategy
from torch.distributed.tensor.placement_types import Partial, Placement

aten = ...
class Reduction(Enum):
    NONE = ...
    MEAN = ...
    SUM = ...


@dataclass(frozen=True)
class NormReduction:
    norm_type: Union[int, float, str]
    ...


ReductionOpType = Union[NormReduction, str]
@dataclass(frozen=True)
class _NormPartial(Partial):
    """
    This placement is used for partial vector norm.

    For p-norms (where p not inf or -inf), the p-norm over n elements computes
        (sum_i x_i^p)^(1/p)
    where the sum is from i=1 to n. The reduction op is the p-norm itself.
    For example, consider 2 ranks, a (4,) tensor sharded on dim-0, and 2-norm:
        Rank 0: [t1, t2] | Rank 1: [t3, t4]
    After computing 2-norm per gradient (partial placement):
        Rank 0: [sqrt(t1^2 + t2^2)] | Rank 1: [sqrt(t3^2 + t4^2)]
    Converting from partial to replicate wants to ultimately get:
        Rank 0/1: [sqrt(t1^2 + t2^2 + t3^2 + t4^2)]
    This can be achieved by computing 2-norm on each rank's result. This holds
    similarly for inf and -inf norm. For 0-norm, the reduction op is sum.
    """
    norm_type: Union[int, float, str] = ...
    def __post_init__(self): # -> None:
        """Set the appropriate reduce op based on the norm type."""
        ...
    
    def __eq__(self, other: object) -> bool:
        ...
    
    def __hash__(self) -> int:
        ...
    


def replicate_reduction_dims(placements: tuple[Placement, ...], reduction_dims: list[int]) -> tuple[Placement, ...]:
    ...

def map_placements_after_reduction(placements: tuple[Placement, ...], reduction_dims: list[int], reduction_dims_map: list[int], reduction_op: ReductionOpType) -> tuple[Placement, ...]:
    """
    Map each placement based on the output shape after reduction.
    """
    ...

def get_placement_from_reduction_op(reduction_op: ReductionOpType) -> Placement:
    ...

def common_reduction_strategy(input_strategy: OpStrategy, reduce_dims: list[int], keep_dim: bool = ..., reduction_linear: bool = ..., reduction_op: ReductionOpType = ...) -> OpStrategy:
    """
    reduction_linear means that the reduction `f` follows this rule:
        f([f(a), f(b)]) = f([a, b])

    reduction linear should be super set of linearity.
    """
    ...

LINEAR_REDUCTION_OP_MAP = ...
@register_op_strategy(list(LINEAR_REDUCTION_OP_MAP.keys()), schema_info=RuntimeSchemaInfo(1))
def linear_reduction_strategy(op_schema: OpSchema) -> OpStrategy:
    ...

@register_op_strategy(aten.cumsum.default, schema_info=RuntimeSchemaInfo(1))
def cumsum_strategy(op_schema: OpSchema) -> OpStrategy:
    ...

@register_op_strategy([aten.var.correction, aten.var.correction_out], schema_info=RuntimeSchemaInfo(1, ["keepdim"]))
def var_reduction_strategy(op_schema: OpSchema) -> OpStrategy:
    ...

@register_op_strategy([aten.linalg_vector_norm.default], schema_info=RuntimeSchemaInfo(1))
def vector_norm_strategy(op_schema: OpSchema) -> OpStrategy:
    ...

@register_op_strategy([aten._foreach_norm.Scalar], schema_info=RuntimeSchemaInfo(1, needs_pytree=True))
def foreach_norm_strategy(op_schema: OpSchema) -> TupleStrategy:
    ...

@register_op_strategy([aten._linalg_svd.default, aten.linalg_qr.default, aten.diagonal_copy.default, aten.diag_embed.default, aten.diag.default, aten.diagonal.default, aten.tril.default, aten.triu.default, aten._linalg_eigh.default, aten.upsample_bicubic2d.default, aten.upsample_bilinear2d.default, aten.upsample_linear1d.default, aten.upsample_nearest2d.default, aten.upsample_trilinear3d.default], schema_info=RuntimeSchemaInfo(1))
def linalg_replicate_strategy(op_schema: OpSchema) -> OpStrategy:
    """
    Since we do not have a simple way to compute some linear algebra operations
    like SVD or QR decomposition, always fall back to replicate.
    """
    ...

@register_op_strategy([aten._log_softmax.default, aten._softmax.default, aten._safe_softmax.default], schema_info=RuntimeSchemaInfo(1))
def softmax_strategy(op_schema: OpSchema) -> OpStrategy:
    ...

@register_op_strategy([aten._log_softmax_backward_data.default, aten._softmax_backward_data.default], schema_info=RuntimeSchemaInfo(2))
def softmax_backward_strategy(op_schema: OpSchema) -> OpStrategy:
    ...

@register_op_strategy([aten.nll_loss_forward.default, aten.nll_loss2d_forward.default], schema_info=RuntimeSchemaInfo(3))
def nll_loss_forward_strategy(op_schema: OpSchema) -> OpStrategy:
    ...

@register_op_strategy([aten.nll_loss_backward.default, aten.nll_loss2d_backward.default], schema_info=RuntimeSchemaInfo(4))
def nll_loss_backward_strategy(op_schema: OpSchema) -> OpStrategy:
    ...

@register_op_strategy([aten.native_layer_norm.default], schema_info=RuntimeSchemaInfo(1))
def layer_norm_strategy(op_schema: OpSchema) -> OpStrategy:
    ...

@register_op_strategy([aten.native_layer_norm_backward.default], schema_info=RuntimeSchemaInfo(2))
def layer_norm_bwd_strategy(op_schema: OpSchema) -> OpStrategy:
    ...

@register_op_strategy([aten.topk.default], schema_info=RuntimeSchemaInfo(2))
def topk_strategy(op_schema: OpSchema) -> OpStrategy:
    ...

