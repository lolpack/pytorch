"""
This type stub file was generated by pyright.
"""

import torch
from typing import Optional
from torch import Tensor
from torch.distributed.device_mesh import DeviceMesh

__all__ = ["is_rng_supported_mesh", "manual_seed", "OffsetBasedRNGTracker"]
_rng_tracker: Optional[_RNGStateTracker] = ...
def is_rng_supported_mesh(device_mesh: DeviceMesh) -> bool:
    """Checks if the current device of ``device_mesh`` supports DTensor's random APIs.
    Currently DTensor Random APIs only supports cuda/cuda-like devices. We suggest
    users call this API to test the availability before using our random APIs.

    Args:
        device_mesh (:class:`DeviceMesh`): The device mesh on which we check if the
            random ops APIs are supported.

    Returns:
        A bool value. True if ``device_mesh`` supports DTensor Random APIs; False otherwise.

    .. warning::
        Currently we only support correct RNG on cuda/cuda-like devices.
    """
    ...

def manual_seed(seed: int, device_mesh: DeviceMesh) -> None:
    """Sets the seed for generating random numbers for the calling rank.

    Args:
        seed (int): The desired seed.
        device_mesh (:class:`DeviceMesh`): The device mesh to set the seed. It is
            required that the ``device_mesh`` include the calling rank. This is
            to ensure that the SPMD region maintains a synchronous RNG state, which
            means no ranks should be initialized with values other than ``seed``.

    Returns:
        None

    .. warning::
        :func:`manual_seed` does not check the ``seed`` value correctness. Users must
        ensure on their own that the value passed in is the desired ``seed`` for ranks
        within ``device_mesh``.
        If ``device_mesh`` is a sub-mesh and the calling rank is not a part of it,
        ``manual_seed`` will throw an error.
        Current implementation only supports a GPU device mesh.
    """
    ...

class _RNGStateTracker:
    """
    _RNGStateTracker stores Random Number Generator (RNG) state (a ByteTensor object)
    in a dict, mapping from a corresponding tag to each state tensor. It also provides
    a set of convenient utility methods to help access/modify the state tensors. The most
    important interface is _distribute_region which will be used when DTensor executes
    a random op (an operator that calls RNG).
    """
    def __init__(self, device: torch.device) -> None:
        ...
    
    @property
    def rng_states(self) -> dict[str, Tensor]:
        ...
    
    @property
    def distribute_region_enabled(self) -> bool:
        ...
    
    @distribute_region_enabled.setter
    def distribute_region_enabled(self, value) -> None:
        ...
    
    def rng_state_is_sync(self, name) -> bool:
        ...
    
    def get_seed(self, name: str) -> int:
        ...
    
    def set_seed(self, name: str, seed: int) -> None:
        ...
    


class OffsetBasedRNGTracker(_RNGStateTracker):
    """
    This subclass of ``_RNGStateTracker`` defines the default policy of how RNG states
    should be shared and synchronized among all ranks to respect the semantics of DTensor
    random operators.

    note: _RNGStateTracker only supports cuda/cuda-like device.
    """
    def __init__(self, device_mesh: DeviceMesh, run_state_sync: bool = ...) -> None:
        ...
    
    def get_offset(self, name: str) -> int:
        ...
    
    def set_offset(self, name: str, offset: int) -> None:
        ...
    


