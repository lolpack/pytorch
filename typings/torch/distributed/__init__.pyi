"""
This type stub file was generated by pyright.
"""

import logging
import pdb
import sys
import traceback
import typing
import torch
from torch._C._distributed_c10d import Backend as _Backend, BuiltinCommHookType, DebugLevel, FileStore, GradBucket, Logger, PrefixStore, ProcessGroup as ProcessGroup, Reducer, Store, TCPStore, Work as _Work, _ControlCollectives, _DEFAULT_FIRST_BUCKET_BYTES, _StoreCollectives, _broadcast_coalesced, _compute_bucket_assignment_by_size, _make_nccl_premul_sum, _register_builtin_comm_hook, _register_comm_hook, _test_python_store, _verify_params_across_processes, get_debug_level, set_debug_level, set_debug_level_from_env
from .device_mesh import DeviceMesh, init_device_mesh
from .distributed_c10d import *
from .distributed_c10d import _CoalescingManager, _all_gather_base, _coalescing_manager, _create_process_group_wrapper, _get_process_group_name, _rank_not_in_group, _reduce_scatter_base, _time_estimator, get_node_local_rank
from .remote_device import _remote_device
from .rendezvous import _create_store_from_options, register_rendezvous_handler, rendezvous

log = ...
def is_available() -> bool:
    """
    Return ``True`` if the distributed package is available.

    Otherwise,
    ``torch.distributed`` does not expose any other APIs. Currently,
    ``torch.distributed`` is available on Linux, MacOS and Windows. Set
    ``USE_DISTRIBUTED=1`` to enable it when building PyTorch from source.
    Currently, the default value is ``USE_DISTRIBUTED=1`` for Linux and Windows,
    ``USE_DISTRIBUTED=0`` for MacOS.
    """
    ...

if is_available() and not torch._C._c10d_init():
    ...
DistError = torch._C._DistError
DistBackendError = torch._C._DistBackendError
DistNetworkError = torch._C._DistNetworkError
DistStoreError = torch._C._DistStoreError
QueueEmptyError = torch._C._DistQueueEmptyError
if is_available():
    class _DistributedPdb(pdb.Pdb):
        """
        Supports using PDB from inside a multiprocessing child process.

        Usage:
        _DistributedPdb().set_trace()
        """
        def interaction(self, *args, **kwargs): # -> None:
            ...
        
    
    
    _breakpoint_cache: dict[int, typing.Any] = ...
    def breakpoint(rank: int = ..., skip: int = ...): # -> None:
        """
        Set a breakpoint, but only on a single rank.  All other ranks will wait for you to be
        done with the breakpoint before continuing.

        Args:
            rank (int): Which rank to break on.  Default: ``0``
            skip (int): Skip the first ``skip`` calls to this breakpoint. Default: ``0``.
        """
        ...
    
else:
    class _ProcessGroupStub:
        ...
    
    
