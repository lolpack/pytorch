"""
This type stub file was generated by pyright.
"""

import contextlib
import torch
import torch.distributed as dist
from typing import Any, Optional, Union
from torch.distributed.device_mesh import DeviceMesh

if torch._running_with_deploy():
    def is_torchdynamo_compiling(): # -> Literal[False]:
        """Can't import torchdynamo in torchdeploy builds currently."""
        ...
    
else:
    ...
RANK_TYPES = Union[list[int], list[list[int]], dist.ProcessGroup, DeviceMesh, tuple["dist.tensor.DeviceMesh", int], str,]
def wait_tensor(tensor): # -> Any:
    """
    Wait on a tensor returned by the collectives ops.

    Waiting follows device semantics, which means blocking on CPU and synchronizing streams on CUDA.
    """
    ...

def broadcast(self: torch.Tensor, src: int, group: RANK_TYPES, tag: str = ...): # -> Tensor:
    """
    Broadcasts the tensor to all processes in the given process group.

    Args:
        src (int): Source rank
        group (ProcessGroup or List[int]): The process group to work on.
        tag (str, optional): A unique identifier for the collective. Default: empty string
    """
    ...

def all_reduce(self: torch.Tensor, reduceOp: str, group: RANK_TYPES, tag: str = ...): # -> Tensor:
    """
    Reduces the tensor data across all machines in such a way that all get
    the final result.

    The input tensor is left unmodified.

    Group can be one of:
        List[int]: ranks participating in the collective.
        List[List[int]]: 2D mesh of ranks taking part of this collective in MPMD.
        ProcessGroup: Will perform a collective using the ranks and tag of the PG.
        DeviceMesh: Do a SPMD collective over all ranks of the mesh
        (DeviceMesh, int): Do a MPMD collective over one dimension of the DeviceMesh

    :: N.B. If you pass a PG or a 1D list to perform a MPMD collective, the compiler won't be able to recover
    that information and perform collective algebraic optimization. Use other forms of input for that.
    """
    ...

def all_gather_tensor(self: torch.Tensor, gather_dim: int, group: RANK_TYPES, tag: str = ...) -> torch.Tensor:
    """
    Gather tensor data across from all machines and concatenate over ``gather_dim``.

    Note that it currently only supports gather_dim = 0.

    The input tensor is left unmodified.
    Group can be one of:
        List[int]: ranks participating in the collective.
        List[List[int]]: 2D mesh of ranks taking part of this collective in MPMD.
        ProcessGroup: Will perform a collective using the ranks and tag of the PG.
        DeviceMesh: Do a SPMD collective over all ranks of the mesh
        (DeviceMesh, int): Do a MPMD collective over one dimension of the DeviceMesh

    :: N.B. If you pass a PG or a 1D list to perform a MPMD collective, the compiler won't be able to recover
    that information and perform collective algebraic optimization. Use other forms of input for that.
    """
    ...

def all_gather_tensor_autograd(self: torch.Tensor, gather_dim: int, group: RANK_TYPES, tag: str = ...): # -> Tensor | Any | None:
    """
    Gather tensor data across from all machines and concatenate over ``gather_dim``.

    Note that it currently only supports gather_dim = 0.

    This function is the same as all_gather_tensor but will propagate the
    backwards gradient across workers.

    See all_gather_tensor for more details on usage.
    """
    ...

def reduce_scatter_tensor(self: torch.Tensor, reduceOp: str, scatter_dim: int, group: RANK_TYPES, tag: str = ...): # -> Tensor:
    """
    Reduces the tensor data across all machines in such a way that all get
    the final result, then scatter the results to corresponding ranks.


    The input tensor is left unmodified.
    Group can be one of:
        List[int]: ranks participating in the collective.
        List[List[int]]: 2D mesh of ranks taking part of this collective in MPMD.
        ProcessGroup: Will perform a collective using the ranks and tag of the PG.
        DeviceMesh: Do a SPMD collective over all ranks of the mesh
        (DeviceMesh, int): Do a MPMD collective over one dimension of the DeviceMesh
    :: N.B. If you pass a PG or a 1D list to perform a MPMD collective, the compiler won't be able to recover
    that information and perform collective algebraic optimization. Use other forms of input for that.
    """
    ...

def reduce_scatter_tensor_autograd(self: torch.Tensor, reduceOp: str, scatter_dim: int, group: RANK_TYPES, tag: str = ...): # -> Any | None:
    """
    Reduces the tensor data across all machines in such a way that all get
    the final result, then scatter the results to corresponding ranks.

    This function is the same as reduce_scatter_tensor but will propagate the
    backwards gradient across workers.

    Currently only the "sum" reduceOp is supported.

    See reduce_scatter_tensor for more details on usage.
    """
    ...

def all_reduce_coalesced(self: list[torch.Tensor], reduceOp: str, group: RANK_TYPES, tag: str = ...) -> list[torch.Tensor]:
    """
    Reduces a list of tensors across all machines in such a way that all get
    the final result.

    The all tensors in the input list are left unmodified.

    Group can be one of:
        List[int]: ranks participating in the collective.
        List[List[int]]: 2D mesh of ranks taking part of this collective in MPMD.
        ProcessGroup: Will perform a collective using the ranks and tag of the PG.
        DeviceMesh: Do a SPMD collective over all ranks of the mesh
        (DeviceMesh, int): Do a MPMD collective over one dimension of the DeviceMesh

    :: N.B. If you pass a PG or a 1D list to perform a MPMD collective, the compiler won't be able to recover
    that information and perform collective algebraic optimization. Use other forms of input for that.
    """
    ...

def all_gather_into_tensor_coalesced(self: list[torch.Tensor], group: RANK_TYPES, tag: str = ...) -> list[torch.Tensor]:
    """
    Gather a list of tensors across from all machines.

    Note that it currently only supports gather_dim = 0.

    The input tensor is left unmodified.
    Group can be one of:
        List[int]: ranks participating in the collective.
        List[List[int]]: 2D mesh of ranks taking part of this collective in MPMD.
        ProcessGroup: Will perform a collective using the ranks and tag of the PG.
        DeviceMesh: Do a SPMD collective over all ranks of the mesh
        (DeviceMesh, int): Do a MPMD collective over one dimension of the DeviceMesh

    :: N.B. If you pass a PG or a 1D list to perform a MPMD collective, the compiler won't be able to recover
    that information and perform collective algebraic optimization. Use other forms of input for that.
    """
    ...

def reduce_scatter_tensor_coalesced(inputs: list[torch.Tensor], reduceOp: str, scatter_dim: list[int], group: RANK_TYPES, tag: str = ...) -> list[torch.Tensor]:
    """
    Reduces a list of tensors across all machines in such a way that all get
    the final result, then scatter the results to corresponding ranks.

    The input tensors are left unmodified.
    Group can be one of:
        List[int]: ranks participating in the collective.
        List[List[int]]: 2D mesh of ranks taking part of this collective in MPMD.
        ProcessGroup: Will perform a collective using the ranks and tag of the PG.
        DeviceMesh: Do a SPMD collective over all ranks of the mesh
        (DeviceMesh, int): Do a MPMD collective over one dimension of the DeviceMesh

    :: N.B. If you pass a PG or a 1D list to perform a MPMD collective, the compiler won't be able to recover
    that information and perform collective algebraic optimization. Use other forms of input for that.
    """
    ...

def all_to_all_single(self: torch.Tensor, output_split_sizes: Optional[list[int]], input_split_sizes: Optional[list[int]], group: RANK_TYPES, tag: str = ...) -> torch.Tensor:
    """
    Each process splits input tensor and then scatters the split list
    to all processes in a group. Then concatenate the received tensors from all
    the processes in the group and return single output tensor.

    Group can be one of:
        List[int]: ranks participating in the collective.
        List[List[int]]: 2D mesh of ranks taking part of this collective in MPMD.
        ProcessGroup: Will perform a collective using the ranks and tag of the PG.
        DeviceMesh: Do a SPMD collective over all ranks of the mesh
        (DeviceMesh, int): Do a MPMD collective over one dimension of the DeviceMesh

    :: N.B. If you pass a PG or a 1D list to perform a MPMD collective, the compiler won't be able to recover
    that information and perform collective algebraic optimization. Use other forms of input for that.
    """
    ...

def all_to_all_single_autograd(self: torch.Tensor, output_split_sizes: Optional[list[int]], input_split_sizes: Optional[list[int]], group: RANK_TYPES, tag: str = ...) -> torch.Tensor:
    """
    Same as all_to_all_single but supports autograd.
    """
    ...

def permute_tensor(self: torch.Tensor, src_dst: list[int], group: RANK_TYPES, tag: str = ...) -> torch.Tensor:
    """
    Permutes the elements of the tensor according to the given source/destination pairs. `src_dst` should
    be defined such that src_dst[m] == n means m sends to n.

    Group can be one of:
        List[int]: ranks participating in the collective.
        List[List[int]]: 2D mesh of ranks taking part of this collective in MPMD.
        ProcessGroup: Will perform a collective using the ranks and tag of the PG.
        DeviceMesh: Do a SPMD collective over all ranks of the mesh
        (DeviceMesh, int): Do a MPMD collective over one
    """
    ...

class AsyncCollectiveTensor(torch.Tensor):
    r"""
    A Tensor wrapper subclass that is used to trigger a call to wait
    prior to first use of the underlying tensor.
    Use it inside functional collective pytorch wrappers like the following:
    def functional_collective(self, group, tag):
        tag, rankset, group_size = _expand_group(group, tag)
        tensor = torch.ops.c10d_functional.{collective}(self, tag, rankset, group_size)
        return _maybe_wrap_tensor(tensor)
    """
    elem: torch.Tensor
    completed: bool
    __slots__ = ...
    @staticmethod
    def __new__(cls, elem: torch.Tensor):
        ...
    
    def __tensor_flatten__(self): # -> tuple[list[str], None]:
        ...
    
    def tolist(self): # -> Any:
        ...
    
    @staticmethod
    def __tensor_unflatten__(inner_tensors, meta, outer_size, outer_stride): # -> AsyncCollectiveTensor:
        ...
    
    def __coerce_same_metadata_as_tangent__(self, expected_metadata: Any, expected_type: Optional[type] = ...): # -> Any | Tensor | None:
        ...
    
    def __repr__(self) -> str:
        ...
    
    def trigger_wait(self): # -> Any | Tensor:
        ...
    
    def wait(self) -> torch.Tensor:
        ...
    
    @classmethod
    def __torch_dispatch__(cls, func, types, args=..., kwargs=...): # -> AsyncCollectiveTensor | PyTree:
        ...
    
    def numpy(self):
        ...
    


class _FromTorchTensor(torch.autograd.Function):
    """
    _FromTorchTensor allows autograd to propagate from a normal Tensor to an
    AsyncCollectiveTensor.
    """
    @staticmethod
    def forward(ctx, input: torch.Tensor) -> torch.Tensor:
        ...
    
    @staticmethod
    def backward(ctx, grad_output: torch.Tensor) -> torch.Tensor:
        ...
    


@contextlib.contextmanager
def allow_inflight_collective_as_graph_input_ctx(value: bool = ...): # -> Generator[None, Any, None]:
    """
    Context manager to temporarily set whether inflight collectives are allowed as torch.compile graph inputs.
    Common use case is when the collective is issued in eager (with `async_op=True`) but waited in compiled region:
    ```
    def all_reduce_eager(x):
        y = x * x
        req = dist.all_reduce(y, op=dist.ReduceOp.SUM, async_op=True)
        return y


    @torch.compile(fullgraph=True)
    def all_reduce_wait_compiled(y):
        torch.ops.c10d_functional.wait_tensor(y)
        return y * y


    x = torch.ones(1280, 1280, device="cuda") + self.rank
    # the context manager ensures that `wait_tensor(y)` will wait on the correct work object
    with allow_inflight_collective_as_graph_input_ctx():
        y = all_reduce_eager(x)
        z = all_reduce_wait_compiled(y)
    ```
    With this context manager, when a collective is called, under the hood the work object of the collective
    will be registered in the work registry, and the wait_tensor() in compiled region called on
    the output tensor of the collective will wait on the correct work object.
    """
    ...

if not torch._running_with_deploy():
    lib_impl = ...
    legacy_lib = ...
    legacy_lib_impl = ...
    ops_defs = ...
    my_module = ...
else:
    ...
def all_gather_tensor_inplace(output_tensor: torch.Tensor, input_tensor: torch.Tensor, group=..., async_op: bool = ..., tag: str = ..., gather_dim: int = ...):
    ...

def reduce_scatter_tensor_inplace(output: torch.Tensor, input: torch.Tensor, op: str = ..., group=..., async_op: bool = ..., scatter_dim: int = ..., tag: str = ...):
    ...

REDUCE_OP_TO_STR = ...
def all_reduce_inplace(tensor: torch.Tensor, op: str = ..., group=..., async_op: bool = ..., tag: str = ...):
    ...

def all_to_all_inplace(output: torch.Tensor, input: torch.Tensor, output_split_sizes=..., input_split_sizes=..., group=..., async_op=..., tag: str = ...):
    ...

def all_gather_inplace(tensor_list: list[torch.Tensor], tensor: torch.Tensor, group=..., async_op=..., tag: str = ...): # -> list[Tensor]:
    ...

traceable_collective_remaps = ...
