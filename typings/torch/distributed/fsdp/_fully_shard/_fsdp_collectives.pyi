"""
This type stub file was generated by pyright.
"""

import torch
import torch.distributed as dist
from typing import Callable, NamedTuple, Optional
from ._fsdp_param import FSDPParam

class AllGatherResult(NamedTuple):
    all_gather_output: torch.Tensor
    all_gather_event: Optional[torch.Event]
    all_gather_work: Optional[dist.distributed_c10d.Work]
    param_all_gather_input_dtypes: list[list[torch.dtype]]
    param_all_gather_input_numels: list[list[int]]
    all_gather_input_split_sizes: list[int]
    ...


def allocate_memory(size: int, dtype: torch.dtype, device: torch.device, group: dist.ProcessGroup, from_process_group: bool) -> torch.Tensor:
    ...

lib = ...
@torch.library.impl(lib, "all_gather_copy_in", "Meta")
def all_gather_copy_in_meta(all_gather_inputs: list[torch.Tensor], inp_split_sizes: list[int], all_gather_input_numel: int, world_size: int, rank: int, dtype: torch.dtype, device: torch.device, group_name: str, allocate_memory_from_process_group: bool) -> tuple[torch.Tensor, torch.Tensor]:
    ...

@torch.library.impl(lib, "all_gather_copy_in", "CUDA")
@torch.library.impl(lib, "all_gather_copy_in", "XPU")
@torch.library.impl(lib, "all_gather_copy_in", "HPU")
@torch.library.impl(lib, "all_gather_copy_in", "CPU")
@torch.library.impl(lib, "all_gather_copy_in", "MTIA")
@torch.library.impl(lib, "all_gather_copy_in", "PrivateUse1")
def all_gather_copy_in_cuda(all_gather_inputs: list[torch.Tensor], inp_split_sizes: list[int], all_gather_input_numel: int, world_size: int, rank: int, dtype: torch.dtype, device: torch.device, group_name: str, allocate_memory_from_process_group: bool) -> tuple[torch.Tensor, torch.Tensor]:
    ...

@torch.library.impl(lib, "split_with_sizes_copy", "Meta")
@torch.library.impl(lib, "split_with_sizes_copy", "CUDA")
@torch.library.impl(lib, "split_with_sizes_copy", "XPU")
@torch.library.impl(lib, "split_with_sizes_copy", "HPU")
@torch.library.impl(lib, "split_with_sizes_copy", "CPU")
@torch.library.impl(lib, "split_with_sizes_copy", "MTIA")
@torch.library.impl(lib, "split_with_sizes_copy", "PrivateUse1")
def split_with_sizes_copy(all_gather_output: torch.Tensor, all_gather_input_split_sizes: list[int], dim: int, out: list[torch.Tensor]) -> None:
    ...

@torch.library.impl(lib, "chunk_cat", "Meta")
@torch.library.impl(lib, "chunk_cat", "CUDA")
@torch.library.impl(lib, "chunk_cat", "XPU")
@torch.library.impl(lib, "chunk_cat", "HPU")
@torch.library.impl(lib, "chunk_cat", "CPU")
@torch.library.impl(lib, "chunk_cat", "MTIA")
@torch.library.impl(lib, "chunk_cat", "PrivateUse1")
def chunk_cat(tensors: list[torch.Tensor], dim: int, num_chunks: int, out: torch.Tensor) -> None:
    ...

@torch.no_grad()
def foreach_all_gather(fsdp_params: list[FSDPParam], group: dist.ProcessGroup, async_op: bool, all_gather_copy_in_stream: torch.Stream, all_gather_stream: torch.Stream, device: torch.device, allocate_memory_from_process_group: bool = ...) -> Optional[AllGatherResult]:
    ...

@torch.no_grad()
def foreach_all_gather_copy_out(all_gather_result: AllGatherResult, fsdp_params: list[FSDPParam], group: dist.ProcessGroup) -> None:
    ...

@torch.no_grad()
def foreach_reduce(fsdp_params: list[FSDPParam], unsharded_grads: list[torch.Tensor], reduce_scatter_group: dist.ProcessGroup, reduce_scatter_stream: torch.Stream, orig_dtype: Optional[torch.dtype], reduce_dtype: Optional[torch.dtype], device: torch.device, gradient_divide_factor: Optional[float], all_reduce_group: Optional[dist.ProcessGroup], all_reduce_stream: torch.Stream, all_reduce_grads: bool, partial_reduce_output: Optional[torch.Tensor], all_reduce_hook: Optional[Callable[[torch.Tensor], None]], allocate_memory_from_process_group: bool = ..., force_sum_reduction_for_comms: bool = ...) -> tuple[torch.Tensor, torch.Event, torch.Event, Optional[torch.Tensor], Optional[torch.Event], Optional[torch.Tensor],]:
    """
    ``unsharded_grads`` owns the references to the gradients computed by
    autograd, so clearing the list frees the gradients.
    """
    ...

def foreach_reduce_scatter_copy_in(unsharded_grads: list[torch.Tensor], reduce_scatter_input: torch.Tensor, world_size: int) -> None:
    ...

