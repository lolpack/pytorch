"""
This type stub file was generated by pyright.
"""

import contextlib
import torch
import torch.nn as nn
from typing import Any, Callable, NamedTuple, Optional
from torch.distributed.tensor import Shard
from torch.utils.hooks import RemovableHandle
from ._fsdp_api import MixedPrecisionPolicy, OffloadPolicy
from ._fsdp_collectives import AllGatherResult
from ._fsdp_common import FSDPMeshInfo, TrainingState

logger = ...
_ModuleToHandleDict = dict[nn.Module, RemovableHandle]
class FSDPCommContext:
    """This has the communication state shared across FSDP states/parameter groups."""
    def lazy_init(self, device: torch.device): # -> None:
        ...
    
    def get_all_gather_streams(self, async_op: bool, training_state: TrainingState) -> tuple[torch.Stream, torch.Stream]:
        ...
    


class AllGatherState(NamedTuple):
    all_gather_result: AllGatherResult
    event: Optional[torch.Event]
    ...


class ReduceScatterState(NamedTuple):
    reduce_scatter_input: torch.Tensor
    event: Optional[torch.Event]
    ...


class AllReduceState(NamedTuple):
    all_reduce_input: torch.Tensor
    event: Optional[torch.Event]
    ...


class FSDPParamGroup:
    """This class represents a parameter group to communicate together."""
    _orig_dtype: Optional[torch.dtype]
    _reduce_dtype: Optional[torch.dtype]
    def __init__(self, params: list[nn.Parameter], modules: tuple[nn.Module, ...], mesh_info: FSDPMeshInfo, post_forward_mesh_info: Optional[FSDPMeshInfo], device: torch.device, shard_placement_fn: Optional[Callable[[nn.Parameter], Optional[Shard]]], mp_policy: MixedPrecisionPolicy, offload_policy: OffloadPolicy) -> None:
        ...
    
    def lazy_init(self): # -> None:
        ...
    
    def unshard(self, async_op: bool = ...): # -> None:
        ...
    
    def wait_for_unshard(self): # -> None:
        """
        1. In forward with implicit prefetching, to overlap the current copy-out
        with the next all-gather, we save a reference to the current all-gather
        result to free after the next copy-out.
        2. Otherwise (explicit prefetching or in backward), we free the
        all-gather result immediately after the current copy-out since we can
        already overlap the current copy-out with the previous reduce-scatter.
        """
        ...
    
    def reshard(self): # -> None:
        ...
    
    def pre_forward(self, module: nn.Module, args: tuple[Any, ...], kwargs: dict[str, Any]) -> tuple[tuple[Any, ...], dict[str, Any]]:
        ...
    
    def post_forward(self, module: nn.Module, input: Any, output: Any): # -> Any:
        ...
    
    def pre_backward(self, default_prefetch: bool, *unused: Any): # -> None:
        ...
    
    def post_backward(self, *unused: Any): # -> None:
        ...
    
    def finalize_backward(self): # -> None:
        ...
    
    @property
    def is_sharded(self) -> bool:
        ...
    
    @property
    def is_sharded_post_forward(self) -> bool:
        ...
    
    @property
    def is_unsharded(self) -> bool:
        ...
    
    @contextlib.contextmanager
    def use_training_state(self, training_state: TrainingState): # -> Generator[None, Any, None]:
        ...
    
    def __repr__(self): # -> str:
        ...
    


class RegisterPostBackwardFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, param_group: FSDPParamGroup, *inputs: torch.Tensor): # -> tuple[Tensor, ...]:
        ...
    
    @staticmethod
    def backward(ctx, *grads: torch.Tensor): # -> tuple[None, *tuple[Tensor, ...]]:
        ...
    


