"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn
from collections.abc import Sequence
from dataclasses import dataclass
from enum import Enum
from typing import Any, Callable, Optional
from torch.distributed.tensor import DTensor, Shard
from torch.distributed.tensor._dtensor_spec import DTensorSpec
from ._fsdp_api import MixedPrecisionPolicy, OffloadPolicy
from ._fsdp_common import FSDPMeshInfo

lib = ...
@torch.library.impl(lib, "copy_", "Meta")
@torch.library.impl(lib, "copy_", "CUDA")
@torch.library.impl(lib, "copy_", "XPU")
@torch.library.impl(lib, "copy_", "HPU")
@torch.library.impl(lib, "copy_", "CPU")
@torch.library.impl(lib, "copy_", "MTIA")
def copy_(tensor, data): # -> None:
    ...

@torch.library.impl(lib, "copy_", "Functionalize")
def copy__functionalize(tensor, data): # -> None:
    ...

if not torch._running_with_deploy():
    ...
class ShardedState(Enum):
    """
    - ``SHARDED``: The sharded parameter is registered to the module. It is the
      only contributor to parameter memory.
    - ``SHARDED_POST_FORWARD``: The unsharded parameter is resharded to a
      smaller world size. Since this data should not be used for computation,
      we do not register it to the module. Users should reshard the module
      before any in-place modifications. Both it and the sharded parameter
      contribute to parameter memory.
    - ``UNSHARDED``: The unsharded parameter is registered to the module. Both
      it and the sharded parameter contribute to parameter memory.
    """
    SHARDED = ...
    SHARDED_POST_FORWARD = ...
    UNSHARDED = ...


@dataclass
class ParamModuleInfo:
    """
    For a parameter, this stores the module and the parameter name to be able
    to do a parameter swap via ``setattr(module, param_name, ...)`` or to get
    the parameter via ``getattr(module, param_name)``. We additionally save
    shared modules and shared parameter names to update them accordingly.
    """
    module: nn.Module
    param_name: str
    shared_modules: list[nn.Module] = ...
    shared_param_names: list[str] = ...


@dataclass
class ExtensionsData:
    all_gather_metadata: Optional[Any] = ...
    all_gather_input_sizes: Sequence[torch.Size] = ...
    def clear(self): # -> None:
        ...
    


class FSDPParam:
    """
    This class manages a parameter with FSDP or FSDP variants applied,
    implementing dim-0 per-parameter sharding.
    """
    orig_dtype: torch.dtype
    param_dtype: Optional[torch.dtype]
    reduce_dtype: Optional[torch.dtype]
    _orig_size: torch.Size
    sharded_size: torch.Size
    contiguous_sharded_stride: tuple[int, ...]
    padded_sharded_param_size: torch.Size
    sharded_post_forward_size: torch.Size
    contiguous_sharded_post_forward_stride: tuple[int, ...]
    _sharded_param_data: torch.Tensor
    sharded_param: nn.Parameter
    _sharded_post_forward_param_data: Optional[torch.Tensor]
    _sharded_post_forward_param: Optional[nn.Parameter]
    _unsharded_param: nn.Parameter
    unsharded_accumulated_grad: Optional[torch.Tensor]
    _sharding_spec: DTensorSpec
    _tp_spec: DTensorSpec
    all_gather_outputs: list[torch.Tensor]
    _extensions_data: ExtensionsData
    _unsharded_inner_tensors: list[torch.Tensor]
    def __init__(self, param: nn.Parameter, module_info: ParamModuleInfo, mesh_info: FSDPMeshInfo, post_forward_mesh_info: Optional[FSDPMeshInfo], device: torch.device, shard_placement_fn: Optional[Callable[[nn.Parameter], Optional[Shard]]], mp_policy: MixedPrecisionPolicy, offload_policy: OffloadPolicy) -> None:
        ...
    
    def init_dtype_attrs(self, mp_policy: MixedPrecisionPolicy): # -> None:
        ...
    
    def init_all_gather_outputs(self, all_gather_input_numels: list[int], all_gather_input_dtypes: list[torch.dtype], world_size: int, device: torch.device, force_recreate: bool = ...): # -> None:
        ...
    
    def init_unsharded_param(self): # -> None:
        """
        [Note: Invariants for torch.compile Traceable FSDP2]
        1. Under compile, we always re-populate the content of `self._unsharded_param`
           per AllGather using the slow path.
        2. Under compile, we always recreate `self.all_gather_outputs` per AllGather.
           This is to ensure the buffer creation is internal to the graph and
           avoid `self.all_gather_outputs` being captured as a graph input.
        3. Under compile, at the end of `free_unsharded_param()`, we always clean up
           `self.all_gather_outputs` and `self._unsharded_inner_tensors`,
           to avoid them being captured as graph output.

        With these invariants, only these tensors will be inputs to the graph:
        - Sharded parameters
        - Placeholders for the `self._unsharded_param` nn.Parameter
        """
        ...
    
    def to_sharded(self) -> None:
        ...
    
    def to_sharded_post_forward(self) -> None:
        ...
    
    def to_unsharded(self) -> None:
        ...
    
    def to_sharded_dtensor(self, tensor: torch.Tensor) -> DTensor:
        """
        Converts a local tensor representing either the sharded parameter or
        sharded gradient to DTensor.
        """
        ...
    
    def to_sharded_post_forward_dtensor(self, tensor: torch.Tensor) -> DTensor:
        ...
    
    def to_accumulated_grad_if_needed(self) -> None:
        ...
    
    def accumulate_unsharded_grad_if_needed(self) -> None:
        ...
    
    def alloc_all_gather_outputs(self) -> None:
        ...
    
    def free_unsharded_param(self) -> None:
        ...
    
    @property
    def all_gather_inputs(self) -> list[torch.Tensor]:
        ...
    
    @property
    def unsharded_param(self) -> nn.Parameter:
        ...
    
    @property
    def unsharded_grad_data(self) -> torch.Tensor:
        ...
    
    @property
    def unsharded_accumulated_grad_data(self) -> torch.Tensor:
        ...
    
    @property
    def shard_mesh(self): # -> DeviceMesh:
        ...
    
    @property
    def shard_mesh_from_root(self): # -> DeviceMesh:
        ...
    
    def reset_sharded_param(self): # -> None:
        ...
    
    def __repr__(self): # -> str:
        ...
    


def alloc_storage(tensor: torch.Tensor) -> None:
    ...

def free_storage(tensor: torch.Tensor) -> None:
    ...

def unsafe_setattr_param(module: nn.Module, param_name: str, param: nn.Parameter) -> None:
    ...

def set_requires_grad_if_needed(src_tensor: torch.Tensor, dst_tensor: torch.Tensor) -> None:
    ...

