"""
This type stub file was generated by pyright.
"""

import torch
from collections.abc import Iterator
from dataclasses import dataclass
from typing import Any, NamedTuple, TYPE_CHECKING
from torch.distributed.fsdp._common_utils import _FSDPState
from torch.distributed.fsdp._flat_param import FlatParamHandle

if TYPE_CHECKING:
    ...
logger = ...
@dataclass
class FSDPParamInfo:
    state: _FSDPState
    handle: FlatParamHandle
    param_indices: dict[str, int]
    param_requires_grad: list[bool]
    ...


def sorted_items(dictionary: dict[str, Any]) -> Iterator[tuple[str, Any]]:
    ...

@dataclass
class _ConsolidatedOptimState:
    """
    This holds the consolidated optimizer state on the target rank. Positive-
    dimension tensor state is communicated across ranks, while zero-dimension
    tensor state and non-tensor state is taken directly from the target rank.

    PyTorch version 1.12 moved to using zero-dimension tensors for scalar
    values, but user implemented optimizers may still use float (i.e. a
    non-tensor). Thus, we support both and handle them identically.

    Attributes:
        tensor_state (Dict[str, torch.Tensor]): Mapping from positive-dimension
            tensor state name to the unsharded flat tensor representing the
            state.
        zero_dim_tensor_state (Dict[str, torch.Tensor]): Mapping from zero-
            dimension tensor state name to its value.
        non_tensor_state (Dict[str, Any]): Mapping from non-tensor state
            name to its value.
    """
    tensor_state: dict[str, torch.Tensor] = ...
    zero_dim_tensor_state: dict[str, torch.Tensor] = ...
    non_tensor_state: dict[str, Any] = ...


class _PosDimTensorInfo(NamedTuple):
    """
    Metadata for positive-dimension tensors used internally for
    :meth:`scatter_full_optim_state_dict`.

    Attributes:
        shape (torch.Size): Sharded tensor shape (which is equal to the
            unsharded tensor shape if the tensor is optimizer state for a
            non-FSDP parameter and is hence not sharded).
        dtype (torch.dtype): Data type of the tensor.
    """
    shape: torch.Size
    dtype: torch.dtype
    ...


class _OptimStateKey(NamedTuple):
    """
    This represents an optimizer state key that may be used commonly across
    ranks. It is based on the unflattened parameter names rather than parameter
    IDs to make it independent of each rank's own optimizer construction.
    """
    unflat_param_names: tuple[str, ...]
    is_fsdp_managed: bool
    ...


@dataclass
class StateInfo:
    tensors: dict[str, _PosDimTensorInfo]
    scalar_tensors: dict[str, torch.Tensor]
    non_tensors: dict[str, Any]
    ...


