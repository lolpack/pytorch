"""
This type stub file was generated by pyright.
"""

import contextlib
from typing import Optional, TYPE_CHECKING
from torch._dynamo.variables.functions import UserFunctionVariable
from torch._guards import Source
from torch._ops import HigherOrderOperator
from .base import VariableTracker
from torch._dynamo.symbolic_convert import InstructionTranslator

"""
This module contains classes and utilities for handling higher-order operators in Dynamo.
It provides functionality for tracing and transforming control flow constructs like
conditions (torch.cond), loops (torch.while_loop), maps (torch.ops.higher_order.map),
and other higher-order operations.

The module includes specialized VariableTracker classes for different types of
higher-order operations, along with utilities for:
- Speculating and capturing subgraphs
- Managing control flow
- Handling autograd function applications
- Supporting function transformations
- Processing activation checkpoints

These classes work together to enable Dynamo to correctly trace and compile code
containing complex control flow patterns and higher-order functions while preserving
their semantic behavior.
"""
if TYPE_CHECKING:
    ...
log = ...
hc_log = ...
def raise_hard_error_if_graph_break(reason): # -> Callable[..., _Wrapped[Callable[..., Any], Any, Callable[..., Any], Any]]:
    ...

@contextlib.contextmanager
def discard_graph_changes(tx): # -> Generator[None, Any, None]:
    ...

def check_meta_consistency_vt(vars1: list[VariableTracker], vars2: list[VariableTracker], lhs_name: str, rhs_name: str, include_contiguity: bool = ...) -> None:
    ...

@contextlib.contextmanager
def dynamo_enable_grad(tx: InstructionTranslator, enable=...): # -> Generator[None, Any, None]:
    ...

@contextlib.contextmanager
def dynamo_under_activation_checkpoint(tx: InstructionTranslator): # -> Generator[None, Any, None]:
    ...

def find_mismatched_vars(var, types, allow_none=...): # -> set[Any]:
    """
    Recursively finds variables whose type is not an instance of the specified types.
    Args:
        var: The variable to check.
        types: A tuple of allowed types.
        allow_none (bool): Whether to allow None values. Defaults to False.
    Returns:
        A set of variables whose type is not an instance of the specified types.
    """
    ...

def only_consist_of(var, types, allow_none=...): # -> bool:
    ...

def are_same_graph_modules(fn_name, a_mod, b_mod, fake_mode): # -> bool:
    ...

def validate_args_and_maybe_create_graph_inputs(sub_args, tracer, tx, set_subgraph_inputs, description, sub_args_names=...): # -> list[VariableTracker] | list[Any]:
    ...

def speculate_subgraph(tx, f, sub_args, sub_kwargs, description, *, source_target=..., always_restore=..., enable_grad=..., set_subgraph_inputs=..., restore_side_effects=..., should_flatten_outputs=..., under_activation_checkpoint=..., supports_input_mutation=..., supports_aliasing=..., tracer=...): # -> tuple[tuple[VariableTracker | Any, VariableTracker | None], Any, Any]:
    ...

def make_attr(tx: InstructionTranslator, name): # -> Proxy:
    ...

class TorchHigherOrderOperatorVariable(VariableTracker):
    def __init__(self, value: HigherOrderOperator, source: Optional[Source] = ..., **kwargs) -> None:
        ...
    
    @staticmethod
    def make(value, source=..., **kwargs): # -> BaseHOPVariable:
        ...
    
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    
    def as_python_constant(self): # -> HigherOrderOperator:
        ...
    


class CustomFunctionHigherOrderOperatorVariable(TorchHigherOrderOperatorVariable):
    """
    Wraps torch._functorch.autograd_function.custom_function_call
    """
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    


class CondHigherOrderVariable(TorchHigherOrderOperatorVariable):
    supports_input_mutation = ...
    supports_aliasing = ...
    @raise_hard_error_if_graph_break(reason="Cond doesn't work unless it is captured completely with torch.compile.")
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    


class CallTorchbindHigherOrderVariable(TorchHigherOrderOperatorVariable):
    def __init__(self, hop, source, script_obj_var, method_name) -> None:
        ...
    
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    


def validate_subgraph_output_types(output: VariableTracker): # -> None:
    """Verify that that the output of the subgraph is a tensor,
    int, bool, SymBool, or SymInt.
    """
    ...

class WhileLoopHigherOrderVariable(TorchHigherOrderOperatorVariable):
    supports_input_mutation = ...
    supports_aliasing = ...
    @raise_hard_error_if_graph_break(reason="while_loop doesn't work unless it is captured completely with torch.compile.")
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    


class AssociativeScanHigherOrderVariable(TorchHigherOrderOperatorVariable):
    supports_input_mutation = ...
    supports_aliasing = ...
    @raise_hard_error_if_graph_break(reason="associative_scan must be captured completely with torch.compile.")
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    


class ScanHigherOrderVariable(TorchHigherOrderOperatorVariable):
    supports_input_mutation = ...
    supports_aliasing = ...
    @raise_hard_error_if_graph_break(reason="scan must be captured completely with torch.compile.")
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    


def non_single_tensor_return_unsupported(api, ret): # -> None:
    ...

class MapHigherOrderVariable(TorchHigherOrderOperatorVariable):
    supports_input_mutation = ...
    supports_aliasing = ...
    @raise_hard_error_if_graph_break(reason="map doesn't work unless it is captured completely with torch.compile.")
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    


class ExecutorchCallDelegateHigherOrderVariable(TorchHigherOrderOperatorVariable):
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    


class FunctorchHigherOrderVariable(UserFunctionVariable):
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    


class FunctionalCallVariable(FunctorchHigherOrderVariable):
    def call_function(self, tx, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    


class ReparametrizeModuleCallVariable(FunctorchHigherOrderVariable):
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    def call_function(self, tx, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    


class WrapHigherOrderVariable(TorchHigherOrderOperatorVariable):
    supports_input_mutation = ...
    supports_aliasing = ...
    def install_subgraph_in_output_graph(self, tx, fn_vt, fn_args_vt, kwargs, body_gmod, attr_name=...):
        ...
    
    def create_wrapped_node(self, tx: InstructionTranslator, fn_vt, fn_args_vt, kwargs, description, under_activation_checkpoint=..., *, subgraph_name=...): # -> tuple[tuple[Proxy, *tuple[Any, ...]], dict[Any, Any], PyTree, VariableTracker | Any, VariableTracker | None, GraphModule, str]:
        ...
    
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    


class WrapWithSetGradEnabledHigherOrderVariable(TorchHigherOrderOperatorVariable):
    """
    This hop is not exposed to users but is inserted into the graph
    after export as a post-processing step.
    """
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    


class WrapWithAutocastHigherOrderVariable(TorchHigherOrderOperatorVariable):
    """
    This hop is not exposed to users but is inserted into the graph
    after export as a post-processing step.
    """
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    


class HintsWrapperHigherOrderVariable(TorchHigherOrderOperatorVariable):
    @raise_hard_error_if_graph_break(reason="Hints_wrapper doesn't work unless it is captured completely with torch.compile.")
    def call_function(self, tx, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    


class OutDtypeHigherOrderVariable(TorchHigherOrderOperatorVariable):
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    


class StrictModeHigherOrderVariable(TorchHigherOrderOperatorVariable):
    @raise_hard_error_if_graph_break(reason="strict_mode HOO doesn't work unless it is captured completely with torch.compile.")
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    


class CheckpointHigherOrderVariable(WrapHigherOrderVariable):
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    


class DynamoBypassingWrapperHigherOrderVariable(WrapHigherOrderVariable):
    def __init__(self, hop, source) -> None:
        ...
    
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    


class ExportTracepointHigherOrderVariable(TorchHigherOrderOperatorVariable):
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    


class RunWithRNGStateHigherOrderVariable(TorchHigherOrderOperatorVariable):
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    


class AutoFunctionalizeHigherOrderVariable(TorchHigherOrderOperatorVariable):
    def call_function(self, tx, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    


class FlexAttentionBackwardHighOrderVariable(TorchHigherOrderOperatorVariable):
    def proxy_submod(self, tx, arg): # -> Proxy:
        ...
    
    def to_proxy(self, tx, arg): # -> Proxy | list[Any] | tuple[Any, ...]:
        ...
    
    def call_function(self, tx, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    


class TraceWrappedHigherOrderOperatorVariable(TorchHigherOrderOperatorVariable):
    """
    Handles torch._dynamo._trace_wrapped_higher_order_op.inner_trace
    by unwrapping the higher order op and inlining through it.  This op
    is created by dynamo to survive through AotAutograd, then unwrapped
    here in the call to dynamo from compiled autograd.
    """
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    


class FlexAttentionHigherOrderVariable(TorchHigherOrderOperatorVariable):
    @staticmethod
    def normalize_to_args(args, kwargs):
        ...
    
    def create_wrapped_node(self, tx: InstructionTranslator, query: VariableTracker, fn: VariableTracker, fn_name: str): # -> tuple[Proxy, tuple[Any, ...]]:
        ...
    
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    


class AutogradFunctionApplyVariable(VariableTracker):
    def __init__(self, fwd_graph, bwd_graph, parent_source, **kwargs) -> None:
        ...
    
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    


def maybe_positional_arg_names(func): # -> list[Any] | None:
    ...

class BaseHOPVariable(WrapHigherOrderVariable):
    supports_input_mutation = ...
    supports_aliasing = ...
    def python_type(self): # -> type[HigherOrderOperator]:
        ...
    
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    


class InvokeSubgraphHigherOrderVariable(WrapHigherOrderVariable):
    supports_input_mutation = ...
    supports_aliasing = ...
    def install_subgraph_in_output_graph(self, tx, fn_vt, fn_args_vt, kwargs, body_gmod, attr_name):
        ...
    
    @raise_hard_error_if_graph_break(reason="torch.compile requires the `nested_compile_region` decorated function to be capturable into a single graph")
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    


_hop_name_to_variable_class = ...
