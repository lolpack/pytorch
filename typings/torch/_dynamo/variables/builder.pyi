"""
This type stub file was generated by pyright.
"""

import dataclasses
import re
import weakref
import torch
from typing import Any, NamedTuple, Optional, TYPE_CHECKING, Union
from torch import SymInt
from torch._subclasses.fake_tensor import FakeTensor
from torch.fx.experimental.symbolic_shapes import DimDynamic, SymIntSymbolicContext
from torch.utils.weak import TensorWeakRef
from ..pgo import FrameStateSizeEntry
from ..source import Source
from ..utils import odict_values, range_iterator, tuple_iterator
from .base import VariableTracker
from torch._dynamo.codegen import PyCodegen
from torch._dynamo.symbolic_convert import InstructionTranslator

"""
This module contains classes and utilities for building variable trackers in Dynamo.
Variable trackers are used to convert Python values into symbolic representations
that can be traced and transformed during graph capture.

The key classes are:

- VariableBuilder: Handles source-tracked objects that need guards and proper
  reconstruction in the output graph. Used for inputs, module attributes, etc.

- SourcelessBuilder: Handles ephemeral objects created during tracing that don't
  need source tracking or guards. Used for temporary lists, intermediate values, etc.

Variable trackers enable Dynamo to track the flow of values through the program,
maintain guards for dynamic properties, and reconstruct values in the output graph.
The builders in this module handle converting Python values into appropriate
VariableTracker instances based on their type and usage context.
"""
if TYPE_CHECKING:
    ...
log = ...
static_inputs_log = ...
DimList = list
def safe_has_grad(t): # -> bool:
    ...

class _missing:
    ...


@dataclasses.dataclass
class GraphArg:
    source: Source
    _example: Union[TensorWeakRef, torch.SymInt]
    pass_arg_as_tensor: bool
    fake_tensor: Optional[torch._subclasses.fake_tensor.FakeTensor]
    is_tensor: bool = ...
    example_strong_ref: Optional[torch.Tensor] = ...
    @property
    def example(self): # -> Tensor | SymInt:
        ...
    
    def __post_init__(self): # -> None:
        ...
    
    def reconstruct(self, codegen: PyCodegen): # -> None:
        ...
    
    def erase(self): # -> None:
        ...
    
    def __eq__(self, other) -> bool:
        ...
    


class BackwardStateGraphArg(GraphArg):
    def __init__(self) -> None:
        ...
    
    def reconstruct(self, codegen: PyCodegen): # -> None:
        ...
    


ITERTOOLS_TYPE_IDS: frozenset[int] = ...
ITERTOOLS_POLYFILLED_TYPE_IDS: set[int] = ...
og_module_named_buffers_fn_ptr = ...
og_module_named_parameters_fn_ptr = ...
class VariableBuilder:
    """Wrap a python value in a VariableTracker() instance"""
    def __init__(self, tx, source: Source) -> None:
        ...
    
    def __call__(self, value):
        ...
    
    def get_source(self): # -> Source | OptimizerSource | AttrSource | AttrProxySource:
        ...
    
    def install_guards(self, *guards): # -> dict[Any, Any] | None:
        ...
    
    def wrap_regex_pattern(self, value: re.Pattern): # -> RegexPatternVariable:
        ...
    
    def wrap_weakref(self, value: weakref.ReferenceType): # -> WeakRefVariable:
        ...
    
    def wrap_removable_handle(self, value):
        ...
    
    def wrap_jit_function(self, value): # -> WrapperUserFunctionVariable:
        ...
    
    def wrap_mapping_proxy(self, value):
        ...
    
    def wrap_user_defined(self, value: Any): # -> UserDefinedObjectVariable:
        ...
    
    def wrap_listlike(self, value: Union[tuple, list, odict_values, NamedTuple]): # -> VariableTracker | TupleVariable:
        ...
    
    def wrap_tuple_iterator(self, value: tuple_iterator):
        ...
    
    def wrap_range_iterator(self, value: range_iterator):
        ...
    
    def wrap_slice_range(self, value: Union[slice, range]): # -> SliceVariable | RangeVariable:
        ...
    
    def mark_static_input(self, value: torch.Tensor, guard: bool): # -> None:
        ...
    
    def wrap_module(self, value: torch.nn.Module): # -> DelayGraphBreakVariable | FSDPManagedNNModuleVariable | UnspecializedBuiltinNNModuleVariable | UnspecializedNNModuleVariable:
        ...
    
    def wrap_literal(self, value): # -> VariableTracker | SymNodeVariable:
        ...
    
    def assert_not_wrapped_by_this_graph(self, value: torch.Tensor): # -> None:
        ...
    
    def wrap_tensor(self, value: torch.Tensor): # -> VariableTracker:
        ...
    
    def wrap_numpy_ndarray(self, value):
        ...
    
    def wrap_symint(self, value, dynamism: Optional[DimDynamic] = ..., context: Optional[SymIntSymbolicContext] = ...): # -> VariableTracker | SymNodeVariable:
        ...
    
    def wrap_symfloat(self, value): # -> VariableTracker:
        ...
    
    def wrap_unspecialized_primitive(self, value): # -> ConstantVariable:
        ...
    


def wrap_fx_proxy(tx, proxy, example_value=..., subclass_type=..., **options) -> VariableTracker:
    ...

def cache_real_value_when_export(tx, proxy, example_value): # -> None:
    ...

def wrap_fx_proxy_cls(target_cls, tx, proxy, example_value=..., subclass_type=..., **options):
    ...

def handle_traced_output(example_value, tx, proxy, options, subclass_type, target_cls):
    ...

def construct_tensor_variable(target_cls, tx, proxy, example_value, subclass_type, options):
    """
    Actually construct a tensor variable after all the pre-processing from
    wrapping a pre-existing or newly created tensor value.
    """
    ...

def get_automatic_dynamic_shapes_mark_as(): # -> Literal[DimDynamic.DYNAMIC, DimDynamic.SIZE_LIKE_UNBACKED, DimDynamic.OBLIVIOUS_SIZE]:
    ...

_DYNAMIC_SOURCES: Optional[set[str]] = ...
_DYNAMIC_SOURCES_CONFIG_HASH: Optional[int] = ...
def get_dynamic_sources() -> set[str]:
    ...

def is_dynamic_source(source_name: str) -> bool:
    ...

def record_automatic_dynamic(tx: InstructionTranslator, name: str, e: torch.Tensor) -> FrameStateSizeEntry:
    ...

_UNBACKED_SOURCES: Optional[set[str]] = ...
_UNBACKED_SOURCES_CONFIG_HASH: Optional[int] = ...
def get_unbacked_sources() -> set[str]:
    ...

def is_unbacked_source(source_name: str) -> bool:
    ...

def wrap_to_fake_tensor_and_record(e, tx, *, source: Optional[Source], is_tensor: bool, parent_context=...): # -> Tensor | TensorWithFlatten | FakeTensor:
    ...

class SourcelessBuilder:
    """
    Like builder, but stateless and does not require a source. Useful for simple type->VT objects, or objects
    that are being created/evaporated during inlining (ex: consider a locally made list of tensors we then iterate over
    .), such a list should not show up as an artifact from inputs, nor in reconstruction, nor in the graph. However,
    there may be reasons to represent it as a ListVariable internally.

    NOTE - Objects produced here are born UNGUARDED due to the nature of sources!

    NOTE - This class is very new! It will have some rough edges, but it was created to stem the bleeding of giant
    if/else type->VariableTracker trees that were cropping up all over dynamo.
    """
    def __init__(self) -> None:
        ...
    
    @staticmethod
    def create(tx: InstructionTranslator, value) -> VariableTracker:
        ...
    
    @staticmethod
    def wrap_constant_literal(value): # -> VariableTracker:
        ...
    
    @staticmethod
    def make_type_handlers(): # -> dict[Any, Any]:
        ...
    


class SourcelessUserDefinedObjectBuilder:
    """
    SourceLessBuilder does not return a UserDefinedObjectVariable, but in some
    cases it might be ok to return UserDefinedObjects. In such case, use this
    builder.
    """
    def __init__(self) -> None:
        ...
    
    @staticmethod
    def create(tx: InstructionTranslator, value) -> VariableTracker:
        ...
    


