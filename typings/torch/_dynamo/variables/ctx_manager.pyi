"""
This type stub file was generated by pyright.
"""

from typing import TYPE_CHECKING, Union
from .base import VariableTracker
from .user_defined import UserDefinedObjectVariable
from torch._dynamo.codegen import PyCodegen
from torch._dynamo.symbolic_convert import InstructionTranslator

"""
This file contains a collection of context manager classes used by Dynamo for tracking
and managing various PyTorch runtime states during graph compilation. These context
managers handle different aspects of PyTorch's execution environment, including:

- Autograd states (grad mode, inference mode)
- CUDA streams and events
- Profiling contexts
- Deterministic algorithms
- Forward/backward AD modes
- SDPA (Scaled Dot Product Attention) kernels
- FSDP (Fully Sharded Data Parallel) states
- AMP (Automatic Mixed Precision) autocast states

The context managers ensure proper state transitions during graph compilation by
tracking enter/exit points and managing cleanup operations. They help maintain
consistency between eager execution and compiled graph behavior by capturing and
restoring state changes.
"""
if TYPE_CHECKING:
    ...
class ContextWrappingVariable(VariableTracker):
    _nonvar_fields = ...
    def __init__(self, target_values, initial_values=..., **kwargs) -> None:
        ...
    
    def enter(self, tx): # -> VariableTracker:
        ...
    
    def set_cleanup_hook(self, tx: InstructionTranslator, fn=...): # -> None:
        ...
    
    def exit(self, tx: InstructionTranslator, *args): # -> VariableTracker:
        ...
    
    def reconstruct_type(self, codegen: PyCodegen): # -> None:
        ...
    
    def reconstruct(self, codegen: PyCodegen): # -> None:
        ...
    
    def module_name(self):
        ...
    
    def fn_name(self):
        ...
    
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    
    def supports_graph_breaks(self): # -> Literal[True]:
        ...
    
    def exit_on_graph_break(self): # -> Literal[True]:
        ...
    
    def cleanup(self): # -> None:
        ...
    
    def cleanup_assert(self): # -> None:
        ...
    


class GenericContextWrappingVariable(UserDefinedObjectVariable):
    def __init__(self, cm_obj, **kwargs) -> None:
        ...
    
    def module_name(self):
        ...
    
    def fn_name(self): # -> str:
        ...
    
    def enter(self, tx): # -> VariableTracker:
        ...
    
    def exit(self, tx: InstructionTranslator, *args): # -> VariableTracker:
        ...
    
    def supports_graph_breaks(self): # -> Literal[False]:
        ...
    
    def exit_on_graph_break(self): # -> Literal[True]:
        ...
    


class RepararametrizeModuleContextVariable(GenericContextWrappingVariable):
    def __init__(self, ctx_manager_vt, mod) -> None:
        ...
    
    def enter(self, tx: InstructionTranslator):
        ...
    
    def exit(self, tx: InstructionTranslator, *args):
        ...
    
    def __getattr__(self, name): # -> Any:
        ...
    


class GradInplaceRequiresGradCtxManagerVariable(ContextWrappingVariable):
    """represents torch grad requires grad"""
    @staticmethod
    def create(tx: InstructionTranslator, target_values, **kwargs): # -> GradInplaceRequiresGradCtxManagerVariable:
        ...
    
    def enter(self, tx): # -> VariableTracker:
        ...
    
    def exit(self, tx: InstructionTranslator, *args): # -> VariableTracker:
        ...
    


class TemporarilyPopInterpreterStackCtxManagerVariable(ContextWrappingVariable):
    """represents torch._functorch.pyfunction.temporarily_pop_interpreter_stack()"""
    @staticmethod
    def create(tx: InstructionTranslator, target_values, **kwargs): # -> TemporarilyPopInterpreterStackCtxManagerVariable:
        ...
    
    def enter(self, tx): # -> VariableTracker:
        ...
    
    def exit(self, tx: InstructionTranslator, *args): # -> VariableTracker:
        ...
    


class JvpIncrementNestingCtxManagerVariable(ContextWrappingVariable):
    """represents torch.func.jvp increment/decrement nesting"""
    _guards_singleton = ...
    @staticmethod
    def create(tx: InstructionTranslator, **kwargs): # -> JvpIncrementNestingCtxManagerVariable:
        ...
    
    def enter(self, tx): # -> VariableTracker:
        ...
    
    def exit(self, tx: InstructionTranslator, *args): # -> VariableTracker:
        ...
    


class SetFwdGradEnabledContextManager(ContextWrappingVariable):
    """represents torch.autograd.forward_ad._set_fwd_grad_enabled() to enable/disable fwd grad"""
    @staticmethod
    def create(tx: InstructionTranslator, target_values, **kwargs): # -> SetFwdGradEnabledContextManager:
        ...
    
    def enter(self, tx): # -> VariableTracker:
        ...
    
    def exit(self, tx: InstructionTranslator, *args): # -> VariableTracker:
        ...
    


class DualLevelContextManager(ContextWrappingVariable):
    """Represents torch.autograd.forward_ad.dual_level ctx manager"""
    _guards_singleton = ...
    @staticmethod
    def create(tx: InstructionTranslator, **kwargs): # -> DualLevelContextManager:
        ...
    
    def enter(self, tx): # -> VariableTracker:
        ...
    
    def exit(self, tx: InstructionTranslator, *args): # -> VariableTracker:
        ...
    


class GradIncrementNestingCtxManagerVariable(ContextWrappingVariable):
    """represents torch.func.grad increment/decrement nesting"""
    _guards_singleton = ...
    @staticmethod
    def create(tx: InstructionTranslator, **kwargs): # -> GradIncrementNestingCtxManagerVariable:
        ...
    
    def enter(self, tx): # -> VariableTracker:
        ...
    
    def exit(self, tx: InstructionTranslator, *args): # -> VariableTracker:
        ...
    


class CatchWarningsCtxManagerVariable(ContextWrappingVariable):
    """Delay a call to warnings.catch_warnings"""
    @staticmethod
    def create(tx: InstructionTranslator, catch_warnings_args): # -> CatchWarningsCtxManagerVariable:
        ...
    
    def __init__(self, catch_warnings_args, **kwargs) -> None:
        ...
    
    def enter(self, tx): # -> VariableTracker:
        ...
    
    def reconstruct(self, cg): # -> None:
        ...
    


class VmapIncrementNestingCtxManagerVariable(ContextWrappingVariable):
    """represents torch VMap increment/decrement nesting"""
    _guards_singleton = ...
    @staticmethod
    def create(tx: InstructionTranslator, target_values, **kwargs): # -> VmapIncrementNestingCtxManagerVariable:
        ...
    
    def enter(self, tx): # -> VariableTracker:
        ...
    
    def exit(self, tx: InstructionTranslator, *args): # -> VariableTracker:
        ...
    


class GradModeVariable(ContextWrappingVariable):
    """represents torch.{no_grad,enable_grad,set_grad_mode}()"""
    _guards_singleton = ...
    @staticmethod
    def create(tx: InstructionTranslator, target_value, initialized=..., **kwargs): # -> GradModeVariable:
        ...
    
    def __init__(self, target_values, initial_values=..., initialized=..., **kwargs) -> None:
        ...
    
    def enter(self, tx): # -> VariableTracker:
        ...
    
    def exit(self, tx: InstructionTranslator, *args): # -> VariableTracker:
        ...
    
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]): # -> VariableTracker:
        ...
    
    def module_name(self): # -> Literal['torch']:
        ...
    
    def fn_name(self): # -> Literal['set_grad_enabled']:
        ...
    


class InferenceModeVariable(ContextWrappingVariable):
    @staticmethod
    def create(tx: InstructionTranslator, target_value, **kwargs): # -> InferenceModeVariable:
        ...
    
    def __init__(self, target_values, initial_values=..., **kwargs) -> None:
        ...
    
    def exit(self, tx: InstructionTranslator, *args): # -> None:
        ...
    
    def enter(self, tx): # -> None:
        ...
    
    def module_name(self): # -> Literal['torch']:
        ...
    
    def fn_name(self): # -> Literal['inference_mode']:
        ...
    


class CUDADeviceVariable(ContextWrappingVariable):
    """represents torch.cuda.device"""
    @staticmethod
    def create(tx: InstructionTranslator, device, **kwargs): # -> CUDADeviceVariable:
        ...
    
    def __init__(self, target_values, initial_values=..., **kwargs) -> None:
        ...
    
    def exit(self, tx: InstructionTranslator, *args): # -> VariableTracker:
        ...
    
    def enter(self, tx): # -> None:
        ...
    
    def module_name(self): # -> Literal['torch.cuda']:
        ...
    
    def fn_name(self): # -> Literal['device']:
        ...
    


class TorchFunctionDisableVariable(ContextWrappingVariable):
    """represents whether torch function overrides are enabled or not"""
    _guards_singleton = ...
    @staticmethod
    def create(tx: InstructionTranslator, **kwargs): # -> TorchFunctionDisableVariable:
        ...
    
    def __init__(self, target_values, initial_values=..., only_subclass=..., **kwargs) -> None:
        ...
    
    def set_cleanup_hook(self, tx: InstructionTranslator, fn=...): # -> None:
        ...
    
    def module_name(self): # -> Literal['torch._C']:
        ...
    
    def fn_name(self): # -> Literal['DisableTorchFunctionSubclass', 'DisableTorchFunction']:
        ...
    


class DeterministicAlgorithmsVariable(ContextWrappingVariable):
    """represents torch.{are_deterministic_algorithms_enabled,use_deterministic_algorithms}()"""
    _guards_singleton = ...
    @staticmethod
    def create(tx: InstructionTranslator, target_value, **kwargs): # -> DeterministicAlgorithmsVariable:
        ...
    
    def __init__(self, target_values, initial_values=..., **kwargs) -> None:
        ...
    
    def enter(self, tx): # -> VariableTracker:
        ...
    
    def module_name(self): # -> Literal['torch']:
        ...
    
    def fn_name(self): # -> Literal['use_deterministic_algorithms']:
        ...
    


class DisabledSavedTensorsHooksVariable(ContextWrappingVariable):
    """represents torch.autograd.graph.disable_saved_tensors_hook."""
    @staticmethod
    def create(tx: InstructionTranslator, target_value, **kwargs): # -> DisabledSavedTensorsHooksVariable:
        ...
    
    def __init__(self, target_values, initial_values=..., **kwargs) -> None:
        ...
    
    def enter(self, tx): # -> VariableTracker:
        ...
    
    def module_name(self): # -> Literal['torch.autograd.graph']:
        ...
    
    def fn_name(self): # -> Literal['disable_saved_tensors_hooks']:
        ...
    


class AutocastModeVariable(ContextWrappingVariable):
    @staticmethod
    def create(func, args, kwargs): # -> AutocastModeVariable:
        ...
    
    def __init__(self, target_values, initial_values=..., **kwargs) -> None:
        ...
    
    def exit(self, tx: InstructionTranslator, *args): # -> VariableTracker:
        ...
    
    def enter(self, tx): # -> None:
        ...
    
    def module_name(self): # -> Literal['torch.amp.autocast_mode']:
        ...
    
    def fn_name(self): # -> Literal['autocast']:
        ...
    


class NullContextVariable(ContextWrappingVariable):
    """
    This class represents Python contextlib.nullcontext.
    """
    def __init__(self, target_values=..., **kwargs) -> None:
        ...
    
    def enter(self, tx): # -> VariableTracker:
        ...
    
    def exit(self, tx: InstructionTranslator, *args): # -> VariableTracker:
        ...
    
    def module_name(self): # -> Literal['contextlib']:
        ...
    
    def fn_name(self): # -> Literal['nullcontext']:
        ...
    


class ProfilerContextVariable(ContextWrappingVariable):
    """
    This class represents a set of torch profiler context objects, where Dynamo
    ignores all the side-effects in the __init__, __enter__ and __exit__ methods
    by treating the object mostly as a `contextlib.nullcontext`, except for edge
    cases like the `__enter__` method which returns the object itself rather
    than `None`, per implementation of the torch objects.
    """
    def __init__(self, **kwargs) -> None:
        ...
    
    def enter(self, tx): # -> Self:
        ...
    
    def exit(self, tx: InstructionTranslator, *args): # -> VariableTracker:
        ...
    
    def module_name(self): # -> Literal['contextlib']:
        ...
    
    def fn_name(self): # -> Literal['nullcontext']:
        ...
    
    def reconstruct(self, cg):
        ...
    


class StreamContextVariable(ContextWrappingVariable):
    @staticmethod
    def create(tx: InstructionTranslator, target_value, **kwargs): # -> StreamContextVariable:
        ...
    
    def __init__(self, target_values, device, initial_values=..., **kwargs) -> None:
        ...
    
    def enter(self, tx): # -> None:
        ...
    
    def exit(self, tx: InstructionTranslator, *args): # -> None:
        ...
    


class PreserveVersionContextVariable(ContextWrappingVariable):
    """
    Wraps torch.autograd._unsafe_preserve_version_counter
    """
    @staticmethod
    def constructor(tx): # -> LambdaVariable:
        ...
    
    def __init__(self, tensors, prev_versions, **kwargs) -> None:
        ...
    
    def enter(self, tx): # -> None:
        ...
    
    def exit(self, tx: InstructionTranslator, *args): # -> VariableTracker:
        ...
    
    def reconstruct(self, codegen: PyCodegen):
        ...
    


class FSDPParamGroupUseTrainingStateVariable(ContextWrappingVariable):
    _guards_singleton = ...
    @staticmethod
    def create(tx: InstructionTranslator, param_group_var, target_value, **kwargs): # -> FSDPParamGroupUseTrainingStateVariable:
        ...
    
    def __init__(self, param_group_var, target_values, initial_values=..., **kwargs) -> None:
        ...
    
    def enter(self, tx): # -> VariableTracker:
        ...
    
    def exit(self, tx: InstructionTranslator, *args): # -> VariableTracker:
        ...
    
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]): # -> VariableTracker:
        ...
    
    def module_name(self): # -> LiteralString:
        ...
    
    def fn_name(self): # -> Literal['use_training_state']:
        ...
    


class SDPAKernelVariable(ContextWrappingVariable):
    """represents torch.nn.attention.sdpa_kernel"""
    @staticmethod
    def create(tx: InstructionTranslator, backends, set_priority=..., **kwargs): # -> SDPAKernelVariable:
        ...
    
    def __init__(self, target_values: list[torch.nn.attention.SDPBackend], initial_values=..., set_priority: bool = ..., **kwargs) -> None:
        ...
    
    def enter(self, tx): # -> VariableTracker:
        ...
    
    def exit(self, tx: InstructionTranslator, *args): # -> VariableTracker:
        ...
    
    def module_name(self): # -> Literal['torch.nn.attention']:
        ...
    
    def fn_name(self): # -> Literal['_sdpa_kernel_variadic']:
        ...
    


class StreamVariable(VariableTracker):
    def __init__(self, proxy, value, device, **kwargs) -> None:
        ...
    
    def python_type(self): # -> type[Stream]:
        ...
    
    def call_method(self, tx, name, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    
    def as_proxy(self):
        ...
    
    def reconstruct(self, codegen: PyCodegen): # -> None:
        ...
    


class EventVariable(VariableTracker):
    def __init__(self, proxy, value, **kwargs) -> None:
        ...
    
    def call_method(self, tx, name, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    
    def as_proxy(self):
        ...
    
    def reconstruct(self, codegen: PyCodegen): # -> None:
        ...
    


class DynamoConfigPatchVariable(ContextWrappingVariable):
    """represents torch._dynamo.patch_dynamo_config"""
    def __init__(self, target_values, **kwargs) -> None:
        ...
    
    def module_name(self): # -> Literal['torch._dynamo']:
        ...
    
    def fn_name(self): # -> Literal['patch_dynamo_config']:
        ...
    


class SetFullgraphVariable(ContextWrappingVariable):
    """represents torch._dynamo.set_fullgraph"""
    def __init__(self, fullgraph, **kwargs) -> None:
        ...
    
    def module_name(self): # -> Literal['torch._dynamo']:
        ...
    
    def fn_name(self): # -> Literal['set_fullgraph']:
        ...
    


class WithExitFunctionVariable(VariableTracker):
    _nonvar_fields = ...
    def __init__(self, ctx: Union[ContextWrappingVariable, GenericContextWrappingVariable], target, **kwargs) -> None:
        ...
    
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    
    def reconstruct(self, codegen: PyCodegen): # -> None:
        ...
    


