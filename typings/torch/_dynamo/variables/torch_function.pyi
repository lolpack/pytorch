"""
This type stub file was generated by pyright.
"""

import contextlib
import functools
from typing import TYPE_CHECKING
from torch.overrides import BaseTorchFunctionMode
from .base import VariableTracker
from .ctx_manager import GenericContextWrappingVariable
from .tensor import TensorVariable
from torch._dynamo.codegen import PyCodegen
from torch._dynamo.symbolic_convert import InstructionTranslator

"""TorchDynamo support for __torch_function__ tensor subclasses.

This module implements support for tensor subclasses with __torch_function__ overrides.
A tensor subclass instance is represented as a TensorWithTFOverrideVariable, which handles
dispatching __torch_function__ on attribute accesses, method calls, and torch API calls.

Unsupported features:
- Triggering __torch_function__ on tensor subclass non-tensor custom attributes
- Graph breaking on mutating guardable tensor properties within a __torch_function__ context
  (can cause excessive recompiles in certain cases)
- Matching exact eager behavior of ignoring __torch_function__ objects in non-tensor
  argument positions of Torch API calls

Supported features:
- Static method implementations of __torch_function__ on custom objects (triggers on torch
  API calls with the object as any argument)
- Triggering __torch_function__ on torch API calls with tensor subclass arguments
- __torch_function__ calls on base tensor attribute access and method calls for tensor
  subclass instances
- Matches dispatch ordering behavior of eager __torch_function__ with subclass/object
  arguments in any position

See https://docs.google.com/document/d/1WBxBSvW3NXhRp9ncmtokJloMLCtF4AYNhJaffvHe8Kw/edit#heading=h.vacn73lozd9w
for more information on the design.
"""
if TYPE_CHECKING:
    ...
bin_ops = ...
bin_int_ops = ...
un_int_ops = ...
tensor_and_int_ops = ...
un_ops = ...
BUILTIN_TO_TENSOR_FN_MAP = ...
BUILTIN_TO_TENSOR_RFN_MAP = ...
def populate_builtin_to_tensor_fn_map(): # -> None:
    class GetMethodMode(BaseTorchFunctionMode):
        """
        Mode to extract the correct methods from torch function invocations
        (Used to get the correct torch.Tensor methods from builtins)
        """
        ...
    
    

banned_attrs = ...
@functools.cache
def get_prev_stack_var_name(): # -> str:
    ...

class TorchFunctionModeStackStateManager:
    def __init__(self) -> None:
        ...
    
    def __enter__(self): # -> None:
        ...
    
    def __exit__(self, exc_type, exc_value, traceback): # -> None:
        ...
    
    @contextlib.contextmanager
    def temp_restore_stack(self): # -> Generator[None, Any, None]:
        ...
    


torch_function_mode_stack_state_mgr = ...
class SymbolicTorchFunctionState:
    def __init__(self, py_stack) -> None:
        ...
    
    def in_torch_function_mode(self): # -> bool:
        ...
    
    def pop_torch_function_mode(self): # -> TorchFunctionModeVariable:
        ...
    
    def push_torch_function_mode(self, mode_var): # -> None:
        ...
    
    def call_torch_function_mode(self, tx, fn, types, args, kwargs): # -> VariableTracker:
        ...
    


class TorchFunctionModeStackVariable(VariableTracker):
    """Fake VT to use as a dummy object, indicating the presence of torch function mode stack mutation"""
    stack_value_singleton = ...
    offset = ...
    def __init__(self, source, symbolic_stack) -> None:
        ...
    
    @classmethod
    def reset(cls): # -> None:
        ...
    
    @classmethod
    def register_mutation(cls, tx: InstructionTranslator): # -> None:
        ...
    
    @classmethod
    def register_device_context_insertion(cls, tx: InstructionTranslator): # -> None:
        ...
    
    @classmethod
    def clear_default_device(cls, tx: InstructionTranslator): # -> None:
        ...
    
    @staticmethod
    def is_device_context(var): # -> bool:
        ...
    
    @classmethod
    def get_mode_index(cls, ind):
        ...
    


class TorchFunctionModeVariable(GenericContextWrappingVariable):
    @staticmethod
    def is_supported_torch_function_mode(ty): # -> Any | bool:
        ...
    
    def __init__(self, value, source=..., **kwargs) -> None:
        ...
    
    def reconstruct(self, codegen: PyCodegen): # -> None:
        ...
    
    def module_name(self): # -> str:
        ...
    
    def fn_name(self): # -> str:
        ...
    
    def python_type(self): # -> type[object]:
        ...
    
    def call_torch_function(self, tx: InstructionTranslator, fn, types, args, kwargs): # -> VariableTracker:
        ...
    
    def enter(self, tx): # -> VariableTracker:
        ...
    
    def exit(self, tx: InstructionTranslator, *args): # -> VariableTracker:
        ...
    
    def reconstruct_type(self, codegen: PyCodegen): # -> None:
        ...
    
    def supports_graph_breaks(self): # -> Literal[True]:
        ...
    
    def exit_on_graph_break(self): # -> Literal[False]:
        ...
    


def call_torch_function(tx, torch_function_var, fn, types, args, kwargs):
    ...

def get_torch_function_fn(tx: InstructionTranslator, vt): # -> VariableTracker:
    ...

def can_dispatch_torch_function(tx: InstructionTranslator, args, kwargs): # -> bool:
    ...

def dispatch_torch_function(tx: InstructionTranslator, fn, args, kwargs): # -> VariableTracker | ConstantVariable | Any:
    """Gathers all args that are TensorWithTFOverrideVariable and dispatches based on the ordering in _get_overloaded_args"""
    ...

class TensorWithTFOverrideVariable(TensorVariable):
    """
    Represents a tensor subclass instance with a __torch_function__ override.
    """
    @classmethod
    def from_tensor_var(cls, tx, tensor_var, class_type, cls_source): # -> Self:
        ...
    
    def install_global(self, tx): # -> None:
        ...
    
    def python_type(self):
        ...
    
    def class_type_var(self, tx): # -> TensorSubclassVariable:
        ...
    
    def global_mangled_class_name(self, tx): # -> str:
        ...
    
    def var_getattr(self, tx: InstructionTranslator, name): # -> VariableTracker | UserMethodVariable | UserDefinedClassVariable | DelayGraphBreakVariable | Any | GetAttrVariable:
        ...
    
    def call_torch_function(self, tx: InstructionTranslator, fn, types, args, kwargs): # -> VariableTracker:
        ...
    
    def call_method(self, tx, name, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker:
        ...
    


