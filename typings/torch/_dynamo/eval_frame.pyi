"""
This type stub file was generated by pyright.
"""

import contextlib
import functools
import threading
import types
import torch
import torch.fx
from dataclasses import dataclass
from enum import Enum
from typing import Any, Callable, NamedTuple, Optional, TYPE_CHECKING, Union
from unittest.mock import patch
from torch import _guards
from torch.export.dynamic_shapes import Constraint
from torch.fx import GraphModule
from .backends.registry import CompilerFn
from torch._subclasses import fake_tensor
from .types import DynamoCallback

"""
This module implements the core frame evaluation handler for TorchDynamo's compilation system.
The eval frame handler intercepts Python bytecode execution at runtime to enable dynamic
compilation and optimization of PyTorch code.

Key components defined here:
- Frame evaluation handlers that intercept and analyze Python execution frames
- Guards management for tracking dependencies and invalidating compiled code
- Optimization contexts and decorators (optimize, run_once, disable, etc.)
- Export functionality for saving optimized graphs
- Backend compiler integrations and callback management

Functions in this file are responsible for modifying the eval frame handler at RUNTIME.
Therefore, all functions in this file are hot and performance-critical. Functions that
only execute at compile time should be placed in torch._dynamo.convert_frame.

The eval frame handler is the core mechanism that enables TorchDynamo to dynamically
intercept, analyze and optimize PyTorch code during execution. It works by registering
a custom frame evaluation function that gets called for every Python frame, allowing
us to detect PyTorch operations and trigger compilation as needed.
"""
if TYPE_CHECKING:
    ...
log = ...
always_optimize_code_objects = ...
null_context = contextlib.nullcontext
class Unset(Enum):
    token = ...


cached_backends: dict[int, CompilerFn] = ...
unset = ...
@dataclass
class DynamoStance:
    stance: str = ...
    skip_guard_eval_unsafe: bool = ...
    backend: Union[str, Callable[..., Any], None] = ...


_stance = ...
_EXAMPLE_INPUTS: Optional[dict[str, list[Any]]] = ...
def get_example_inputs(key) -> list[Any]:
    ...

DONT_WRAP_FILES = ...
class OptimizedModule(torch.nn.Module):
    """
    Wraps the original nn.Module object and later patches its
    forward method to optimized self.forward method.
    """
    _torchdynamo_orig_callable: Callable[..., Any]
    get_compiler_config: Callable[[], Any]
    _opt_mod_attributes = ...
    def __init__(self, mod: torch.nn.Module, dynamo_ctx) -> None:
        ...
    
    def __call__(self, *args, **kwargs): # -> Any:
        ...
    
    def __reduce__(self): # -> tuple[type[Self], tuple[Module, Any]]:
        ...
    
    def __getstate__(self): # -> dict[str, Any]:
        ...
    
    def __setstate__(self, state): # -> None:
        ...
    
    @property
    def training(self): # -> bool:
        ...
    
    @training.setter
    def training(self, value): # -> None:
        ...
    
    def __getattr__(self, name): # -> Module | Any | None:
        ...
    
    def __setattr__(self, name, val) -> None:
        ...
    
    def __delattr__(self, name): # -> None:
        ...
    
    def __dir__(self): # -> list[str]:
        ...
    


def remove_from_cache(f): # -> None:
    """
    Make sure f.__code__ is not cached to force a recompile
    """
    ...

def nothing(): # -> None:
    ...

def always_false(): # -> Literal[False]:
    ...

def innermost_fn(fn): # -> Callable[..., object]:
    """
    In case of nesting of _TorchDynamoContext calls, find the innermost
    function. TorchDynamo caches on fn.__code__ object, so its necessary to find
    the innermost function to pass on the optimize, run, disable etc.
    """
    ...

def make_set_enable_dynamic(enable: bool): # -> None:
    ...

class DynamoTLS(threading.local):
    traced_frame_infos: list[str] = ...


dynamo_tls = ...
def clear_dynamo_tls(): # -> None:
    ...

def guard_collectives_hook(guard_eval_result): # -> bool:
    ...

_not_set = ...
class _TorchDynamoContext:
    def __init__(self, callback: DynamoCallback, on_enter=..., backend_ctx_ctor=..., patch_fn=..., first_ctx=..., *, error_on_graph_break=..., export=..., dynamic=..., compiler_config=..., package=...) -> None:
        ...
    
    def __enter__(self): # -> None:
        ...
    
    def __exit__(self, exc_type, exc_val, exc_tb): # -> None:
        ...
    
    def __call__(self, fn): # -> OptimizedModule | type[Module] | type[Any] | _Wrapped[Callable[..., Any], object, Callable[..., Any], object]:
        ...
    


class OptimizeContext(_TorchDynamoContext):
    def __init__(self, callback, backend_ctx_ctor, first_ctx=..., *, error_on_graph_break=..., export=..., dynamic=..., compiler_config=..., rebuild_ctx: Optional[Callable[[], Union[OptimizeContext, _NullDecorator]]] = ..., package=...) -> None:
        ...
    
    def __reduce__(self): # -> tuple[type[Self], tuple[DynamoCallback, null_context, bool], dict[str, Any]]:
        ...
    


class RunOnlyContext(_TorchDynamoContext):
    def __init__(self) -> None:
        ...
    
    def __reduce__(self): # -> tuple[type[Self], tuple[()]]:
        ...
    


class DisableContext(_TorchDynamoContext):
    def __init__(self, msg: Optional[str] = ..., wrapping: bool = ...) -> None:
        ...
    
    def __call__(self, fn): # -> OptimizedModule | type[Module] | type[Any] | _Wrapped[Callable[..., Any], object, Callable[..., Any], object] | Callable[..., object]:
        ...
    
    def __reduce__(self): # -> tuple[type[Self], tuple[()]]:
        ...
    


def get_compiler_fn(compiler_fn): # -> WrapBackendDebug:
    ...

class _NullDecorator(contextlib.nullcontext):
    def __call__(self, fn): # -> Callable[..., object]:
        ...
    


def check_if_dynamo_supported(): # -> None:
    ...

def is_dynamo_supported(): # -> bool:
    ...

def check_if_inductor_supported(): # -> None:
    ...

def is_inductor_supported(): # -> bool:
    ...

def check_for_incompatible_configs(): # -> None:
    ...

def optimize(*args, **kwargs): # -> OptimizeContext | _NullDecorator:
    ...

@patch("torch._dynamo.symbolic_convert.explain", True)
def explain(f, *extra_args, **extra_kwargs): # -> ExplainOutput | Callable[..., ExplainOutput]:
    ...

class FlattenInputOutputSignature(torch.fx.Transformer):
    def __init__(self, m: torch.fx.GraphModule, flat_args: tuple[Any], matched_input_elements_positions: list[int], flat_results: list[Any], matched_output_elements_positions: list[int], example_fake_inputs: list[torch.Tensor], flat_args_dynamic_dims: list[set[int]], fake_mode: Optional[fake_tensor.FakeTensorMode] = ...) -> None:
        ...
    
    def placeholder(self, target, args, kwargs):
        ...
    
    def output(self, target, args, kwargs): # -> Any:
        ...
    
    def run_node(self, n): # -> Any:
        ...
    
    def transform(self): # -> GraphModule:
        ...
    


class ExportResult(NamedTuple):
    graph_module: torch.fx.GraphModule
    guards: _guards.GuardsSet
    ...


def check_signature_rewritable(graph): # -> None:
    ...

def rewrite_signature(f_sig, graph, fake_mode, flat_args, in_spec, example_fake_inputs, graph_captured_input, graph_captured_output, dynamo_traced_result, flat_args_dynamic_dims): # -> GraphModule:
    ...

def export(f: Callable[..., Any], *extra_args, aten_graph: bool = ..., pre_dispatch: bool = ..., decomposition_table: Optional[dict[torch._ops.OpOverload, Callable[..., Any]]] = ..., tracing_mode: str = ..., dynamic_shapes: Optional[Union[dict[str, Any], tuple[Any], list[Any]]] = ..., specialize_float: bool = ..., assume_static_by_default: bool = ..., same_signature: bool = ..., disable_constraint_solver: bool = ..., prefer_deferred_runtime_asserts_over_guards: bool = ..., allow_complex_guards_as_runtime_asserts: bool = ..., _log_export_usage: bool = ..., constraints: Optional[list[Constraint]] = ..., **extra_kwargs) -> Callable[..., ExportResult]:
    """
    Export an input function f to a format that can be executed outside of PyTorch using the FX graph.

    Args:
        f (callable): A PyTorch function to be exported.

        aten_graph (bool): If True, exports a graph with ATen operators.
        If False, exports a graph with Python operators. Default is False.

        pre_dispatch (bool): If True, exports a graph with ATen operators,
        but before any logic in the PyTorch dispatcher has run.
        This can be useful if you want to apply further transformations on a graph before running it
        through autograd, autocast, or any other functionalities that are integrated into the dispatcher.
        This flag is only valid if aten_graph=True is set.
        Default is False.

        decomposition_table (dict): A dictionary that maps operators to their decomposition functions.
        Required if aten_graph or tracing_mode is specified. Default is None.

        tracing_mode (str): If "symbolic", turn on dynamic shapes support. Default is "symbolic".

        dynamic_shapes:
         An optional argument where the type should either be:
         1) a dict from argument names of ``f`` to their dynamic shape specifications,
         2) a tuple that specifies dynamic shape specifications for each input in original order.
         If you are specifying dynamism on keyword args, you will need to pass them in the order that
         is defined in the original function signature.

         The dynamic shape of a tensor argument can be specified as either
         (1) a dict from dynamic dimension indices to :func:`Dim` types, where it is
         not required to include static dimension indices in this dict, but when they are,
         they should be mapped to None; or (2) a tuple / list of :func:`Dim` types or None,
         where the :func:`Dim` types correspond to dynamic dimensions, and static dimensions
         are denoted by None. Arguments that are dicts or tuples / lists of tensors are
         recursively specified by using mappings or sequences of contained specifications.

        same_signature (bool): If True, rewrite the returned graph's signature to be the same as f.

        disable_constraint_solver (bool): Whether the dim constraint solver must be disabled.

    Returns:
        A function that given args and kwargs, returns a tuple of (graph, guards)
        Graph: An FX graph representing the execution of the input PyTorch function with the provided arguments and options.
        Guards: The guards we accumulated during tracing f above

    Raises:
        AssertionError: If decomposition_table is specified without setting aten_graph=True,
        or if graph breaks during tracing in export.

        AssertionError: If Dynamo input and output is not consistent with traced input/output.

    Note - this headerdoc was authored by ChatGPT, with slight modifications by the author.
    """
    ...

def optimize_assert(*args, **kwargs): # -> OptimizeContext:
    ...

class TorchPatcher:
    @staticmethod
    @functools.cache
    def patch(): # -> None:
        ...
    
    @staticmethod
    def suppress_torch_distributed_warnings(fn): # -> Callable[..., Any]:
        ...
    


def skip_code(code: types.CodeType): # -> None:
    ...

