"""
This type stub file was generated by pyright.
"""

import torch
from typing import Optional, TYPE_CHECKING, Union
from torch._dynamo.source import GetItemSource

"""
Provides functionality for compiling PyTorch's autograd (automatic differentiation) system.

This module implements compiled autograd, which traces and optimizes backward pass
computations at runtime. The key components are:

- AutogradCompilerInstance: Traces and compiles autograd graphs using FX
- Context managers (_enable/_disable): Control when compiled autograd is active
- Utility functions: Support graph manipulation, tensor operations, and hooks

Compiled autograd can significantly improve backward pass performance by removing
Python overhead and enabling additional optimizations. It works by capturing
backward computations into an FX graph that can be compiled and optimized,
while maintaining the same semantics as eager mode autograd.
"""
if TYPE_CHECKING:
    ...
TURN_OFF_MSG = ...
compiled_autograd_log = ...
verbose_log = ...
def snapshot_verbose_logging_enabled():
    ...

def snapshot_cudagraph_enabled():
    ...

def maybe_clone(x): # -> Tensor:
    ...

def extract_bw_module(CompiledFunction): # -> Callable[..., Any]:
    ...

class NaNChecker:
    def __init__(self, accumulate_grad: bool) -> None:
        ...
    
    def prep_with_graph(self, graph: torch.fx.Graph): # -> None:
        ...
    
    def prep_with_inputs(self, inputs: tuple[torch.Tensor]): # -> None:
        ...
    
    def check(self, out: tuple[torch.Tensor]): # -> None:
        ...
    


class OpNamespace:
    def __init__(self) -> None:
        ...
    
    def add(self, name, fn, is_custom_function, is_traceable): # -> str:
        ...
    
    def get(self, name): # -> Any:
        ...
    


class Op:
    def __init__(self, name, fn, is_custom_function) -> None:
        ...
    
    def __call__(self, *args, **kwargs):
        ...
    
    def __repr__(self): # -> str:
        ...
    


ops = ...
_graph_placeholders = ...
_impure_targets = ...
COMPILE_COUNTER = ...
def make_compile_context(compiled_autograd_id): # -> _GeneratorContextManager[CompileContext | None, None, None]:
    ...

class AutogradCompilerInstance:
    def __init__(self, compiler_fn) -> None:
        ...
    
    def wrap_fake(self, x, source): # -> FakeTensor:
        ...
    
    @staticmethod
    def source(name, idx) -> GetItemSource:
        ...
    
    def begin_capture(self, inputs: list[torch.Tensor], sizes: list[int], scalars: list[Union[int, float]], origins: list[list[tuple[int, str]]], accumulate_grad: bool, check_nans: bool): # -> tuple[str, list[Tensor], list[int], list[int | float]]:
        ...
    
    def log_compile_reasons(self, compile_reasons: list[str]): # -> None:
        ...
    
    def proxy_call_aot_backward(self, pinputs, psaved_tensors, saved_tensors, pctx, ctx, maybe_backward_state_idx): # -> PyTree:
        ...
    
    def proxy_call_backward(self, inputs, output_metadatas, saved_tensors, backward_idx: int, ctx: torch.autograd.function.BackwardCFunction, maybe_backward_state_idx: Optional[int]): # -> tuple[Tensor | None, ...]:
        ...
    
    def call_copy_slices_prologue(self, inputs, base_sizes, base_strides, base_storage_offset, view_sizes, view_strides, view_storage_offset): # -> list[Tensor]:
        ...
    
    def call_copy_slices_epilogue(self, needs_input_grad, result, res, grad_slice): # -> list[Tensor]:
        ...
    
    def allocate_dummy(self): # -> Tensor:
        ...
    
    def bind_function(self, fn_name, fn, is_custom_function, is_traceable): # -> str:
        """Binds ops.fn_name = fn"""
        ...
    
    def apply_functional(self, fn_name, grads, args, output_metadata): # -> list[Tensor]:
        """Proxies a call to ops.fn_name(grads, *args) into the graph"""
        ...
    
    def proxy_call(self, fn, args, output_metadata): # -> list[Tensor]:
        """Proxies a call to fn(*args) into the graph"""
        ...
    
    def validate_outputs(self, _, outputs, args, output_metadata):
        """Proxies a call to ops.validate_outputs(outputs, *args) into the graph"""
        ...
    
    def accumulate(self, old_var, new_var): # -> Tensor:
        ...
    
    def accumulate_grad(self, variable, grad, has_post_hooks): # -> None:
        ...
    
    def proxy_call_hook(self, hook, *args, **kwargs): # -> Proxy:
        ...
    
    def unpack_hook(self, hook_id, data_id): # -> Tensor:
        ...
    
    def tensor_pre_hook(self, inputs, hook_id, i: int):
        ...
    
    def cpp_tensor_pre_hook(self, inputs: list[torch.Tensor], hook_id: int, i: int): # -> list[Tensor]:
        ...
    
    def pre_hook(self, inputs, hook_id): # -> list[Tensor | Any]:
        ...
    
    def post_hook(self, outputs, inputs, hook_id): # -> list[Tensor | Any]:
        ...
    
    def post_acc_grad_hook(self, input, hook_id): # -> list[Tensor]:
        ...
    
    def move_graph_nodes_to_cuda(self, graph) -> list[int]:
        ...
    
    def is_sym_node(self, node): # -> bool:
        ...
    
    def dce(self): # -> None:
        ...
    
    def remove_unused_sizes(self): # -> set[int]:
        ...
    
    def create_graph_module(self, id): # -> GraphModule:
        ...
    
    def end_capture(self, outputs): # -> tuple[Callable[..., Any], Any]:
        ...
    
    @staticmethod
    def get_all_nodes(args): # -> list[Node]:
        ...
    
    @staticmethod
    def is_placeholder(node): # -> bool:
        ...
    
    def reorder_accumulate_grad_nodes(self): # -> None:
        """
        Usage of AOTAutograd causes all the accumulate_grad_ nodes to get pushed to the end of
        the graph.  This differs from eager mode, which schedules them as soon as possible. This
        pass attempts to reorder the graph to mimic eager behavior.
        """
        ...
    
    def delay_unpack_hook_nodes(self): # -> None:
        """
        We can delay unpack hooks until they are needed, even later than in the eager autograd engine.
        """
        ...
    
    def reorder_tensor_pre_hook_nodes(self): # -> None:
        """
        Usage of AOTAutograd causes all the tensor_pre_hook nodes to get pushed
        to the end of the graph. This differs from eager mode, which schedules
        them as soon as possible. This pass attempts to reorder the graph to
        mimic eager behavior.
        """
        ...
    
    def reorder_pre_hook_nodes_to_schedule_asap(self): # -> None:
        """
        In this function, we schedule the pre hooks as soon as possible. This
        does not match eager behavior (schedule pre hook right before its
        registered node), but it can make acc grad be scheduled properly when
        the pre hooks are registered to them. After reordering acc grad node, we
        will reorder the pre hooks again to mimic eager behavior.
        """
        ...
    
    def reorder_pre_hook_nodes_to_mimic_eager(self): # -> None:
        """
        Usage of AOTAutograd causes all the pre_hook nodes to get pushed to the
        end of the graph. This differs from eager mode, which schedules them
        right before their registered node execution. This pass attempts to
        reorder the graph to mimic eager behavior.
        """
        ...
    
    def reorder_post_acc_grad_hook_nodes(self): # -> None:
        """
        Usage of AOTAutograd causes all the post_acc_grad_hook nodes to get
        pushed to the end of the graph. This differs from eager mode, which
        schedules them as soon as possible. This pass attempts to reorder the
        graph to mimic eager behavior.
        """
        ...
    
    def reorder_post_hook_nodes(self): # -> None:
        """
        Usage of AOTAutograd causes all the post_hook nodes to get pushed to the
        end of the graph. This differs from eager mode, which schedules them as
        soon as possible. This pass attempts to reorder the graph to mimic eager
        behavior.
        """
        ...
    
    def to_proxy(self, t): # -> list[Any] | tuple[Any, ...] | Proxy | None:
        ...
    
    def bind_objects_to_proxies(self, objects, proxies, origins: Optional[list[tuple[int, str]]] = ...): # -> list[Any]:
        ...
    
    def bind_backward_state(self, index: int): # -> BackwardState:
        ...
    
    def set_node_origin(self, node_name: str, nodecall_index: int, pyobj: Optional[torch.autograd.Function]): # -> None:
        ...
    


compiled_autograd_enabled = ...
compiled_autograd_enabled_force_eager = ...
in_compiled_autograd_region = ...
active_disable_ctx = ...
depth = ...
def reset() -> None:
    ...

def copy_slices_prologue(inputs, base_sizes, base_strides, base_storage_offset, view_sizes, view_strides, view_storage_offset): # -> list[Any]:
    ...

def copy_slices_epilogue(needs_input_grad, result, res, grad_slice): # -> list[None]:
    ...

