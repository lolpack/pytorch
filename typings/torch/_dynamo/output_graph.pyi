"""
This type stub file was generated by pyright.
"""

import contextlib
import traceback
import sympy
import torch._guards
import torch.nn
from dataclasses import dataclass
from typing import Any, Callable, Optional, TYPE_CHECKING, Union
from torch import Tensor, fx
from torch._C._dynamo import guards
from torch._guards import Source
from .backends.registry import CompiledFn, CompilerFn
from .bytecode_transformation import Instruction
from .side_effects import SideEffects
from .variables.base import VariableTracker
from .variables.builder import GraphArg
from torch._dynamo.symbolic_convert import InstructionTranslatorBase

"""
Core graph building functionality for PyTorch's Dynamo system. This module contains
the essential components for constructing and managing FX graphs during compilation:

- OutputGraph: Manages the overall graph construction and compilation process. It owns
  a SubgraphTracer and handles graph compilation, execution, and state management.
  OutputGraph also manages features like graph deduplication, symbolic shape handling,
  and tracking of side effects.

- SubgraphTracer: Handles the actual FX graph construction by tracing Python code.
  It supports advanced features like higher-order operators through nested tracers,
  lifting of free variables, and handling of symbolic shapes.

The module supports key Dynamo features including:
- Higher-order operators through nested SubgraphTracers
- Graph deduplication for optimization
- Symbolic shape handling and propagation
- Side effect tracking and management
- Guard insertion and management
"""
if TYPE_CHECKING:
    ...
log = ...
graph_tabular_log = ...
graph_code_log = ...
graph_sizes_log = ...
trace_call_log = ...
RootGuardManager = guards.RootGuardManager
@dataclass(frozen=True)
class VariableTrackerCacheKey:
    vt_id: int
    source: Source
    ...


@dataclass(frozen=True)
class AliasingInfo:
    has_aliasing: bool
    msg: str
    ...


@dataclass(frozen=True)
class MutationInfo:
    has_mutation: bool
    msg: str
    ...


class VariableTrackerCache:
    def __init__(self) -> None:
        ...
    
    def lookup(self, value, source): # -> None:
        ...
    
    def add(self, value, source, vt): # -> None:
        ...
    
    def clone(self): # -> VariableTrackerCache:
        ...
    
    def clear(self): # -> None:
        ...
    


@dataclass
class GraphCompileReason:
    """Stores why a given output graph was compiled; i.e. what caused the graph break."""
    reason: str
    user_stack: list[traceback.FrameSummary]
    graph_break: bool = ...
    def __post_init__(self): # -> None:
        ...
    


class FakeRootModule(torch.nn.Module):
    """Trick the constructor of fx.GraphModule"""
    def __init__(self, nn_modules: dict[str, torch.nn.Module]) -> None:
        ...
    
    def __repr__(self) -> str:
        ...
    
    def add_nn_modules(self, nn_modules: dict[str, torch.nn.Module]): # -> None:
        ...
    


class WrapperBackend:
    def __init__(self, backend: CompilerFn) -> None:
        ...
    
    def __call__(self, gm: torch.fx.GraphModule, example_inputs: list[torch.Tensor]): # -> Callable[..., Any] | CompiledFn:
        ...
    


Scope = dict[str, object]
@dataclass
class OutputGraphGuardsState:
    """
    A base class containing fields that are considered "persistent" when we
    want to save all the important state for reconstrucing guards in a different
    process. Normally we don't need to add states here, but we may have to when
    the information is needed to serialize the guards, so the fields here are
    supposed to be serializable as a requirement.
    """
    local_scope: Scope
    global_scope: Scope
    torch_function_mode_stack: list[torch.overrides.TorchFunctionMode]
    guard_on_key_order: set[Source]
    input_source_to_sizes_strides: dict[Source, dict[str, Any]]
    dual_level: int
    functorch_layers: list[torch._functorch.pyfunctorch.FuncTorchInterpreter]
    current_device: Optional[torch.device]
    export: bool = ...
    export_constraints: bool = ...
    _guards: Optional[torch._guards.GuardsSet] = ...
    _aotautograd_guards: Optional[list[torch._guards.GuardEnvExpr]] = ...
    @property
    def shape_env(self):
        ...
    
    @property
    def guards(self): # -> GuardsSet | None:
        ...
    
    @property
    def aotautograd_guards(self): # -> list[GuardEnvExpr] | None:
        ...
    


@dataclass
class StackLocalsMetadata:
    """
    Stores metadata for a frame's stack and locals for the purposes of building resume functions
    """
    stack_null_idxes: list[int] = ...
    locals_null_keys: list[str] = ...
    stack_ctx_args: list[tuple[int, tuple[Any, ...]]] = ...
    stack_ctx_idxes_orig: list[int] = ...
    locals_ctx_args: list[tuple[str, tuple[Any, ...]]] = ...


class OutputGraph(OutputGraphGuardsState):
    """
    Wrapper class to hold outputs of InstructionTranslator.  Mainly the
    generated fx.Graph.

    OutputGraph is 1:1 with a frame being processed. Each frame is associated
    with some root InstructionTranslator. When user code calls a function,
    we construct a InliningInstructionTranslator that continues to write into
    the root InstructionTranslator's OutputGraph.
    """
    side_effects: SideEffects
    def __init__(self, code_options: dict[str, Any], compiler_fn: Optional[CompilerFn], root_tx, export: bool, export_constraints, frame_state, local_scope: Scope, global_scope: Scope, f_code, torch_function_mode_stack, package) -> None:
        ...
    
    def mark_bytecode_tracing_start(self): # -> None:
        ...
    
    def mark_bytecode_tracing_stop(self): # -> None:
        ...
    
    def install_builtins_dict_in_fglobals(self): # -> str:
        ...
    
    def add_backward_state_hook(self, hook: VariableTracker, prefix=...): # -> tuple[str, Proxy]:
        ...
    
    def get_backward_state_proxy(self): # -> Proxy:
        ...
    
    def init_ambient_guards(self): # -> None:
        ...
    
    def maybe_install_saved_tensors_hooks_subgraphs(self) -> Optional[list[str]]:
        ...
    
    def dump_guards_state(self): # -> OutputGraphGuardsState:
        ...
    
    def synthetic_graph_input(self, fn, args): # -> Any:
        """
        call fn(*args) before the graph runs and turn the result into a fake input.
        """
        ...
    
    def add_cleanup_hook(self, fn: Callable[[], Any]): # -> None:
        ...
    
    def call_cleanup_hooks(self): # -> None:
        ...
    
    @property
    def root_tracer(self): # -> SubgraphTracer:
        ...
    
    @property
    def current_tracer(self): # -> SubgraphTracer:
        ...
    
    def is_root_tracer(self): # -> bool:
        ...
    
    @property
    def graph(self): # -> Graph:
        ...
    
    @graph.setter
    def graph(self, value): # -> None:
        ...
    
    @property
    def input_name_to_proxy(self): # -> dict[str, Proxy]:
        ...
    
    @property
    def real_value_cache(self): # -> dict[Node, Tensor]:
        ...
    
    @property
    def bound_symbols(self): # -> dict[Symbol, Proxy | LazyProxy]:
        ...
    
    def create_proxy(self, *args, **kwargs): # -> Proxy:
        ...
    
    def create_node(self, *args, **kwargs): # -> Node:
        ...
    
    def remove_node(self, *args, **kwargs): # -> None:
        ...
    
    @contextlib.contextmanager
    def subtracer(self, source_target, prior_tracer): # -> Generator[Any | SubgraphTracer, Any, None]:
        ...
    
    @property
    def output(self): # -> Self:
        ...
    
    @property
    def fake_mode(self): # -> Any:
        ...
    
    @property
    def shape_env(self):
        ...
    
    @property
    def guards(self) -> torch._guards.GuardsSet:
        ...
    
    @property
    def nn_modules(self) -> dict[str, Any]:
        ...
    
    @property
    def aotautograd_guards(self): # -> list[GuardEnvExpr]:
        ...
    
    def save_global_state(self, out=...): # -> None:
        """
        Saves to out if it is provided. Else saves to the tracing context's global_state.
        """
        ...
    
    def push_tx(self, tx): # -> None:
        ...
    
    def pop_tx(self): # -> InstructionTranslatorBase:
        ...
    
    @property
    def current_tx(self): # -> InstructionTranslatorBase | None:
        ...
    
    def count_calls(self): # -> int:
        ...
    
    def is_empty_graph(self): # -> bool:
        ...
    
    def get_submodule(self, keys): # -> dict[str, Module] | Module:
        ...
    
    def new_var(self, name=...): # -> str:
        ...
    
    def update_co_names(self, name): # -> None:
        """Ensure self.code_options.co_names contains name"""
        ...
    
    @staticmethod
    def module_key_name(*names): # -> str:
        ...
    
    def register_static_attr_and_return_proxy(self, attr_prefix: str, attr_value: Any) -> fx.Proxy:
        ...
    
    def register_attr_or_module(self, target: Union[torch.nn.Module, torch.Tensor, Any], *names, **options): # -> Any:
        ...
    
    def handle_aliases_for_stolen_lists(self, tx): # -> tuple[list[Any], dict[Any, Any]] | tuple[list[Any], dict[Source, Source]]:
        ...
    
    def compile_subgraph(self, tx: InstructionTranslatorBase, reason: GraphCompileReason, partial_convert=..., stack_pops=...): # -> list[Any]:
        """
        Compiles the current subgraph, with inputs w.r.t. self.root_tx, and codegens:
            - Call the compiled subgraph
            - Apply side effects
            - Codegen stack and locals
            - Store the locals

        Python does not allow NULL to be an arg to a function, so we do not codegen NULLs on the stack,
        unless the value is one of the top `stack_pops` values on the stack (these values are expected to be
        popped immediately after this generated code. The prologue of the resume function is expected to restore
        any dropped NULLs.

        Returns stack indices and locals keys where we dropped NULLs, and where we found inactive context manager objects.
        """
        ...
    
    def codegen_suffix(self, tx, stack_values, cg): # -> None:
        ...
    
    def cleanup_graph(self): # -> None:
        """
        Remove "creation_timestamp" from node meta

        Remove this pattern from the graph:
            torch._C._set_grad_enabled(False)
            torch._C._set_grad_enabled(True)
        """
        ...
    
    def get_graph_sizes_structured(self): # -> dict[Any, Any]:
        ...
    
    def get_graph_sizes(self, name: str): # -> str:
        ...
    
    @contextlib.contextmanager
    def restore_global_state(self): # -> Generator[None, Any, None]:
        """
        Momentarily restores the global state to what it was prior to tracing the current output
        """
        ...
    
    def run_compiler_collective(self): # -> None:
        ...
    
    def compile_and_call_fx_graph(self, tx, rv, root): # -> list[DataclassInstance]:
        """
        Generate code from self.graph and return the Instruction()s to
        call that generated code.

        Code is generated w.r.t. self.root_tx.
        tx is only used for preserving GraphModule metadata
        """
        ...
    
    @property
    def placeholders(self) -> list[fx.Node]:
        ...
    
    @property
    def graphargs(self) -> list[GraphArg]:
        ...
    
    def call_user_compiler(self, gm: fx.GraphModule, example_inputs: list[Tensor]) -> CompiledFn:
        ...
    
    def dedup_pass(self): # -> dict[str, GraphModule] | dict[Any, Any]:
        ...
    
    def install_subgraph(self, name, sub_gm): # -> str:
        ...
    
    def example_inputs(self) -> list[torch.Tensor]:
        ...
    
    def remove_unused_get_attr_nodes(self) -> None:
        ...
    
    def remove_unused_graphargs(self) -> None:
        ...
    
    def remove_tensorify_specialized_graphargs(self) -> None:
        ...
    
    def add_output_instructions(self, prefix: list[Instruction]) -> None:
        """
        We call this on the creation of a new compiled subgraph that is inserted
        before user code.
        """
        ...
    
    def install_global_unsafe(self, name, value) -> None:
        """
        WARNING: prefer the safer `install_global_by_id/install_global`.
        torch.compile instances should be independent of each other;
        one footgun is to have one instance depend on the existence of
        a global installed by another instance. This can happen if we mangle
        a global the same way across both instances.
        """
        ...
    
    def install_global_by_id(self, prefix, value) -> str:
        """
        Installs a global if it hasn't been installed already.
        This is determined by (prefix, id(value)) pair.

        Returns the name of the newly installed global.
        """
        ...
    
    def install_global(self, prefix, value) -> str:
        """
        Installs a global, generating a unique name for it.

        Returns the name of the newly installed global.
        """
        ...
    
    def cleanup(self) -> None:
        ...
    
    def add_graph_finalizer(self, register_finalizer: Callable[[fx.GraphModule], None]) -> None:
        ...
    
    def example_value_from_input_node(self, node: torch.fx.Node): # -> Any:
        """Extract the non-fake example tensor"""
        ...
    


err_epilogue = ...
def check_pt2_compliant_op(output_graph, kind, target, args, kwargs): # -> None:
    ...

_compile_id_counter = ...
class LazyProxy:
    def __init__(self, tracer, fn, *args, **kwargs) -> None:
        ...
    
    def __call__(self):
        ...
    


class SubgraphTracer(fx.Tracer):
    """
    Holds an FX graph that is being traced. OutputGraph owns a SubgraphTracer
    and the separation of responsibilities is that SubgraphTracer is
    responsible for building the graph while OutputGraph is responsible for
    compiling and executing the graph.
    """
    def __init__(self, output_graph, parent=..., is_export=..., source_target=...) -> None:
        ...
    
    def create_proxy(self, kind, target, args, kwargs, name=..., type_expr=..., proxy_factory_fn=...): # -> Proxy:
        ...
    
    def create_node(self, op, target, args=..., kwargs=..., name=..., type_expr=...): # -> Node:
        ...
    
    def remove_node(self, node): # -> None:
        ...
    
    def create_graph_input(self, name, type_expr, example_value, before=..., source=...): # -> Proxy:
        ...
    
    def lift_tracked_freevar_to_input(self, proxy): # -> Proxy | LazyProxy:
        ...
    
    def maybe_lift_tracked_freevar_to_input(self, arg): # -> slice[Any, slice[Any, Any, Any] | Any | Proxy | LazyProxy, Any] | Proxy | LazyProxy:
        """
        If arg is a free variable, then lift it to be an input.
        Returns the new lifted arg (if arg was a freevar), else the
        original arg.
        """
        ...
    
    def track_unbacked_symbols(self, example_value, e_proxy: Union[LazyProxy, torch.fx.Proxy]): # -> None:
        ...
    
    def lookup_unbound_symbols(self, s: torch.SymInt) -> list[sympy.Symbol]:
        ...
    
    def has_input_mutation(self): # -> MutationInfo:
        ...
    
    def has_aliasing(self): # -> AliasingInfo:
        ...
    


