"""
This type stub file was generated by pyright.
"""

import torch
from typing import Any, Optional
from torch._ops import HigherOrderOperator, OpOverload
from torch._subclasses import FakeTensorMode
from torch.fx.experimental._backward_state import BackwardState
from torch.fx.experimental.proxy_tensor import ProxyTorchDispatchMode
from torch.overrides import TorchFunctionMode

"""trace_wrapped(*args, fn) is equivalent to fn(*args), but with a twist:
if you make_fx trace through this call, we will not actually trace into fn; instead,
we will directly insert it as a call_function to fn in the graph.
(Unlike make_fx, Dynamo WILL inline into fn.)
You can think of this as a one off allow_in_graph equivalent for proxy tensor tracing.

Because proxy tensor tracing does not actually run the function, there are
requirements on the behavior of fn. We are still figuring it out, but here is the current state:

1) fn SHOULD only take a single argument, which must be a tensor
2) fn MUST return a new tensor with the same metadata as the original tensor
   (e.g., zeros_like(input) is a permissible implementation of fn).
   This is verified via an extra assert that is inserted into the traced graph.
3) fn MAY have side effects, but it MAY NOT perform metadata mutation on other tensors
   participating in proxy tensor tracing (it MAY mutate other tensors, it MAY mutate Python state)
These requirements stem from the requirement that we need to continue performing proxy tensor tracing,
which assumes accurate fake tensor metadata, without actually running fn.
In the future, we may allow for a "meta" function associated with fn to allow for more interesting input-output patterns.

Note that tensors / Python state are allowed to be mutated.
This is relaxed constraint is not always sound, but it is sound for backward tracing with fake
tensors as it takes place in AOTAutograd, as the backward pass is guaranteed not to depend on concrete
tensor values (via fake tensor) or Python state (because the autograd engine doesn't depend on Python).

The intended use case for this function is to allow AOTAutograd to defer complex
backward hooks to compiled autograd. AOTAutograd performs a make_fx trace which preserves
the function call as is in the graph, and only when we Dynamo through the backward graph in
compiled autograd do we inline into the function.
"""
Tensor = torch.Tensor
__all__ = ["trace_wrapped"]
if not torch._running_with_deploy():
    @torch.library.custom_op("flex_lib::zeros_and_scatter", mutates_args=())
    def zeros_and_scatter(shape: list[int], indices: list[Tensor], vals: Tensor) -> Tensor:
        """Custom Op so that we can register a custom lowering for the new_output + scatter in the backwards pass"""
        ...
    
    @zeros_and_scatter.register_fake
    def _(shape: list[int], indices: list[Tensor], vals: Tensor) -> Tensor:
        ...
    
    @zeros_and_scatter.register_vmap
    def _(info, indims, shape, indices, value): # -> tuple[Any, None]:
        """The batching rule is special in that it returns a tensor that is not batched"""
        ...
    
class ModIndex(torch.autograd.Function):
    generate_vmap_rule = ...
    @staticmethod
    def forward(x: Tensor, indices: list[Tensor]) -> Tensor:
        ...
    
    @staticmethod
    def setup_context(ctx: Any, inputs: tuple[Any, ...], output: Any) -> None:
        ...
    
    @staticmethod
    def backward(ctx, gradOut): # -> tuple[Any, None]:
        ...
    


mod_index = ...
class TransformGetItemToIndex(TorchFunctionMode):
    def __torch_function__(self, func: OpOverload, types: tuple[torch._C._TensorMeta, ...], args: tuple[object, ...] = ..., kwargs: Optional[dict[str, object]] = ...) -> object:
        ...
    


def trace_wrapped(*args: Any, **kwargs: Any) -> Any:
    ...

class TraceWrapped(HigherOrderOperator):
    def __init__(self) -> None:
        ...
    
    def __call__(self, *args: Any, **kwargs: Any) -> Any:
        ...
    


_trace_wrapped_op = ...
@_trace_wrapped_op.py_impl(ProxyTorchDispatchMode)
def inner_trace(mode: ProxyTorchDispatchMode, *args: Any, bw_state: Optional[BackwardState] = ..., **kwargs: Any) -> Any:
    ...

@_trace_wrapped_op.py_impl(FakeTensorMode)
def inner_fake(*args: Any, **kwargs: Any) -> None:
    ...

def autograd_function_backward_rewritten(original_backward: Any) -> Any:
    ...

