"""
This type stub file was generated by pyright.
"""

import torch
from typing import Any, Optional, TYPE_CHECKING, Union
from ..scheduler import BaseSchedulerNode, BaseScheduling, FusedSchedulerNode, Scheduler, SchedulerNode
from collections.abc import Sequence
from typing_extensions import TypeAlias
from sympy import Expr
from torch.utils._ordered_set import OrderedSet
from .common import BackendFeature

if TYPE_CHECKING:
    _IntLike: TypeAlias = Union[int, Expr]
class CUDACombinedScheduling(BaseScheduling):
    """
    Scheduler for CUDA Kernels, which delegates calls as appropriate
    to the CUDA-C++ and Triton Schedulers, which both work for CUDA devices
    and use a unified-wrapper for codegen.

    If Scheduling code needs to be specialized for the case of mixed Triton / CUDA C++ code,
    this would also be the place to do it.
    """
    def __init__(self, scheduler: Optional[Scheduler]) -> None:
        ...
    
    def get_backend_features(self, device: torch.device) -> OrderedSet[BackendFeature]:
        ...
    
    def choose_node_backend(self, node: BaseSchedulerNode) -> BaseScheduling:
        ...
    
    def can_fuse_vertical(self, node1: BaseSchedulerNode, node2: BaseSchedulerNode) -> bool:
        ...
    
    def can_fuse_horizontal(self, node1: BaseSchedulerNode, node2: BaseSchedulerNode) -> bool:
        ...
    
    def group_fn(self, sizes: Sequence[Sequence[_IntLike]]) -> tuple[tuple[_IntLike, ...], ...]:
        ...
    
    def codegen_template(self, template_node: BaseSchedulerNode, epilogue_nodes: Sequence[BaseSchedulerNode], prologue_nodes: Sequence[BaseSchedulerNode]) -> Optional[str]:
        ...
    
    def codegen_node(self, node: Union[FusedSchedulerNode, SchedulerNode]) -> None:
        ...
    
    def codegen_sync(self) -> None:
        ...
    
    def flush(self) -> None:
        ...
    
    def codegen_combo_kernel(self, *args: Any, **kwargs: Any) -> None:
        ...
    
    def benchmark_fused_nodes(self, nodes: Sequence[BaseSchedulerNode]) -> tuple[float, str]:
        ...
    
    def benchmark_codegened_module(self, module): # -> tuple[float, str]:
        ...
    
    def generate_kernel_code_from_nodes(self, nodes: Sequence[Any], benchmark_kernel: bool = ...) -> str:
        ...
    
    def benchmark_combo_kernel(self, node_list: Sequence[BaseSchedulerNode]) -> tuple[float, float, list[Optional[str]]]:
        ...
    


