"""
This type stub file was generated by pyright.
"""

import functools
from typing import Optional
from ..lowering import register_lowering

triton_version = ...
has_triton = ...
log = ...
aten = ...
prims = ...
mm_template = ...
persistent_tma_mm_template = ...
load_scales = ...
apply_scaling = ...
device_tma = ...
scaled_mm_device_tma_template = ...
@functools.cache
def lazy_register_extern_choice(fn): # -> ExternKernelChoice:
    ...

aten_mm = ...
aten_addmm = ...
aten__int_mm = ...
aten__sparse_semi_structured_mm = ...
aten__fp8_mm = ...
@functools.lru_cache
def using_b200() -> bool:
    """Returns true if the device is a NVIDIA B200, otherwise returns false."""
    ...

def bias_addmm(inp, mat1, mat2, *, out=..., alpha=..., beta=...): # -> Tensor:
    """
    Giving torch.addmm a 1D tensor calls a different (faster) cublasLt
    kernel under the hood.  There are a few shapes where this is slower,
    but they are rare.
    """
    ...

def check_supported_striding(mat_a, mat_b) -> None:
    ...

aten_bias_addmm = ...
def decomposeK(a, b, k_splits):
    ...

@register_lowering(aten.mm, type_promotion_kind=None)
def tuned_mm(mat1, mat2, *, layout=...): # -> TensorBox | ShapeAsConstantBuffer:
    """
    Lowering for autotuning aten.mm with different backends (Aten, Triton, CUTLASS, etc.)
    """
    ...

@register_lowering(aten._int_mm, type_promotion_kind=None)
def tuned_int_mm(mat1, mat2, *, layout=...): # -> TensorBox | ShapeAsConstantBuffer:
    ...

@register_lowering(aten.addmm, type_promotion_kind=None)
def tuned_addmm(inp, mat1, mat2, *, alpha=..., beta=..., layout=...): # -> TensorBox | ShapeAsConstantBuffer:
    ...

@register_lowering(aten._sparse_semi_structured_mm, type_promotion_kind=None)
def tuned_sparse_semi_structured_mm(mat1, mat1_meta, mat2, *, out_dtype=..., layout=...): # -> TensorBox | ShapeAsConstantBuffer:
    ...

@register_lowering(aten._scaled_mm.default, type_promotion_kind=None)
def tuned_scaled_mm(mat_a, mat_b, scale_a, scale_b, bias=..., scale_result=..., out_dtype=..., use_fast_accum=..., layout=...): # -> TensorBox | ShapeAsConstantBuffer:
    """
    Performs an optimized matrix multiplication where scaling factors are applied
    to the inputs and/or output.

    Args:
        mat1 (Tensor): First input matrix
        mat2 (Tensor): Second input matrix
        scale1 (Tensor): Scale factor applied to mat1 (supports broadcasting)
        scale2 (Tensor): Scale factor applied to mat2 (supports broadcasting)
        bias (Tensor, optional): Optional bias tensor to add to the result
        layout: Layout hint for optimization

    Returns:
        Tensor: The result of the scaled matrix multiplication
    """
    ...

def dims_are_int(dims): # -> bool:
    ...

def mm_autoheuristic(mat1, mat2, m, n, k, choices, name, input_nodes, ops, precondition, top_k: Optional[int] = ..., always_included=...): # -> list[ChoiceCaller] | ChoiceCaller | None:
    ...

def get_size_hints(mat1, mat2, m, n, k): # -> tuple[int, int, int]:
    ...

def get_size_hints_strides(mat1, mat2): # -> tuple[Any, Any]:
    ...

