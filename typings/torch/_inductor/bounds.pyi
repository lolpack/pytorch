"""
This type stub file was generated by pyright.
"""

import sympy
import torch
from typing import Any, Callable, Optional
from sympy import Expr
from torch.utils._sympy.value_ranges import SymPyValueRangeAnalysis, ValueRanges
from .loop_body import LoopBody, LoopBodyBlock
from .ops_handler import DefaultHandler, ReductionType, StoreMode
from .utils import cache_on_self

log = ...
class BoundVars:
    """
    Performs Value Range Analysis on LoopBody's fx graph by calling BoundVars.run()
    It exposes the ranges of the nodes in the `bounds` variable

    Note. A current limitation of this analysis is that it just works on a per-loop basis.
    We should be able to propagate the bounds between across the whole graph. This may benefit
    the case a bounded variable is returned by a kernel and fed into another.
    """
    def __init__(self, loop_body: LoopBody) -> None:
        ...
    
    def __repr__(self) -> str:
        ...
    
    @cache_on_self
    def get_bounds(self) -> dict[torch.fx.Node, ValueRanges[Expr]]:
        ...
    
    def swap_submodules(self, submodules: dict[str, Callable[..., Any]]) -> dict[str, Callable[..., ValueRanges[Expr]]]:
        ...
    
    def masked_subblock(self, subblock: LoopBodyBlock, env: dict[torch.fx.Node, ValueRanges[Expr]], mask: Any, value: Any, submodules: dict[str, Callable[..., Any]]) -> ValueRanges[Expr]:
        ...
    
    def set_indirect(self, old: Expr, new: ValueRanges[Expr]) -> ValueRanges[Expr]:
        ...
    
    def get_index(self, name: str) -> ValueRanges[Expr]:
        ...
    


class ValueRangeAnalysis(SymPyValueRangeAnalysis, DefaultHandler):
    def __init__(self) -> None:
        ...
    
    @staticmethod
    def bool_handler(*args: Any, **kwargs: Any) -> ValueRanges[Any]:
        ...
    
    def load(self, name: str, index: sympy.Expr) -> ValueRanges[Any]:
        ...
    
    def store(self, name: str, index: sympy.Expr, value: Any, mode: StoreMode = ...) -> None:
        ...
    
    def reduction(self, dtype: torch.dtype, src_dtype: torch.dtype, reduction_type: ReductionType, value: Any) -> ValueRanges[Any]:
        ...
    
    @classmethod
    def index_expr(cls, index: Any, dtype: torch.dtype) -> ValueRanges[Any]:
        ...
    
    @staticmethod
    def to_dtype(x: Any, dtype: torch.dtype, src_dtype: Optional[torch.dtype] = ..., use_compute_types: bool = ...) -> ValueRanges[Any]:
        ...
    
    @staticmethod
    def square(x: Any) -> ValueRanges[Any]:
        ...
    
    @staticmethod
    def neg(x: Any) -> ValueRanges[Any]:
        ...
    
    @classmethod
    def truncdiv(cls, a: Any, b: Any) -> ValueRanges[Any]:
        ...
    
    @classmethod
    def sub(cls, a: Any, b: Any) -> ValueRanges[Any]:
        ...
    


