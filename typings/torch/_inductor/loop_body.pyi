"""
This type stub file was generated by pyright.
"""

import collections
import sympy
import torch.fx
from enum import Enum
from typing import Any, Callable, NamedTuple, Optional, TYPE_CHECKING, TypeVar
from torch.fx.proxy import TracerBase
from .ops_handler import DefaultHandler, OpsHandler, WrapperHandler
from .utils import cache_on_self

if TYPE_CHECKING:
    ...
T = TypeVar("T")
class InterpreterShim(torch.fx.Interpreter):
    def __init__(self, graph, submodules) -> None:
        ...
    
    def run_node(self, n: torch.fx.Node) -> Any:
        ...
    
    def run(self, *args, **kwargs): # -> Any:
        ...
    


class LightTracer(TracerBase):
    def __init__(self) -> None:
        ...
    


class MemoryEntry(NamedTuple):
    index_name: str
    buffer_name: Optional[str]
    mode: Optional[str]
    ...


class MemoryUsageType(Enum):
    LOAD = ...
    LOAD_SEED = ...
    STORE = ...
    STORE_REDUCTION = ...
    INDEX_EXPR = ...
    CHECK_BOUNDS = ...
    BUCKETIZE = ...


class LoopBody:
    """
    Captures the body of a Loops subclass into an FX graph.  Persists any
    indexing simplifications and makes it easier to analyze loop bodies.
    """
    indexing_exprs: dict[str, sympy.Expr]
    indexing_exprs_name: dict[sympy.Expr, str]
    submodules: dict[str, Any]
    subblocks: dict[str, LoopBodyBlock]
    indirect_vars: list[sympy.Symbol]
    indirect_var_ranges: dict[sympy.Symbol, sympy.Expr]
    root_block: LoopBodyBlock
    memory_usage: dict[MemoryUsageType, list[MemoryEntry]]
    op_counts: collections.Counter[str]
    def __init__(self, fn, args, var_ranges, iter_vars, reduce_vars) -> None:
        ...
    
    def has_op(self, name: str): # -> bool:
        ...
    
    def merge_loops(self) -> LoopBody:
        """
        Merge both iteration and reduction loops and return a new LoopBody.
        """
        ...
    
    def reorder_iter_loops(self, new_order) -> LoopBody:
        """
        Reorder iteration loops and return a new LoopBody.
        """
        ...
    
    @property
    def vars(self): # -> tuple[Any, Any]:
        ...
    
    @cache_on_self
    def get_nodes(self): # -> list[Node]:
        ...
    
    @cache_on_self
    def bounds(self): # -> BoundVars:
        ...
    
    def get_read_expr(self, buffer_name): # -> Expr:
        ...
    
    def get_write_expr(self, buffer_name): # -> Expr:
        ...
    
    def get_read_exprs(self): # -> list[Expr]:
        ...
    
    def get_all_read_expr(self, buffer_name): # -> list[Any]:
        ...
    
    def get_write_exprs(self): # -> list[Expr]:
        ...
    
    def get_all_write_expr(self, buffer_name): # -> list[Any]:
        ...
    
    def debug_str(self): # -> str:
        ...
    
    def is_memory_copy(self) -> bool:
        """
        True of this contains only a single loads and store.
        Note, this could involve a layout change.
        """
        ...
    
    __repr__ = ...
    def add_index_expr(self, expr: sympy.Expr, mtype: MemoryUsageType, buffer_name: Optional[str] = ..., mode: Optional[str] = ...): # -> str:
        ...
    
    def add_submodule(self, block, prefix): # -> str:
        """Not actually for nn.Modules, but subblocks in generated code are mapped to FX call_module opcodes"""
        ...
    
    def add_indirect(self, size): # -> Symbol:
        ...
    
    def replace_indirect(self, old, new): # -> None:
        """Swap in a variable used in indirect indexing"""
        ...
    
    def get_index(self, name): # -> Expr:
        ...
    
    def indexing_from_args(self, indices): # -> dict[str, Expr]:
        ...
    
    def __call__(self, *indices): # -> Any:
        ...
    
    def bind_set_indirect_shim(self, var, size, check, wrap_neg): # -> Callable[..., None]:
        ...
    
    def bind_scan_shim(self, combine_fn): # -> Callable[..., tuple[Any, ...]]:
        ...
    
    def bind_masked_shim(self, name): # -> Callable[..., Any]:
        ...
    


class LoopBodyBlock:
    """
    Captures the body of a Loops subclass into an FX graph.
    In normal cases there will be a 1:1 mapping between LoopBody and
    LoopBodyBlock, however in the case of ops.masked() the masked out
    operations will manifest as an extra LoopBodyBlock.
    """
    def __init__(self, body: LoopBody, fn: Callable[..., Any], args: list[Any]) -> None:
        ...
    
    def __call__(self): # -> Any:
        ...
    
    def debug_str(self, name=...): # -> str:
        ...
    
    def contains_only_ops(self, allowed_ops) -> bool:
        ...
    
    def clone(self, body: LoopBody): # -> LoopBodyBlock:
        """Shallow copy with a new parent LoopBody"""
        ...
    


class CountOps(DefaultHandler):
    def __init__(self, inner: OpsHandler[Any], counts: collections.Counter[str]) -> None:
        ...
    


class CaptureIndexing(WrapperHandler):
    name = ...
    def __init__(self, inner: OpsHandler[Any], body: LoopBody, tracer: LightTracer) -> None:
        ...
    
    def load(self, name: str, index: sympy.Expr): # -> Any:
        ...
    
    def load_seed(self, name: str, index: int): # -> Any:
        ...
    
    def store(self, name, index, value, mode=...): # -> None:
        ...
    
    def store_reduction(self, name, index, value): # -> None:
        ...
    
    def reduction(self, dtype, src_dtype, reduction_type, value): # -> tuple[Any, ...] | Any:
        ...
    
    def index_expr(self, index, dtype): # -> Any:
        ...
    
    def check_bounds(self, index, size, lower, upper): # -> None:
        ...
    
    def bucketize(self, values: T, boundaries: tuple[str, sympy.Expr, sympy.Expr, sympy.Expr], boundary_indices: T, indexing_dtype: torch.dtype, right: bool, sorter: Optional[tuple[str, sympy.Expr]] = ..., sorter_indices: Optional[T] = ...) -> T:
        """
        See [Note: Inductor bucketize op]
        """
        ...
    
    def masked(self, mask_proxy, masked_body: Callable[..., Any], other_proxy): # -> Proxy:
        """
        Recursively capture the masked out body in another LoopBodyBlock
        """
        ...
    
    def scan(self, dtype_proxy, combine_fn: Callable[[tuple[Any, ...], tuple[Any, ...]], tuple[Any, ...]], value_proxy): # -> tuple[Any, ...]:
        ...
    
    def sort(self, dtypes, values, stable, descending): # -> tuple[Any, ...]:
        ...
    
    def frexp(self, value_proxy): # -> tuple[Any, Any]:
        ...
    
    def indirect_indexing(self, index_proxy, size, check=..., wrap_neg=...): # -> Symbol:
        """
        Flow data from tensors into indexing formulas.
        Introduce a call_module to update the indexing.
        """
        ...
    
    def output(self, *result): # -> None:
        ...
    


