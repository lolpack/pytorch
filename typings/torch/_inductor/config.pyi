"""
This type stub file was generated by pyright.
"""

import torch
import torch._inductor.custom_graph_pass
from typing import Any, Callable, Literal, Optional, TYPE_CHECKING, Union
from torch._environment import is_fbcode
from torch.utils._config_typing import *

inplace_padding = ...
can_inplace_pad_graph_input = ...
def fx_graph_remote_cache_default() -> Optional[bool]:
    ...

def vec_isa_ok_default() -> Optional[bool]:
    ...

def autotune_remote_cache_default() -> Optional[bool]:
    ...

def bundled_autotune_remote_cache_default() -> Optional[bool]:
    ...

def bundle_triton_into_fx_graph_cache_default() -> Optional[bool]:
    ...

def static_cuda_launcher_default() -> bool:
    ...

def prologue_fusion_enabled() -> bool:
    ...

enable_auto_functionalized_v2 = ...
debug = ...
disable_progress = ...
verbose_progress = ...
precompilation_timeout_seconds: int = ...
fx_graph_cache: bool = ...
fx_graph_remote_cache: Optional[bool] = ...
bundle_triton_into_fx_graph_cache: Optional[bool] = ...
non_blocking_remote_cache_write: bool = ...
autotune_local_cache: bool = ...
autotune_remote_cache: Optional[bool] = ...
bundled_autotune_remote_cache: Optional[bool] = ...
force_disable_caches: bool = ...
unsafe_skip_cache_dynamic_shape_guards: bool = ...
unsafe_marked_cacheable_functions: dict[str, str] = ...
sleep_sec_TESTING_ONLY: Optional[int] = ...
triton_kernel_default_layout_constraint: Literal["needs_fixed_stride_order", "flexible_layout"] = ...
cpp_wrapper: bool = ...
cpp_wrapper_build_separate: bool = ...
cpp_cache_precompile_headers: bool = ...
online_softmax = ...
dce = ...
static_weight_shapes = ...
size_asserts = ...
nan_asserts = ...
scalar_asserts = ...
alignment_asserts = ...
pick_loop_orders = ...
inplace_buffers = ...
allow_buffer_reuse = ...
memory_planning = ...
use_fast_math = ...
bfloat16_atomic_adds_enabled = ...
memory_pool: Literal["none", "intermediates", "outputs", "combined"] = ...
benchmark_harness = ...
epilogue_fusion = ...
prologue_fusion = ...
epilogue_fusion_first = ...
pattern_matcher = ...
b2b_gemm_pass = ...
post_grad_custom_pre_pass: torch._inductor.custom_graph_pass.CustomGraphPassType = ...
post_grad_custom_post_pass: torch._inductor.custom_graph_pass.CustomGraphPassType = ...
joint_custom_pre_pass: Optional[Callable[[torch.fx.Graph], None]] = ...
joint_custom_post_pass: Optional[Callable[[torch.fx.Graph], None]] = ...
pre_grad_custom_pass: Optional[Callable[[torch.fx.graph.Graph], None]] = ...
_pre_fusion_custom_pass: Optional[Callable[[list[torch._inductor.scheduler.BaseSchedulerNode]], list[torch._inductor.scheduler.BaseSchedulerNode],]] = ...
_post_fusion_custom_pass: Optional[Callable[[list[torch._inductor.scheduler.BaseSchedulerNode]], list[torch._inductor.scheduler.BaseSchedulerNode],]] = ...
split_cat_fx_passes = ...
efficient_conv_bn_eval_fx_passes = ...
is_predispatch = ...
group_fusion = ...
batch_fusion = ...
pre_grad_fusion_options: dict[str, dict[str, Any]] = ...
post_grad_fusion_options: dict[str, dict[str, Any]] = ...
reorder_for_locality = ...
dynamic_scale_rblock = ...
force_fuse_int_mm_with_mul = ...
use_mixed_mm = ...
fx_passes_numeric_check: dict[str, Any] = ...
mixed_mm_choice: Literal["default", "triton", "aten", "heuristic"] = ...
reorder_for_compute_comm_overlap = ...
reorder_for_compute_comm_overlap_passes: list[Union[str, Callable[[list[torch._inductor.scheduler.BaseSchedulerNode]], list[torch._inductor.scheduler.BaseSchedulerNode],],]] = ...
reorder_prefetch_limit: Optional[int] = ...
reorder_for_peak_memory = ...
estimate_op_runtime = ...
intra_node_bw = ...
inter_node_bw = ...
use_experimental_benchmarker: bool = ...
max_autotune = ...
max_autotune_pointwise = ...
max_autotune_gemm = ...
disable_decompose_k = ...
autotune_num_choices_displayed: Optional[int] = ...
graph_partition = ...
force_same_precision: bool = ...
max_autotune_gemm_backends = ...
max_autotune_conv_backends = ...
max_autotune_gemm_search_space: Literal["DEFAULT", "EXHAUSTIVE"] = ...
max_autotune_flex_search_space: Literal["DEFAULT", "EXHAUSTIVE"] = ...
autotune_fallback_to_aten = ...
unbacked_symint_fallback = ...
search_autotune_cache = ...
save_args = ...
autotune_in_subproc = ...
max_autotune_subproc_result_timeout_seconds = ...
max_autotune_subproc_graceful_timeout_seconds = ...
max_autotune_subproc_terminate_timeout_seconds = ...
autotune_multi_device = ...
coordinate_descent_tuning = ...
coordinate_descent_check_all_directions = ...
coordinate_descent_search_radius = ...
autoheuristic_collect = ...
autoheuristic_use = ...
def run_autoheuristic(name: str) -> bool:
    ...

def collect_autoheuristic(name: str) -> bool:
    ...

def use_autoheuristic(name: str) -> bool:
    ...

autoheuristic_log_path = ...
layout_opt_default = ...
layout_optimization = ...
force_layout_optimization = ...
keep_output_stride = ...
warn_mix_layout = ...
realize_reads_threshold = ...
realize_opcount_threshold = ...
realize_acc_reads_threshold = ...
fallback_random = ...
implicit_fallbacks = ...
assume_unaligned_fallback_output = ...
aggressive_fusion = ...
debug_fusion: bool = ...
benchmark_fusion: bool = ...
enabled_metric_tables = ...
loop_ordering_after_fusion: bool = ...
score_fusion_memory_threshold = ...
benchmark_epilogue_fusion = ...
max_epilogue_benchmarked_choices = ...
max_fusion_size = ...
max_fusion_buffer_group_pairwise_attempts = ...
max_pointwise_cat_inputs = ...
force_pointwise_cat = ...
unroll_reductions_threshold = ...
comment_origin = ...
conv_1x1_as_mm = ...
split_reductions = ...
min_num_split = ...
benchmark_kernel = ...
constant_and_index_propagation = ...
always_keep_tensor_constants = ...
assert_indirect_indexing = ...
compute_all_bounds = ...
combo_kernels = ...
benchmark_combo_kernel = ...
combo_kernels_autotune = ...
combo_kernel_allow_mixed_sizes = ...
combo_kernel_foreach_dynamic_shapes = ...
joint_graph_constant_folding = ...
debug_index_asserts = ...
emulate_precision_casts = ...
is_nightly_or_source = ...
developer_warnings = ...
optimize_scatter_upon_const_tensor = ...
add_pre_grad_passes: Optional[str] = ...
remove_pre_grad_passes: Optional[str] = ...
def decide_worker_start_method() -> str:
    ...

worker_start_method: str = ...
worker_suppress_logging: bool = ...
_fuse_ddp_communication = ...
_fuse_ddp_bucket_size = ...
_fuse_ddp_communication_passes: list[Union[Callable[..., None], str]] = ...
_micro_pipeline_tp: bool = ...
class _collective:
    auto_select: bool = ...
    one_shot_all_reduce_threshold_bytes: int = ...


def parallel_compile_enabled_internally() -> bool:
    """
    TODO: Remove when parallel compiled is fully enabled internally. For rollout, use a
    knob to enable / disable. The justknob should not be performed at import, however.
    So for fbcode, we assign compile_threads to 'None' below and initialize lazily in
    async_compile.py.
    """
    ...

def decide_compile_threads() -> int:
    """
    Here are the precedence to decide compile_threads
    1. User can override it by TORCHINDUCTOR_COMPILE_THREADS.  One may want to disable async compiling by
       setting this to 1 to make pdb happy.
    2. Set to 1 if it's win32 platform
    3. decide by the number of CPU cores
    """
    ...

compile_threads: Optional[int] = ...
use_static_cuda_launcher: bool = ...
static_launch_user_defined_triton_kernels: bool = ...
strict_static_cuda_launcher: bool = ...
global_cache_dir: Optional[str]
if is_fbcode():
    ...
else:
    global_cache_dir = ...
kernel_name_max_ops = ...
shape_padding = ...
comprehensive_padding = ...
pad_channels_last = ...
disable_padding_cpu = ...
padding_alignment_bytes = ...
padding_stride_threshold = ...
pad_outputs = ...
bw_outputs_user_visible = ...
force_shape_pad: bool = ...
permute_fusion = ...
profiler_mark_wrapper_call = ...
generate_intermediate_hooks = ...
debug_ir_traceback = ...
_raise_error_for_testing = ...
_profile_var = ...
profile_bandwidth = ...
profile_bandwidth_regex = ...
profile_bandwidth_output: Optional[str] = ...
profile_bandwidth_with_do_bench_using_profiling = ...
disable_cpp_codegen = ...
freezing: bool = ...
freezing_discard_parameters: bool = ...
decompose_mem_bound_mm: bool = ...
assume_aligned_inputs: bool = ...
unsafe_ignore_unsupported_triton_autotune_args: bool = ...
check_stack_no_cycles_TESTING_ONLY: bool = ...
always_complex_memory_overlap_TESTING_ONLY: bool = ...
enable_linear_binary_folding = ...
annotate_training: bool = ...
enable_caching_generated_triton_templates: bool = ...
class cpp:
    threads = ...
    no_redundant_loops = ...
    dynamic_threads = ...
    simdlen: Optional[int] = ...
    min_chunk_size = ...
    cxx: tuple[Literal[None], str] = ...
    enable_kernel_profile = ...
    weight_prepack = ...
    inject_relu_bug_TESTING_ONLY: Optional[str] = ...
    inject_log1p_bug_TESTING_ONLY: Optional[str] = ...
    vec_isa_ok: Optional[bool] = ...
    descriptive_names: Literal["torch", "original_aten", "inductor_node"] = ...
    max_horizontal_fusion_size = ...
    fallback_scatter_reduce_sum = ...
    enable_unsafe_math_opt_flag = ...
    enable_floating_point_contract_flag = ...
    enable_tiling_heuristics = ...
    enable_grouped_gemm_template = ...
    gemm_max_k_slices = ...
    gemm_cache_blocking = ...
    gemm_thread_factors = ...
    enable_loop_tail_vec = ...
    enable_concat_linear = ...
    use_decompose_tanh = ...
    use_small_dequant_buffer = ...


class triton:
    cudagraphs = ...
    cudagraph_trees = ...
    cudagraph_skip_dynamic_graphs = ...
    cudagraph_capture_sizes: Optional[tuple[Union[int, tuple[int, ...]]]] = ...
    slow_path_cudagraph_asserts = ...
    cudagraph_trees_history_recording = ...
    cudagraph_support_input_mutation = ...
    cudagraph_unexpected_rerecord_limit = ...
    cudagraph_dynamic_shape_warn_limit: Optional[int] = ...
    force_cudagraph_sync = ...
    force_cudagraphs_warmup = ...
    fast_path_cudagraph_asserts = ...
    skip_cudagraph_warmup = ...
    debug_sync_graph = ...
    debug_sync_kernel = ...
    dense_indexing = ...
    coalesce_tiling_analysis: bool = ...
    max_tiles: Optional[int] = ...
    prefer_nd_tiling: bool = ...
    autotune_pointwise = ...
    autotune_cublasLt = ...
    autotune_at_compile_time: Optional[bool] = ...
    autotune_with_sample_inputs: bool = ...
    tile_reductions: bool = ...
    tiling_prevents_pointwise_fusion = ...
    tiling_prevents_reduction_fusion = ...
    unique_kernel_names = ...
    unique_user_kernel_names = ...
    descriptive_names: Literal["torch", "original_aten", "inductor_node"] = ...
    persistent_reductions = ...
    cooperative_reductions = ...
    force_cooperative_reductions = ...
    multi_kernel: Literal[0, 1, 2, 3] = ...
    divisible_by_16 = ...
    min_split_scan_rblock = ...
    store_cubin = ...
    spill_threshold: int = ...
    use_block_ptr = ...
    inject_relu_bug_TESTING_ONLY: Optional[str] = ...
    codegen_upcast_to_fp32 = ...
    enable_persistent_tma_matmul = ...
    skip_l1_cache = ...
    disallow_failing_autotune_kernels_TESTING_ONLY = ...


class aot_inductor:
    """
    Settings for Ahead-Of-Time Inductor Compilation
    """
    output_path = ...
    debug_compile = ...
    compile_wrapper_opt_level = ...
    debug_intermediate_value_printer: Literal["0", "1", "2", "3"] = ...
    filtered_kernel_names = ...
    serialized_in_spec = ...
    serialized_out_spec = ...
    use_runtime_constant_folding: bool = ...
    force_mmap_weights: bool = ...
    package: bool = ...
    package_cpp_only: bool = ...
    metadata: dict[str, str] = ...
    raise_error_on_ignored_optimization: bool = ...
    dump_aoti_minifier: bool = ...
    repro_level: int = ...
    presets: dict[str, Any] = ...
    allow_stack_allocation: bool = ...
    use_minimal_arrayref_interface: bool = ...
    package_constants_in_so: bool = ...
    package_constants_on_disk: bool = ...
    precompile_headers: bool = ...
    embed_kernel_binary: bool = ...
    emit_multi_arch_kernel: bool = ...
    model_name_for_generated_files: Optional[str] = ...
    custom_ops_to_c_shims: dict[torch._ops.OpOverload, list[str]] = ...
    custom_op_libs: Optional[list[str]] = ...


class cuda:
    """Settings for cuda backend, today this consists of cutlass"""
    arch: Optional[str] = ...
    version: Optional[str] = ...
    compile_opt_level: Literal["-O0", "-O1", "-O2", "-O3", "-OS"] = ...
    enable_cuda_lto = ...
    enable_ptxas_info = ...
    enable_debug_info = ...
    use_fast_math = ...
    cutlass_dir = ...
    cutlass_max_profiling_configs: Optional[int] = ...
    cutlass_max_profiling_swizzle_options: list[int] = ...
    cutlass_epilogue_fusion_enabled = ...
    cutlass_tma_only = ...
    cuda_cxx: Optional[str] = ...
    cutlass_backend_min_gemm_size: int = ...
    generate_test_runner: bool = ...
    cutlass_op_allowlist_regex: Optional[str] = ...
    cutlass_op_denylist_regex: Optional[str] = ...
    cutlass_instantiation_level: str = ...
    cutlass_presets: Optional[str] = ...
    cutlass_hash_with_compile_cmd: bool = ...
    cutlass_prescreening: bool = ...
    cutlass_enabled_ops: str = ...
    use_binary_remote_cache: bool = ...
    upload_to_binary_remote_cache: bool = ...
    binary_remote_cache_force_write: bool = ...


class rocm:
    arch: list[str] = ...
    ck_supported_arch: list[str] = ...
    compile_opt_level: Literal["-O0", "-O1", "-O2", "-O3", "-Os", "-Oz", "-Omin", "-Ofast", "-Omax"] = ...
    is_debug = ...
    save_temps = ...
    use_fast_math = ...
    flush_denormals = ...
    print_kernel_resource_usage = ...
    rocm_home: Optional[str] = ...
    ck_dir = ...
    generate_test_runner: bool = ...
    n_max_profiling_configs: Optional[int] = ...
    ck_max_profiling_configs: Optional[int] = ...
    ck_tile_max_profiling_configs: Optional[int] = ...
    use_preselected_instances: bool = ...
    kBatch_sweep: Optional[list[int]] = ...
    split_k_threshold: int = ...


cpu_backend: Literal["cpp", "triton", "halide"] = ...
cuda_backend: Literal["triton", "halide"] = ...
class halide:
    cpu_target = ...
    gpu_target = ...
    scheduler_cuda: Literal["Anderson2021", "Li2018", "Adams2019", "Mullapudi2016"] = ...
    scheduler_cpu: Literal["Anderson2021", "Li2018", "Adams2019", "Mullapudi2016"] = ...
    asserts = ...
    debug = ...
    scan_kernels = ...


class trace:
    enabled = ...
    save_real_tensors = ...
    debug_dir: Optional[str] = ...
    debug_log = ...
    info_log = ...
    fx_graph = ...
    fx_graph_transformed = ...
    ir_pre_fusion = ...
    ir_post_fusion = ...
    output_code = ...
    graph_diagram = ...
    draw_orig_fx_graph = ...
    dot_graph_shape = ...
    log_url_for_graph_xform = ...
    compile_profile = ...
    upload_tar: Optional[Callable[[str], None]] = ...
    log_autotuning_results = ...
    log_inductor_triton_kernel_to_post_grad_node_info: bool = ...


_save_config_ignore: list[str] = ...
_cache_config_ignore_prefix: list[str] = ...
external_matmul: list[Callable[[torch.Tensor, torch.Tensor, torch.Tensor], None]] = ...
class test_configs:
    force_extern_kernel_in_multi_template: bool = ...
    max_mm_configs: Optional[int] = ...
    runtime_triton_dtype_assert = ...
    static_cpp_dtype_assert = ...
    autotune_choice_name_regex: Optional[str] = ...
    autotune_choice_desc_regex: Optional[str] = ...
    graphsafe_rng_func_ignores_fallback_random = ...


if TYPE_CHECKING:
    ...
