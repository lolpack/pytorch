"""
This type stub file was generated by pyright.
"""

import contextlib
import dataclasses
import enum
import functools
import sympy
import torch
from collections.abc import Collection, Generator, Iterable, Iterator, Mapping, MutableMapping, MutableSet, Sequence, ValuesView
from typing import Any, Callable, Generic, Literal, NamedTuple, Optional, Protocol, TYPE_CHECKING, TypeVar, Union
from typing_extensions import Concatenate, ParamSpec, Self, TypeAlias, TypeGuard, dataclass_transform
from torch.utils._ordered_set import OrderedSet
from torch import SymInt
from torch._prims_common import ELEMENTWISE_TYPE_PROMOTION_KIND
from torch.fx import GraphModule
from torch.fx.experimental.symbolic_shapes import ShapeEnv
from torch.fx.node import Node
from .codegen.common import WorkspaceArg
from .codegen.wrapper import PythonWrapperCodegen
from .graph import GraphLowering
from .ir import Buffer, ExternKernel, ExternKernelOut, IRNode, Layout, Operation, ReinterpretView
from .output_code import CompiledFxGraph
from .scheduler import BaseSchedulerNode, SchedulerBuffer
from torch.utils._sympy.symbol import SymT
from torch.utils._sympy.value_ranges import ValueRanges

OPTIMUS_EXCLUDE_POST_GRAD = ...
if TYPE_CHECKING:
    ...
GPU_TYPES = ...
T = TypeVar("T")
@functools.cache
def get_gpu_type() -> str:
    ...

_IS_WINDOWS = ...
log = ...
_T = TypeVar("_T")
VarRanges = dict[sympy.Expr, sympy.Expr]
InputType = Optional[Union[torch.Tensor, int, torch.SymInt]]
GPU_KERNEL_BIN_EXTS = ...
GPU_ALIGN_BYTES = ...
ALIGNMENT = ...
TMA_ALIGNMENT = ...
TMA_DESCRIPTOR_SIZE = ...
ALIGN_BYTES = ...
class align(sympy.Function):
    """Symbolically round up to the nearest multiple of ALIGN_BYTES"""
    nargs = ...
    is_integer = ...
    @classmethod
    def eval(cls, value: sympy.Expr) -> Optional[sympy.Expr]:
        ...
    


@dataclasses.dataclass(frozen=True)
class GraphPartitionMap:
    """
    Mapping from the partition info (e.g., input/output) to the graph info
    """
    id: int
    input_index_mapping: list[Optional[int]]
    output_index_mapping: list[Optional[int]]
    constant_names: list[str]
    ...


def fp8_bench(fn: Callable[[], Any], warmup: int = ..., rep: int = ...) -> float:
    """
    Returns benchmark results by examining torch profiler events.
    This could be more accurate as it doesn't count CPU side overhead.
    However, this also requires manually excluding irrelevant event, e.g.
    vectorized_elementwise_kernel which is used to fill L2 cache,
    various CUDA events, etc, so could also be fragile.
    """
    ...

def do_bench_using_profiling(fn: Callable[[], Any], warmup: int = ..., rep: int = ...) -> float:
    """
    Returns benchmark results by examining torch profiler events.
    This could be more accurate as it doesn't count CPU side overhead.
    However, this also requires manually excluding irrelevant event, e.g.
    vectorized_elementwise_kernel which is used to fill L2 cache,
    various CUDA events, etc, so could also be fragile.
    """
    ...

@functools.cache
def has_torchvision_roi_align() -> bool:
    ...

def decode_device(device: Union[Optional[torch.device], str]) -> torch.device:
    ...

def sympy_product(it: Iterable[sympy.Expr]) -> sympy.Expr:
    ...

def sympy_dot(seq1: Sequence[sympy.Expr], seq2: Sequence[sympy.Expr]) -> sympy.Expr:
    ...

def unique(it: Iterable[_T]) -> ValuesView[_T]:
    ...

def ceildiv(number: Union[int, sympy.Expr], denom: Union[int, sympy.Expr]) -> Union[int, sympy.Expr]:
    ...

def convert_shape_to_inductor(lst: Iterable[Union[int, torch.SymInt]]) -> list[sympy.Expr]:
    """
    Gets the shape and stride of a tensor. For non-symbolic tensors, this is
    trivial. But for symbolic tensors, we need to map from SymIntNode into
    sympy.Expr.
    """
    ...

def convert_to_symint(i: Union[int, sympy.Expr]) -> Union[int, torch.SymInt]:
    """
    Like convert_shape_to_symint, but operates on a single expression.
    """
    ...

def convert_shape_to_symint(lst: Iterable[Union[int, sympy.Expr]]) -> list[Union[int, torch.SymInt]]:
    """
    Takes a list of shapes from Inductor and converts them into symints (or just
    ints if all shapes are static).
    """
    ...

def is_view(op: torch._ops.OpOverload) -> bool:
    """
    Does this op overload have aliasing
    """
    ...

def is_pointwise_use(use: Node, is_pointwise_fn: Callable[[torch._ops.OpOverload], bool] = ...) -> bool:
    """
    Do all uses of this op have torch.Tag.pointwise or return True for optional `is_pointwise_fn`

    Uses in views ops will follow the views uses
    """
    ...

def gen_gm_and_inputs(target: Any, args: list[Any], kwargs: dict[str, Any]) -> tuple[GraphModule, list[torch.Tensor]]:
    ...

def synchronize(device: str = ...) -> None:
    ...

def timed(model: Callable[..., Any], example_inputs: Sequence[Any], times: int = ..., device: str = ...) -> float:
    ...

def print_performance(model: Callable[..., Any], example_inputs: Sequence[Any] = ..., times: int = ..., repeat: int = ..., baseline: float = ..., device: str = ...) -> float:
    ...

def precompute_method(obj: Any, method: str) -> None:
    """Replace obj.method() with a new method that returns a precomputed constant."""
    ...

def precompute_methods(obj: Any, methods: list[str]) -> None:
    """Replace methods with new methods that returns a precomputed constants."""
    ...

def cmp(a: int, b: int) -> int:
    ...

def pad_listlike(x: Union[int, Sequence[int]], size: int) -> Sequence[int]:
    ...

def tuple_sorted(x: tuple[_T, ...]) -> list[_T]:
    ...

P = ParamSpec("P")
RV = TypeVar("RV", covariant=True)
class CachedMethod(Protocol, Generic[P, RV]):
    @staticmethod
    def clear_cache(cache: Any) -> None:
        ...
    
    def __call__(self, *args: P.args, **kwargs: P.kwargs) -> RV:
        ...
    


def cache_on_self(fn: Callable[Concatenate[Any, P], RV]) -> CachedMethod[P, RV]:
    ...

def aggregate_origins(node_schedule: Union[Sequence[BaseSchedulerNode], ExternKernel]) -> OrderedSet[Node]:
    ...

def get_fused_kernel_name(node_schedule: Sequence[BaseSchedulerNode], descriptive_names: Literal[True, "torch", "original_aten", "inductor_node"]) -> str:
    ...

def get_kernel_metadata(node_schedule: Union[Sequence[BaseSchedulerNode], ExternKernel], wrapper: PythonWrapperCodegen) -> tuple[str, str]:
    ...

def dominated_nodes(initial_queue: Iterable[torch.fx.Node], skip_filter: Optional[Callable[[Any], bool]] = ...) -> OrderedSet[torch.fx.Node]:
    """Returns the set of nodes whose values depend on those within initial_queue"""
    ...

def gather_origins(args: Sequence[IRNode], kwargs: dict[str, IRNode]) -> OrderedSet[IRNode]:
    ...

def sympy_str(expr: sympy.Expr) -> str:
    """
    Normal sympy str is very slow, this is a lot faster.  The result are
    somewhat worse, as it doesn't do as much simplification.  So don't
    use this for final codegen.
    """
    ...

def get_bounds_index_expr(index: sympy.Expr) -> ValueRanges[Any]:
    ...

def prefix_is_reduction(prefix: str) -> bool:
    ...

def sympy_index_symbol_with_prefix(prefix: SymT, idx: int) -> sympy.Symbol:
    """
    Used to generate an integer-nonnegative symbol.
    """
    ...

def generate_assert(check: bool) -> bool:
    ...

def sympy_index_symbol(name: str) -> sympy.Symbol:
    """
    Used to generate an integer-nonnegative symbol.
    """
    ...

def sympy_subs(expr: sympy.Expr, replacements: dict[sympy.Expr, Any]) -> sympy.Expr:
    """
    When the passed replacement symbol v is a string, it is converted to a symbol with name v that
    have the same replaced expression integer and nonnegative properties.
    """
    ...

def is_symbolic(a: Any) -> TypeGuard[Union[torch.SymInt, torch.Tensor]]:
    ...

def any_is_symbolic(*args: Any) -> bool:
    ...

def get_first_incompatible_cudagraph_node(gm: torch.fx.GraphModule) -> Optional[torch.fx.Node]:
    ...

def output_node(gm: torch.fx.GraphModule) -> Node:
    """Get the output node from an FX graph"""
    ...

def get_all_devices(gm: torch.fx.GraphModule) -> OrderedSet[torch.device]:
    ...

def unload_xpu_triton_pyds() -> None:
    ...

_registered_caches: list[Any] = ...
def clear_on_fresh_cache(obj: Any) -> Any:
    """
    Use this decorator to register any caches that should be cache_clear'd
    with fresh_cache().
    """
    ...

def clear_caches() -> None:
    """
    Clear all registered caches.
    """
    ...

@contextlib.contextmanager
def fresh_cache(cache_entries: Optional[dict[str, Any]] = ..., dir: Optional[str] = ..., delete: bool = ...) -> Iterator[None]:
    """
    Contextmanager that provides a clean tmp cachedir for pt2 caches.

    Optionally, pass a dict as 'cache_entries' to get a list of filenames and sizes
    generated with this cache instance.
    """
    ...

clear_on_fresh_inductor_cache = ...
clear_inductor_caches = ...
fresh_inductor_cache = ...
def argsort(seq: Sequence[Any]) -> list[int]:
    ...

def argsort_sym(shape_env: ShapeEnv, seq: Sequence[Union[int, torch.SymInt, sympy.Expr]]) -> list[int]:
    ...

@functools.lru_cache(8)
def get_dtype_size(dtype: torch.dtype) -> int:
    ...

class LineContext(NamedTuple):
    context: Any
    ...


@dataclasses.dataclass
class ValueWithLineMap:
    value: str
    line_map: list[tuple[int, LineContext]]
    ...


class IndentedBuffer:
    tabwidth = ...
    def __init__(self, initial_indent: int = ...) -> None:
        ...
    
    @contextlib.contextmanager
    def set_tabwidth(self, tabwidth: int) -> Iterator[None]:
        ...
    
    def getvaluewithlinemap(self) -> ValueWithLineMap:
        ...
    
    def getvalue(self) -> str:
        ...
    
    def getrawvalue(self) -> str:
        ...
    
    def clear(self) -> None:
        ...
    
    def __bool__(self) -> bool:
        ...
    
    def prefix(self) -> str:
        ...
    
    def newline(self) -> None:
        ...
    
    def writeline(self, line: Union[LineContext, DeferredLineBase, str]) -> None:
        ...
    
    def writelines(self, lines: Sequence[Union[LineContext, DeferredLineBase, str]]) -> None:
        ...
    
    def indent(self, offset: int = ...) -> contextlib.AbstractContextManager[None]:
        ...
    
    def do_indent(self, offset: int = ...) -> None:
        ...
    
    def do_unindent(self, offset: int = ...) -> None:
        ...
    
    def splice(self, other_code: Union[IndentedBuffer, str], strip: bool = ...) -> None:
        ...
    
    def map(self, func: Callable[[Any], Any]) -> IndentedBuffer:
        ...
    
    def __repr__(self) -> str:
        ...
    
    def __add__(self, other: Self) -> IndentedBuffer:
        ...
    


class FakeIndentedBuffer(IndentedBuffer):
    def __init__(self) -> None:
        ...
    
    def __getattribute__(self, name: str) -> Any:
        ...
    


@contextlib.contextmanager
def restore_stdout_stderr() -> Iterator[None]:
    ...

class DeferredLineBase:
    """A line that can be 'unwritten' at a later time"""
    def __init__(self, line: str) -> None:
        ...
    
    def __call__(self) -> Union[str, None]:
        """Returns either self.line or None to indicate the line has been 'unwritten'"""
        ...
    
    def with_prefix(self, prefix: str) -> Self:
        ...
    
    def lstrip(self) -> Self:
        ...
    
    def __getitem__(self, index: Union[int, slice]) -> Self:
        ...
    
    def __bool__(self) -> bool:
        ...
    
    def __len__(self) -> int:
        ...
    


class DelayReplaceLine(DeferredLineBase):
    """At end of codegen call `line.replace(key, value_fn())`"""
    def __init__(self, key: str, value_fn: Callable[[], str], line: str) -> None:
        ...
    
    def __call__(self) -> str:
        ...
    


@functools.cache
def is_big_gpu(index_or_device: Union[int, torch.device] = ...) -> bool:
    ...

@functools.lru_cache
def get_max_num_sms() -> int:
    ...

def get_num_sms() -> int:
    """Handle experimental carveout if set otherwise return hardware SM count"""
    ...

def get_tma_workspace_arg(num_tma_descriptors: int, device: torch.device, num_programs: Optional[int] = ...) -> WorkspaceArg:
    """Builds and returns a WorkspaceArg for the device side TMA workspace buffer."""
    ...

def use_triton_template(layout: Layout, *, enable_int32: bool = ..., enable_float8: bool = ...) -> bool:
    ...

def use_triton_tma_template(*matrices: IRNode) -> bool:
    ...

def use_cutlass_template(layout: Layout, m: int, n: int, k: int) -> bool:
    ...

decompose_k_threshold = ...
k_splits_limit = ...
default_k_splits = ...
_IntLike: TypeAlias = Union[int, sympy.Expr]
def use_decompose_k_choice(m: _IntLike, n: _IntLike, k: _IntLike) -> bool:
    ...

@functools.cache
def get_k_splits(m: _IntLike, n: _IntLike, k: _IntLike) -> list[int]:
    ...

@functools.cache
def try_import_ck_lib() -> tuple[Optional[str], Callable[[], list[Any]], Callable[[], list[Any]], type[Any]]:
    ...

def use_ck_template(layout: Layout) -> bool:
    ...

def use_ck_gemm_template(layout: Layout, m: int, n: int, k: int) -> bool:
    ...

def use_ck_tile_gemm_template(layout: Layout, m: int, n: int, k: int) -> bool:
    ...

def use_ck_conv_template(layout: Layout) -> bool:
    ...

def use_cpp_bmm_template(layout: Layout, mat1: Union[ReinterpretView, Buffer], mat2: IRNode) -> bool:
    ...

def use_cpp_gemm_template(layout: Layout, mat1: IRNode, mat2: IRNode, mat2_transposed: bool = ..., require_constant_mat2: bool = ..., is_woq_int4: bool = ..., q_group_size: Optional[int] = ...) -> bool:
    ...

def use_aten_gemm_kernels() -> bool:
    ...

class DebugDirManager:
    counter = ...
    prev_debug_name: str
    def __init__(self) -> None:
        ...
    
    def __enter__(self) -> None:
        ...
    
    def __exit__(self, *args: Any) -> None:
        ...
    


def run_and_get_code(fn: Callable[P, _T], *args: P.args, **kwargs: P.kwargs) -> tuple[_T, list[str]]:
    ...

def run_and_get_kernels(fn: Callable[P, _T], *args: P.args, **kwargs: P.kwargs) -> tuple[_T, list[str]]:
    ...

def run_fw_bw_and_get_code(fn: Callable[..., Any]) -> tuple[Any, list[str]]:
    ...

def get_code(fn: Callable[P, _T], *args: P.args, **kwargs: P.kwargs) -> list[str]:
    """Get the inductor-generated code, but skip any actual compilation or running."""
    ...

def get_triton_code(fn: Callable[P, _T], *args: P.args, **kwargs: P.kwargs) -> str:
    ...

def run_and_get_triton_code(fn: Callable[P, _T], *args: P.args, **kwargs: P.kwargs) -> str:
    ...

def run_and_get_graph_lowering(fn: Callable[P, _T], *args: P.args, **kwargs: P.kwargs) -> tuple[Any, list[GraphLowering]]:
    ...

@contextlib.contextmanager
def override_lowering(aten_op: Callable[..., Any], override_fn: Callable[..., Any]) -> Iterator[None]:
    """
    Override the lowering of aten_op with override_fn.
    The first argument of override_fn is the original lowering fn.
    """
    ...

def add_scheduler_init_hook(pre_fn: Callable[..., Any], post_fn: Optional[Callable[..., Any]] = ...) -> Any:
    """
    Add hook functions to be called at the beginning and end of Scheduler.__init__.
    Used for unit tests.
    """
    ...

def developer_warning(msg: str) -> None:
    """
    Warnings that will be actionable for PyTorch developers, but not
    end users.  Allows us to easily disable them in stable releases but
    keep them on for nightly builds.
    """
    ...

def get_benchmark_name() -> Optional[str]:
    """
    An experimental API used only when config.benchmark_kernel is true.

    The benchmark name is only available at codegen time. So we can not
    directly call it in benchmark_all_kernels which is run after codegen.

    The function assumes the argument after --only is the benchmark name.
    It works for torchbench.py/hugginface.py/timm_models.py. But for ad-hoc
    scripts, this function may return None.

    There are 2 flavors of --only argument we need handle:
    1. --only model_name
    2. --only=model_name
    """
    ...

def is_ones(items: Sequence[Any]) -> bool:
    ...

def is_zeros(items: Sequence[Any]) -> bool:
    ...

def is_cpu_device(inputs: Sequence[torch.Tensor]) -> bool:
    ...

def get_sympy_Expr_dtype(val: sympy.Expr) -> torch.dtype:
    ...

@contextlib.contextmanager
def maybe_profile(should_profile: bool, *args: Any, **kwargs: Any) -> Iterator[Any]:
    ...

def parallel_num_threads() -> int:
    ...

@functools.cache
def get_backend_num_stages() -> int:
    ...

@functools.cache
def get_device_tflops(dtype: torch.dtype) -> float:
    """
    We don't want to throw errors in this function. First check to see if the device is in device_info.py,
    then fall back to the inaccurate triton estimation.
    """
    ...

@functools.cache
def get_gpu_dram_gbps() -> int:
    ...

def get_gpu_shared_memory() -> int:
    ...

def is_welford_reduction(reduction_type: str) -> bool:
    ...

def reduction_num_outputs(reduction_type: str) -> int:
    ...

def is_linux() -> bool:
    ...

def is_windows() -> bool:
    ...

def has_free_symbols(itr: Iterable[Any]) -> bool:
    ...

def is_dynamic(*args: Any) -> bool:
    ...

class Placeholder(enum.Enum):
    KERNEL_NAME = ...
    DESCRIPTIVE_NAME = ...


def pass_execution_and_save(func: Callable[..., Any], gm: GraphModule, inp: Sequence[Any], msg: str) -> None:
    ...

def is_multi_outputs_template(input_buf: Optional[Union[Buffer, Operation]]) -> bool:
    """
    Check if input buffer is a multi-outputs template buffer
    """
    ...

def is_output_of_multi_outputs_template(input_buf: Optional[Union[Buffer, Operation]]) -> bool:
    """
    Check if input buffer is a output of multi-outputs template buffer
    """
    ...

def is_collective(node: Optional[Union[Node, Operation]], op: Optional[torch._ops.OperatorBase] = ...) -> bool:
    ...

def is_wait(node: Optional[Union[IRNode, Operation]]) -> bool:
    ...

def contains_collective(snode: BaseSchedulerNode) -> bool:
    ...

def contains_wait(snode: BaseSchedulerNode) -> bool:
    ...

def is_fallback_op(node: Optional[Operation], op: Union[torch._ops.OpOverload, Collection[torch._ops.OpOverload]]) -> bool:
    ...

def buf_name_to_fused_snode(buf_name: str, name_to_buf: dict[str, Any], name_to_fused_node: dict[str, Any]) -> Any:
    ...

def find_recursive_deps_of_node(snode: BaseSchedulerNode, collected_node_set: MutableSet[BaseSchedulerNode], name_to_buf: dict[str, SchedulerBuffer], name_to_fused_node: dict[str, BaseSchedulerNode], criteria_cb: Callable[[Any], bool] = ...) -> None:
    ...

def find_recursive_users_of_node(snode: BaseSchedulerNode, collected_node_set: MutableSet[BaseSchedulerNode], name_to_buf: dict[str, SchedulerBuffer], name_to_fused_node: dict[str, BaseSchedulerNode], criteria_cb: Callable[[Any], bool] = ...) -> None:
    ...

def num_fw_fixed_arguments(dynamo_gm_num_inputs: int, aot_fw_gm_num_inputs: int) -> int:
    "Computes the number of inputs to the aot fw graph which have fixed addresses (params and buffers)"
    ...

def count_tangents(fx_g: torch.fx.GraphModule) -> int:
    """
    Infers which inputs are static for a backwards graph
    """
    ...

@dataclasses.dataclass
class BoxedBool:
    value: bool
    def __bool__(self) -> bool:
        ...
    
    @staticmethod
    def disable(obj: Any) -> Union[BoxedBool, bool]:
        ...
    


@contextlib.contextmanager
def collect_defined_kernels(kernel_list: list[str]) -> Iterator[None]:
    ...

def get_cloned_parameter_buffer_name(name: str) -> str:
    ...

def is_gpu(device: Optional[str]) -> bool:
    ...

def device_need_guard(device: str) -> bool:
    ...

def needs_fallback_due_to_atomic_add_limitations(dtype: torch.dtype) -> bool:
    ...

def use_scatter_fallback(op_overload: torch._ops.OpOverload, reduction_type: Optional[str], self_dtype: torch.dtype, src_dtype: torch.dtype, src_device_type: str, src_is_tensor: bool) -> bool:
    ...

def dump_node_schedule(node_schedule: Sequence[BaseSchedulerNode]) -> None:
    """
    An API that can be used in pdb to dump a node_schedule.
    Right mainly dump the read/write dependencies but can add more as needed.
    """
    ...

def tensor_is_aligned(tensor: torch.Tensor) -> bool:
    ...

def should_assume_input_aligned(example_input: torch.Tensor) -> bool:
    ...

def maybe_get_suppress_shape_guards_ctx() -> contextlib.AbstractContextManager[None]:
    ...

def run_and_get_cpp_code(fn: Callable[P, _T], *args: P.args, **kwargs: P.kwargs) -> tuple[_T, str]:
    ...

def shape_env_from_inputs(inputs: Sequence[InputType]) -> Optional[ShapeEnv]:
    ...

def align_inputs_from_check_idxs(model: Callable[[list[InputType]], _T], inputs_to_check: Sequence[int], mutated_input_idxs: OrderedSet[int]) -> Callable[[list[InputType]], _T]:
    ...

def clone_preserve_strides(x: torch.Tensor) -> torch.Tensor:
    ...

def copy_misaligned_inputs(new_inputs: list[InputType], check_inputs_idxs: Sequence[int], return_pair_idxs: Optional[OrderedSet[int]] = ...) -> tuple[list[torch.Tensor], list[torch.Tensor]]:
    """
    Clones misaligned tensors which we inferred were aligned. Returns a tuple of [old_tensors], [new_tensors] for every
    cloned tensor which is in `return_pair_idxs`.
    """
    ...

def remove_unaligned_input_idxs(inputs: Sequence[InputType], static_input_idxs: Sequence[int]) -> Sequence[int]:
    """
    We require all inputs to be aligned, so introduce a copy for any
    that aren't.
    """
    ...

def expr_fits_within_32bit(e: sympy.Expr) -> bool:
    ...

def set_tracing_context_output_strides(example_inputs: Sequence[Any], compiled_graph: CompiledFxGraph) -> None:
    ...

def should_use_remote_fx_graph_cache() -> bool:
    ...

def normalize_name(name: str) -> str:
    ...

_triton_type_mapping = ...
_torch_triton_mapping = ...
_triton_type_re = ...
def triton_type(dtype: torch.dtype) -> str:
    """Convert torch.dtype to triton type"""
    ...

def triton_type_to_torch(dtype: str) -> torch.dtype:
    ...

def is_same_tensor(data: torch.Tensor, value: torch.Tensor) -> bool:
    ...

def is_same_mkldnn_tensor(data: torch.Tensor, value: torch.Tensor) -> bool:
    ...

@functools.cache
def boolean_ops() -> tuple[str, ...]:
    ...

@dataclasses.dataclass
class OpDtypeRule:
    type_promotion_kind: ELEMENTWISE_TYPE_PROMOTION_KIND
    override_return_dtype: Optional[torch.dtype]
    ...


op_dtype_propagation_rules: dict[str, OpDtypeRule] = ...
def register_op_dtype_propagation_rules(name: str, type_promotion_kind: ELEMENTWISE_TYPE_PROMOTION_KIND, override_return_dtype: Optional[torch.dtype]) -> None:
    ...

op_requires_libdevice_fp64: OrderedSet[str] = ...
def register_op_requires_libdevice_fp64(name: str) -> None:
    ...

def get_current_backend() -> str:
    ...

def upcast_compute_type(dtype: torch.dtype) -> torch.dtype:
    """Maybe upcast [b]float16 to float32"""
    ...

KeyType = TypeVar("KeyType")
ValType = TypeVar("ValType")
class ScopedDict(MutableMapping[KeyType, ValType]):
    """
    A dictionary-like object that allows for scoped updates. It maintains
    an original dictionary and a set of new items that can override
    the original items within the scope.  The original dictionary is
    unmodified.
    """
    def __init__(self, original_dict: Mapping[KeyType, ValType]) -> None:
        ...
    
    def __getitem__(self, key: KeyType) -> ValType:
        ...
    
    def __setitem__(self, key: KeyType, value: ValType) -> None:
        ...
    
    def __contains__(self, key: object) -> bool:
        ...
    
    def get(self, key: KeyType, default: Optional[ValType] = ...) -> Optional[ValType]:
        ...
    
    def __len__(self) -> int:
        ...
    
    def __iter__(self) -> Iterator[KeyType]:
        ...
    
    def __bool__(self) -> bool:
        ...
    
    def __delitem__(self, key: KeyType) -> None:
        ...
    


@dataclass_transform(frozen_default=True)
def ir_dataclass(cls: Optional[type[Any]] = ..., /, *, frozen: bool = ...) -> Any:
    ...

def get_donated_idxs() -> Optional[list[int]]:
    ...

def set_kernel_post_grad_provenance_tracing(node_schedule: Union[Sequence[BaseSchedulerNode], ExternKernelOut], kernel_name: str, is_extern: bool = ...) -> None:
    ...

class TritonAttrsDescriptorVersion(enum.Enum):
    V0_NO_TRITON = ...
    V1_COMPILER = ...
    V2_BACKENDS = ...
    V3_BACKENDS_TUPLE = ...
    V4_DICT = ...


@functools.cache
def get_triton_attrs_descriptor_version() -> TritonAttrsDescriptorVersion:
    ...

def triton_version_uses_attrs_dict() -> bool:
    ...

def is_cudagraph_unsafe_op(node: Operation) -> bool:
    """
    Returns True if the node is an op that is not cudagraphable.
    Usually only custom ops have this tag.
    """
    ...

def get_ld_library_path() -> str:
    ...

def is_codegen_graph_partition_subgraph(wrapper: PythonWrapperCodegen) -> bool:
    ...

def dtype_from_size(size: int) -> torch.dtype:
    ...

SUPPORTED_MKLDNN_DEVICES = ...
def is_mkldnn_bf16_supported(device_type: str) -> bool:
    """
    Returns True if the device supports MKL-DNN BF16.
    """
    ...

def is_mkldnn_fp16_supported(device_type: str) -> bool:
    """
    Returns True if the device supports MKL-DNN FP16.
    """
    ...

def tabulate_2d(elements: Sequence[Sequence[T]], headers: Sequence[T]) -> str:
    ...

def zip_dicts(dict1: Mapping[KeyType, ValType], dict2: Mapping[KeyType, ValType], d1_default: ValType | None = ..., d2_default: ValType | None = ...) -> Generator[tuple[KeyType, ValType | None, ValType | None], None, None]:
    """
    Zip two dictionaries together, replacing missing keys with default values.

    Args:
        dict1 (dict): The first dictionary.
        dict2 (dict): The second dictionary.
        d1_default (Any): the default value for the first dictionary
        d2_default (Any): the default value for the second dictionary

    Yields:
        tuple: A tuple containing the key, the value from dict1 (or d1_default if missing),
               and the value from dict2 (or d2_default if missing).
    """
    ...

