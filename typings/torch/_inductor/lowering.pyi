"""
This type stub file was generated by pyright.
"""

import contextlib
import sympy
import torch
import torch.fx
from collections.abc import Iterable, Sequence
from typing import Any, Callable, Optional, TYPE_CHECKING, TypeVar, Union
from typing_extensions import ParamSpec
from torch._higher_order_ops.associative_scan import associative_scan_op
from torch._higher_order_ops.triton_kernel_wrap import triton_kernel_wrapper_mutation
from torch._prims_common import ELEMENTWISE_TYPE_PROMOTION_KIND
from . import inductor_prims, ir
from .ir import IRNode, TensorBox
from .virtualized import ops
from .ops_handler import ReductionType

if TYPE_CHECKING:
    ...
_T = TypeVar("_T")
_P = ParamSpec("_P")
FALLBACK_ALLOW_LIST = ...
log = ...
lowerings: dict[Union[Callable[..., Any], str], Callable[..., Any]] = ...
_maybe_layout_constraints: dict[torch._ops.OpOverload, Optional[Callable[..., Any]]] = ...
fallbacks = ...
aten = ...
tr_c10d = ...
prims = ...
needs_realized_inputs = ...
foreach_ops = ...
inplace_foreach_ops = ...
inplaceable_foreach_ops: dict[torch._ops.OpOverload, torch._ops.OpOverload] = ...
quantized_decomposed = ...
def cur_node_has_non_foreach_users(): # -> bool:
    ...

def group_foreach_args(arg_pairs: Iterable[Union[tuple[Any, Any], Any]]): # -> defaultdict[Any, list[Any]]:
    ...

def maybe_layout_constraints(fn: Callable[..., Any]) -> Optional[Callable[..., Any]]:
    """Get layout constraints. Returns None if there are no layout constraints."""
    ...

def tag_to_layout_constraint(tag): # -> Callable[..., tuple[tuple[Any | dict[Any, Any | dict[Any, Any | dict[Any, Any] | list[Any] | tuple[Any, ...]] | list[Any] | tuple[Any, ...]] | list[Any] | tuple[Any, ...], ...], dict[Any, Any | dict[Any, Any | dict[Any, Any | dict[Any, Any] | list[Any] | tuple[Any, ...]] | list[Any] | tuple[Any, ...]] | list[Any] | tuple[Any, ...]]]] | Callable[..., tuple[PyTree, PyTree]] | Callable[..., tuple[tuple[Any | dict[Any, Any | dict[Any, Any | dict[Any, Any]]], ...], dict[str, Any | dict[Any, Any | dict[Any, Any | dict[Any, Any]]]]]] | None:
    ...

def assert_nyi(cond, msg): # -> None:
    ...

def add_needs_realized_inputs(fn): # -> list[list[Any] | None] | None:
    ...

def add_layout_constraint(fn, constraint): # -> None:
    ...

DTYPE_ID_LOOKUP = ...
def decode_dtype(dtype: int): # -> int:
    ...

def is_integer_type(x): # -> bool:
    ...

def is_boolean_type(x): # -> bool:
    ...

def get_promoted_dtype(*args, type_promotion_kind: ELEMENTWISE_TYPE_PROMOTION_KIND): # -> dtype:
    ...

def get_overloads(aten_fn): # -> list[Any]:
    ...

def in_namespace(op, namespace): # -> bool:
    ...

def transform_args(args: list[Any], kwargs: dict[str, Any], broadcast: bool, type_promotion_kind: Optional[ELEMENTWISE_TYPE_PROMOTION_KIND], convert_input_to_bool: bool) -> tuple[list[Any], dict[str, Any]]:
    ...

def register_lowering(aten_fn, broadcast=..., type_promotion_kind: Optional[ELEMENTWISE_TYPE_PROMOTION_KIND] = ..., convert_input_to_bool=..., lowering_dict=...) -> Callable[[Callable[_P, _T]], Callable[_P, _T]]:
    """
    Shim to support decorator syntax.
    """
    ...

def broadcast_symbolic_shapes(a, b): # -> tuple[Any, ...]:
    """
    Broadcasting logic based on symbolic shapes.

    We give the shapes 0 and 1 concrete values, while all other shapes
    are symbolic sympy formulas.
    """
    ...

def promote_constants(inputs, override_return_dtype=..., type_promotion_kind=...): # -> list[IndexingConstant | Constant] | list[Any]:
    ...

def make_pointwise(fn, override_return_dtype=..., override_device=..., override_fn_when_input_bool=..., allow_alpha=..., triton_fallback=...): # -> Callable[..., Any | TensorBox]:
    ...

def make_foreach_pointwise(pw_fn, allow_alpha=...): # -> Callable[..., list[None]]:
    ...

def to_dtype(x: TensorBox, dtype: torch.dtype, copy=...): # -> TensorBox:
    ...

def to_dtype_bitcast(x: TensorBox, dtype: torch.dtype, *, copy=...): # -> TensorBox | PyTree:
    ...

def to_device(x: TensorBox, device: torch.device, *, copy=..., non_blocking=...): # -> TensorBox | ShapeAsConstantBuffer:
    ...

def register_pointwise(aten_fn, name=..., broadcast=..., type_promotion_kind=..., convert_input_to_bool=..., override_return_dtype=..., override_fn_when_input_bool=..., allow_alpha=..., triton_fallback=...): # -> Callable[..., Any]:
    """A pointwise function that maps ops.{name} to inputs"""
    ...

def register_frexp(): # -> Callable[..., tuple[Any | TensorBox, Any | TensorBox]]:
    """A pointwise function that maps ops.frexp to inputs"""
    ...

def register_foreach_pointwise(aten_fn, pointwise_lowering_fn, allow_alpha=...): # -> _Wrapped[Callable[..., Any], list[None], Callable[..., Any], list[None]]:
    ...

@register_lowering(aten.where, broadcast=False, type_promotion_kind=None)
def where(cond, a, b): # -> TensorBox:
    ...

@register_lowering(aten.broadcast_tensors, broadcast=False, type_promotion_kind=None)
def broadcast_tensors(*inputs): # -> list[Any]:
    ...

@register_lowering([aten.alias, aten.detach, aten.detach_, aten.lift, prims.view_of])
def nop(x):
    ...

if hasattr(aten, "lift_fresh"):
    ...
@register_lowering(aten.squeeze, type_promotion_kind=None)
def squeeze(x, dim=...): # -> TensorBox:
    ...

@register_lowering(aten.squeeze_copy, type_promotion_kind=None)
def squeeze_copy(x, dim=...): # -> TensorBox:
    ...

@register_lowering([aten.squeeze_])
def squeeze_(x, dim=...): # -> TensorBox:
    ...

@register_lowering(aten.isinf)
def isinf(x): # -> TensorBox:
    ...

@register_lowering(aten.isnan)
def isnan(x): # -> TensorBox:
    ...

@register_lowering(aten.ceil)
def ceil(x): # -> TensorBox:
    ...

@register_lowering(aten.floor)
def floor(x): # -> TensorBox:
    ...

@register_lowering(aten.round.default)
def round(x): # -> TensorBox:
    ...

@register_lowering(aten.trunc)
def trunc(x): # -> TensorBox:
    ...

@register_lowering(aten.expand, type_promotion_kind=None)
def expand(x, sizes): # -> ReinterpretView | ExpandView | TensorBox:
    ...

@register_lowering(prims.broadcast_in_dim, type_promotion_kind=None)
def broadcast_in_dim(a, shape, broadcast_dimensions): # -> ReinterpretView | ExpandView | TensorBox:
    ...

@register_lowering(aten.expand_as, type_promotion_kind=None)
def expand_as(x, y): # -> ReinterpretView | ExpandView | TensorBox:
    ...

@register_lowering(aten.repeat)
def repeat(x, repeats): # -> TensorBox:
    ...

@register_lowering(aten._unsafe_view, type_promotion_kind=None)
@register_lowering(aten.view, type_promotion_kind=None)
@register_lowering(aten.reshape, type_promotion_kind=None)
def view(x, sizes): # -> TensorBox:
    ...

@register_lowering(aten.permute, type_promotion_kind=None)
def permute(x, dims): # -> TensorBox:
    ...

@register_lowering(aten.slice, type_promotion_kind=None)
def slice_(x, dim=..., start=..., end=..., step=..., clamp=...): # -> TensorBox:
    ...

@register_lowering(aten.as_strided, type_promotion_kind=None)
def as_strided(x, size, stride, storage_offset=...): # -> TensorBox:
    ...

@register_lowering(aten.as_strided_, type_promotion_kind=None)
def as_strided_(x, size, stride, storage_offset=...): # -> TensorBox:
    ...

@register_lowering(aten.as_strided_copy, type_promotion_kind=None)
def as_strided_copy(x, size, stride, storage_offset=...): # -> TensorBox:
    ...

def pointwise_cat(inputs, dim=...): # -> TensorBox:
    ...

@register_lowering(quantized_decomposed.quantize_per_channel, type_promotion_kind=None)
def quantized_decomposed_quantize_per_channel(input: TensorBox, scales: TensorBox, zero_points: TensorBox, axis: int, quant_min: int, quant_max: int, dtype: torch.dtype) -> TensorBox:
    ...

@register_lowering(quantized_decomposed.dequantize_per_channel, type_promotion_kind=None)
def quantized_decomposed_dequantize_per_channel(input: TensorBox, scales: TensorBox, zero_points: TensorBox, axis: int, quant_min: int, quant_max: int, dtype: torch.dtype, *, out_dtype: Optional[torch.dtype] = ...) -> TensorBox:
    ...

@register_lowering(quantized_decomposed.quantize_per_tensor.default, type_promotion_kind=None)
def quantized_decomposed_quantize_per_tensor_default(input: TensorBox, scale: float, zero_point: int, quant_min: int, quant_max: int, dtype: torch.dtype) -> TensorBox:
    ...

@register_lowering(quantized_decomposed.dequantize_per_tensor.default, type_promotion_kind=None)
def quantized_decomposed_dequantize_per_tensor_default(input: TensorBox, scale: float, zero_point: int, quant_min: int, quant_max: int, dtype: torch.dtype, *, out_dtype: Optional[torch.dtype] = ...) -> TensorBox:
    ...

@register_lowering(quantized_decomposed.quantize_per_tensor.tensor, type_promotion_kind=None)
def quantized_decomposed_quantize_per_tensor_tensor(input: TensorBox, scale: TensorBox, zero_point: TensorBox, quant_min: int, quant_max: int, dtype: torch.dtype) -> TensorBox:
    ...

@register_lowering(quantized_decomposed.dequantize_per_tensor.tensor, type_promotion_kind=None)
def quantized_decomposed_dequantize_per_tensor_tensor(input: TensorBox, scale: TensorBox, zero_point: TensorBox, quant_min: int, quant_max: int, dtype: torch.dtype, *, out_dtype: Optional[torch.dtype] = ...) -> TensorBox:
    ...

@register_lowering(aten.cat)
def cat(inputs, dim=...): # -> PyTree | TensorBox:
    ...

@register_lowering(aten.diagonal, type_promotion_kind=None)
def diagonal(input, offset: int = ..., dim1: int = ..., dim2: int = ...): # -> TensorBox:
    ...

@register_lowering(aten.diagonal_copy, type_promotion_kind=None)
def diagonal_copy(input, offset: int = ..., dim1: int = ..., dim2: int = ...): # -> TensorBox:
    ...

@register_lowering(aten.diagonal_scatter, type_promotion_kind=None)
def diagonal_scatter(input, src, offset: int = ..., dim1: int = ..., dim2: int = ...): # -> TensorBox:
    ...

@register_lowering(aten.select, type_promotion_kind=None)
def select(x, dim, idx): # -> TensorBox:
    ...

@register_lowering(aten.split, type_promotion_kind=None)
def split(x, sizes, dim=...): # -> list[Any]:
    ...

@register_lowering(aten.split_with_sizes, type_promotion_kind=None)
def split_with_sizes(x, sizes, dim=...): # -> list[Any]:
    ...

@register_lowering(aten.unbind, type_promotion_kind=None)
def unbind(x, dim=...): # -> list[TensorBox]:
    ...

@register_lowering(aten.unfold, type_promotion_kind=None)
def unfold(x, dimension, size, step): # -> TensorBox:
    ...

@register_lowering(aten.unsqueeze, type_promotion_kind=None)
def unsqueeze(x, dim): # -> TensorBox:
    ...

@register_lowering(aten.unsqueeze_, type_promotion_kind=None)
def unsqueeze_(x, dim): # -> TensorBox:
    ...

@register_lowering(aten.glu)
def glu(x, dim=...): # -> TensorBox:
    ...

def fallback_handler(kernel, add_to_fallback_set=...): # -> Callable[..., PyTree]:
    ...

def unsupported_input_tensor(t: torch.Tensor, node=...): # -> bool:
    "Do not support reading or writing to this tensor"
    ...

def unsupported_output_tensor(t: torch.Tensor, node=...): # -> bool:
    "Do not support writing tensor but can read from it"
    ...

def fallback_node_due_to_unsupported_type(node: torch.fx.Node, allow_cpu_inputs=...): # -> bool:
    ...

def make_fallback(op, layout_constraint=..., warn=..., override_decomp=...): # -> None:
    ...

def philox_rand_offset(shape): # -> TensorBox:
    """
    TorchInductor offset calculation differs from PyTorch eager offset
    calculation for random ops (tl.rand vs torch.rand). In future, we should
    strive for same impl for tl.rand and torch.rand.
    """
    ...

@register_lowering(torch.ops.rngprims.philox_rand, type_promotion_kind=None)
def philox_rand(size, seed, offset, stride, device, dtype): # -> tuple[TensorBox, TensorBox]:
    ...

@register_lowering(aten.native_dropout, type_promotion_kind=None)
def native_dropout(x, p, train): # -> PyTree:
    ...

@register_lowering(aten.bernoulli_, type_promotion_kind=None)
def bernoulli_(x, *args):
    ...

@register_lowering(aten.bernoulli.p, type_promotion_kind=None)
def bernoulli_p(x, *args):
    ...

def warn_triton_random(): # -> None:
    ...

fallback_rand_default = ...
fallback_rand_generator = ...
fallback_randn_default = ...
fallback_randn_generator = ...
@register_lowering(aten.rand)
def rand(*args, **kwargs): # -> PyTree:
    ...

@register_lowering(aten.randn)
def randn(*args, **kwargs): # -> PyTree:
    ...

@register_lowering(inductor_prims.force_stride_order, type_promotion_kind=None)
def inductor_force_stride_order(input_tensor, stride): # -> Any:
    ...

@register_lowering(inductor_prims.seed, type_promotion_kind=None)
def inductor_seed(device: torch.device):
    ...

@register_lowering(inductor_prims.seeds, type_promotion_kind=None)
def inductor_seeds(count, device): # -> ShapeAsConstantBuffer | TensorBox:
    ...

@register_lowering(inductor_prims.lookup_seed, type_promotion_kind=None)
def inductor_lookup_seed(seeds, index): # -> TensorBox:
    ...

@register_lowering(inductor_prims.random, type_promotion_kind=None)
def inductor_random(size: list[int], seed: TensorBox, mode: str, *, offset: int = ...): # -> TensorBox:
    ...

@register_lowering(inductor_prims.randint, type_promotion_kind=None)
def inductor_randint(low: int, high: int, size: list[int], seed: TensorBox, *, offset: int = ...): # -> TensorBox:
    ...

@register_lowering(aten.searchsorted.Tensor, type_promotion_kind=None)
def searchsorted(sorted_sequence: TensorBox, self: TensorBox, *, out_int32: bool = ..., right: bool = ..., side: Optional[str] = ..., sorter: Optional[TensorBox] = ...) -> TensorBox:
    ...

@register_lowering(aten.bucketize, type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.NO_OPMATH)
def bucketize(input: TensorBox, boundaries: TensorBox, *, out_int32: bool = ..., right: bool = ...): # -> PyTree | TensorBox:
    ...

def require_dense(_, *args, **kwargs): # -> tuple[PyTree, PyTree]:
    ...

def require_contiguous(_, *args, **kwargs): # -> tuple[PyTree, PyTree]:
    ...

def require_contiguous_strides(_, *args, **kwargs): # -> tuple[PyTree, PyTree]:
    ...

def require_channels_last(_, *args, **kwargs): # -> tuple[PyTree, PyTree]:
    ...

def constrain_to_fake_tensor(arg, fake_arg): # -> Any | dict[Any, Any | dict[Any, Any | dict[Any, Any] | list[Any] | tuple[Any, ...]] | list[Any] | tuple[Any, ...]] | list[Any] | tuple[Any, ...]:
    ...

def constrain_to_fake_tensors(args, kwargs, fake_args, fake_kwargs): # -> tuple[tuple[Any | dict[Any, Any | dict[Any, Any | dict[Any, Any] | list[Any] | tuple[Any, ...]] | list[Any] | tuple[Any, ...]] | list[Any] | tuple[Any, ...], ...], dict[Any, Any | dict[Any, Any | dict[Any, Any | dict[Any, Any] | list[Any] | tuple[Any, ...]] | list[Any] | tuple[Any, ...]] | list[Any] | tuple[Any, ...]]]:
    ...

def constrain_to_fx_strides(fx_node, *args, **kwargs): # -> tuple[tuple[Any | dict[Any, Any | dict[Any, Any | dict[Any, Any]]], ...], dict[str, Any | dict[Any, Any | dict[Any, Any | dict[Any, Any]]]]]:
    ...

def sdpa_constraint(fx_node, *args, **kwargs): # -> tuple[tuple[Any, ...], dict[str, Any]]:
    ...

if torch.xpu.is_available():
    ...
@register_lowering(aten.copy, type_promotion_kind=None)
def copy(self, src, non_blocking=...): # -> TensorBox:
    ...

@register_lowering(aten.clone)
def clone(x, *, memory_format=...): # -> TensorBox:
    ...

def clone_preserve_reinterpret_view(x): # -> TensorBox:
    ...

if hasattr(aten, "lift_fresh_copy"):
    ...
@register_lowering(prims.iota)
def iota(length, *, start, step, dtype, device, requires_grad): # -> TensorBox:
    ...

@register_lowering(aten.select_scatter, type_promotion_kind=None)
def select_scatter(x, src, dim: int, index: int): # -> TensorBox:
    ...

@register_lowering(aten.slice_scatter, type_promotion_kind=None)
def slice_scatter(x, src, dim=..., start=..., end=..., step=...): # -> TensorBox:
    ...

@register_lowering([torch.tensor, aten.scalar_tensor])
def tensor(data, *, dtype=..., device=..., layout=..., pin_memory=...): # -> TensorBox:
    ...

@register_lowering(torch.as_tensor)
def as_tensor(data, dtype=..., device=...): # -> TensorBox | ShapeAsConstantBuffer:
    ...

@register_lowering(torch.LongTensor)
def long_tensor(data): # -> TensorBox:
    ...

@register_lowering(aten.full_like, type_promotion_kind=None)
def full_like(x, fill_value, **kwargs):
    ...

def tensor_constructor(fill_value): # -> Callable[..., TensorBox]:
    ...

@register_lowering([torch.empty, aten.empty])
def empty(*size, names=..., dtype=..., layout=..., device=..., pin_memory=..., memory_format=...): # -> TensorBox:
    ...

def create_tensor_like(creation_fn): # -> Callable[..., Any]:
    """
    Shim to convert X_like(...) into X(...).  For example zeros_like() into zeros().
    """
    ...

def constant_like(fill_value): # -> Callable[..., Any]:
    ...

empty_like = ...
ones_like = ...
zeros_like = ...
def new_constant(fill_value): # -> Callable[..., TensorBox]:
    ...

@register_lowering(aten.new_empty)
def new_empty(x, size, *, dtype=..., layout=..., device=..., pin_memory=...): # -> TensorBox:
    ...

@register_lowering(aten.empty_strided)
def empty_strided(size, stride, *, dtype=..., layout=..., device=..., pin_memory=...): # -> TensorBox:
    ...

@register_lowering(aten.new_empty_strided)
def new_empty_strided(x, size, stride, *, dtype=..., layout=..., device=..., pin_memory=...): # -> TensorBox:
    ...

@register_lowering(prims.copy_strided.default)
def copy_strided(x, stride): # -> Any:
    ...

@register_lowering([torch.full, aten.full])
def full(size, fill_value, **kwargs): # -> TensorBox:
    ...

@register_lowering(aten.gather, type_promotion_kind=None)
def gather(x, dim, index, sparse_grad=...): # -> TensorBox:
    ...

@register_lowering(aten.embedding, type_promotion_kind=None)
def embedding(weight, indices, padding_idx=..., scale_grad_by_freq=..., sparse=...): # -> PyTree | TensorBox:
    ...

def check_and_broadcast_indices(indices, device): # -> tuple[list[None], list[int]]:
    ...

def index_output_size_and_inner_fn(x_size, indices, tensor_indices, tensor_size, indices_loaders, indexed_size, x_loader, check, wrap_neg=...): # -> tuple[Any, Callable[..., list[Any] | Any]]:
    ...

def index_impl(x, indices, check): # -> TensorBox:
    ...

def index_impl_helper(x, indices, check, wrap_neg=...): # -> tuple[Any, Callable[..., Any], Callable[..., list[Any] | Any]]:
    ...

@register_lowering(aten.index, type_promotion_kind=None)
def index(x, indices): # -> TensorBox | PyTree:
    ...

@register_lowering(aten.index_put, type_promotion_kind=None)
def index_put(x, indices, values, accumulate=...): # -> TensorBox:
    ...

def index_put_as_masked_fill(self, indices, value, accumulate): # -> TensorBox:
    ...

def index_put_fallback(self, indices, values, accumulate):
    ...

@register_lowering(aten.index_put_, type_promotion_kind=None)
def index_put_(self, indices, values, accumulate=...): # -> TensorBox:
    ...

def index_put_impl_(self, indices, values, accumulate, check, may_realize=...): # -> TensorBox:
    ...

fallback__unsafe_masked_index = ...
fallback__unsafe_masked_index_put_accumulate = ...
@make_pointwise
def clamp(a, min, max): # -> Any:
    ...

@register_lowering(aten.as_strided_scatter, type_promotion_kind=None)
def as_strided_scatter(self, src, size, stride, storage_offset=...): # -> TensorBox:
    ...

@register_lowering(aten.scatter, type_promotion_kind=None)
def scatter(x, dim: int, index, src, **kwargs): # -> TensorBox:
    ...

def scatter_fallback(op_overload: torch._ops.OpOverload, self, dim: int, index, src, *, reduce: Optional[str] = ..., include_self: bool = ...): # -> None:
    ...

@register_lowering(aten.scatter_, type_promotion_kind=None)
def scatter_(self, dim: int, index, src, *, reduce: Optional[str] = ...): # -> TensorBox:
    ...

@register_lowering(aten.scatter_add, type_promotion_kind=None)
def scatter_add(x, dim: int, index, src): # -> TensorBox:
    ...

@register_lowering(aten.scatter_add_, type_promotion_kind=None)
def scatter_add_(x, dim: int, index, src): # -> TensorBox:
    ...

@register_lowering(aten.scatter_reduce, type_promotion_kind=None)
def scatter_reduce(x, dim: int, index, src, reduction_type, **kwargs): # -> TensorBox:
    ...

@register_lowering(aten.scatter_reduce_, type_promotion_kind=None)
def scatter_reduce_(self, dim: int, index, src, reduce, *, include_self: bool = ...): # -> TensorBox:
    ...

def upsample_nearestnd(x, output_size, scales_x: tuple[Optional[float], ...], n: int = ..., exact: bool = ...): # -> TensorBox:
    ...

@register_lowering(aten.upsample_nearest1d.default)
def upsample_nearest1d(x, output_size, scales: Optional[float] = ...): # -> TensorBox:
    ...

@register_lowering(aten.upsample_nearest2d.default)
def upsample_nearest2d(x, output_size, scales_h: Optional[float] = ..., scales_w: Optional[float] = ...): # -> TensorBox:
    ...

@register_lowering(aten.upsample_nearest3d.default)
def upsample_nearest3d(x, output_size, scales_d: Optional[float] = ..., scales_h: Optional[float] = ..., scales_w: Optional[float] = ...): # -> TensorBox:
    ...

@register_lowering(prims.rev.default)
def rev(x, dims): # -> TensorBox:
    ...

def inplace_constant_pad_nd(x: TensorBox, padding: Sequence[int], fill_value: float) -> Optional[TensorBox]:
    """
    This optimization changes the semantics of padding from 'clone'
    style to 'view' style.

    Thanks to functionalization, this change can still maintain numerical
    correctness.
    """
    ...

@register_lowering(aten.constant_pad_nd, type_promotion_kind=None)
def constant_pad_nd(x, padding, fill_value=...): # -> TensorBox:
    ...

def range_mask_low(i: sympy.Expr, low: Union[sympy.Expr, int]): # -> Any:
    ...

def range_mask_high(i: sympy.Expr, high: sympy.Expr): # -> Any:
    ...

def range_mask(i: sympy.Expr, high: sympy.Expr, low: sympy.Expr): # -> Any:
    ...

def constant_boundary_condition(x, fill_value, padding=..., pad_fill_value=..., dim=...): # -> Callable[..., Any]:
    ...

def pooling_size(x, i, kernel_size, stride, padding, ceil_mode, *, dilation=...): # -> tuple[type[AppliedUndef] | Any, Any | Literal[False]]:
    ...

def should_fallback_max_pool_with_indices(kernel_size, *, n_dim): # -> Any:
    ...

def max_pool_checks(x, kernel_size, stride, padding, dilation, n_dim, *, assert_fallback=...): # -> tuple[Sequence[int], Sequence[int], Sequence[int], Sequence[int], Any]:
    ...

@register_lowering(aten.max_pool2d_with_indices, type_promotion_kind=None)
def max_pool2d_with_indices(x, kernel_size, stride=..., padding=..., dilation=..., ceil_mode=...): # -> tuple[TensorBox, TensorBox]:
    ...

@register_lowering(aten.max_pool3d_with_indices, type_promotion_kind=None)
def max_pool3d_with_indices(x, kernel_size, stride=..., padding=..., dilation=..., ceil_mode=...): # -> tuple[TensorBox, TensorBox]:
    ...

fallback_max_pool2d_with_indices_backward = ...
@register_lowering(aten.max_pool2d_with_indices_backward, type_promotion_kind=None)
def max_pool2d_with_indices_backward(grad_output, x, kernel_size, stride, padding, dilation, ceil_mode, indices): # -> PyTree | TensorBox:
    ...

def pad_adaptive_loader(x, pad_val=...): # -> Callable[..., Any]:
    ...

def compute_indices_adaptive_pooling(start_index, end_index, h_in, w_in, h_out, w_out): # -> tuple[partial[Any], partial[Any], partial[Any], partial[Any]]:
    ...

fallback_adaptive_avg_pool2d = ...
fallback_adaptive_max_pool2d = ...
@register_lowering(aten.adaptive_max_pool2d)
def adaptive_max_pool2d(x, output_size): # -> tuple[TensorBox, TensorBox] | PyTree:
    ...

@register_lowering(aten.fractional_max_pool2d)
def fractional_max_pool2d(x, kernel_size, output_size, random_samples): # -> tuple[TensorBox, TensorBox]:
    ...

@register_lowering(aten.fractional_max_pool3d)
def fractional_max_pool3d(x, kernel_size, output_size, random_samples): # -> tuple[TensorBox, TensorBox]:
    ...

@register_lowering(aten.upsample_nearest2d_backward.default)
def upsample_nearest2d_backward(x, output_size=..., input_size=..., scales_h=..., scales_w=...): # -> PyTree | TensorBox:
    ...

fallback_avg_pool2d = ...
fallback_avg_pool3d = ...
@register_lowering(aten.avg_pool2d, type_promotion_kind=None)
def avg_pool2d(x, kernel_size, stride=..., padding=..., ceil_mode=..., count_include_pad=..., divisor_override=...): # -> PyTree | TensorBox:
    ...

@register_lowering(aten.avg_pool3d, type_promotion_kind=None)
def avg_pool3d(x, kernel_size, stride=..., padding=..., ceil_mode=..., count_include_pad=..., divisor_override=...): # -> PyTree | TensorBox:
    ...

fallback_avg_pool2d_backward = ...
@register_lowering(aten.avg_pool2d_backward, type_promotion_kind=None)
def avg_pool2d_backward(grad_output, x, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override=...): # -> PyTree | TensorBox:
    ...

fallback_avg_pool3d_backward = ...
@register_lowering(aten.avg_pool3d_backward, type_promotion_kind=None)
def avg_pool3d_backward(grad_output, x, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override=...): # -> PyTree | TensorBox:
    ...

def make_reduction(reduction_type: ReductionType, override_return_dtype=...): # -> Callable[..., TensorBox]:
    ...

@register_lowering(aten.mean)
def mean(x, axis=..., keepdim=..., *, dtype=...): # -> TensorBox:
    ...

def var_mean_sum_(x, axis, correction, keepdim, return_mean): # -> tuple[Any | TensorBox] | tuple[Any | TensorBox, TensorBox | Any]:
    ...

def use_two_step_variance(x, axis, keepdim): # -> bool:
    ...

def var_mean_welford_(x, axis, *, correction, keepdim, return_mean): # -> tuple[Any | TensorBox, TensorBox] | tuple[Any | TensorBox]:
    ...

def var_mean_helper_(x, *, axis, correction, keepdim, return_mean): # -> TensorBox | tuple[TensorBox | Any, ...]:
    ...

@register_lowering([aten.var, prims.var])
def var_(x, axis=..., *, correction=..., keepdim=...): # -> TensorBox | tuple[TensorBox | Any, ...]:
    ...

@register_lowering(aten.var_mean)
def var_mean(x, axis=..., *, correction=..., keepdim=...): # -> TensorBox | tuple[TensorBox | Any, ...]:
    ...

def pow_recursive(x, y, dtype): # -> Any:
    ...

@make_pointwise
def pow_native(a, b): # -> Any:
    ...

fallback_pow_tensor_tensor = ...
fallback_pow_scalar = ...
fallback_pow_tensor_scalar = ...
@register_lowering(aten.pow, broadcast=True)
def pow(a, b): # -> TensorBox | PyTree:
    ...

def mutate_to(changed, val, unsafe_alias=...): # -> TensorBox:
    ...

@register_lowering(aten.fill_)
def fill_(x, fill_value): # -> TensorBox:
    ...

@register_lowering(aten.copy_, type_promotion_kind=None)
def copy_(dst, src, non_blocking=...): # -> TensorBox:
    ...

@make_pointwise
def floordiv(a, b): # -> Any:
    ...

@make_pointwise
def truncdiv(a, b): # -> Any:
    ...

@register_lowering(aten.div, broadcast=True)
def div_mode(a, b, rounding_mode=...): # -> Any | TensorBox:
    ...

@register_lowering([aten.mul], broadcast=True)
def mul(a, b): # -> TensorBox:
    ...

def get_constant_value(x: ir.IRNode) -> Optional[ir.Constant]:
    """Try convert an arbitrary IR node into an ir.Constant value"""
    ...

@register_lowering([prims.div], broadcast=True)
def div_prim(a, b): # -> Any | TensorBox:
    ...

@register_lowering([aten.true_divide, aten.div.Tensor], broadcast=True, type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def div(a, b): # -> Any | TensorBox:
    ...

@register_lowering([aten.fmod, prims.fmod], broadcast=True)
def fmod(a, b): # -> TensorBox:
    ...

@register_lowering([aten.sum, prims.sum])
def sum_(x, axis=..., keepdims=..., *, dtype=...): # -> TensorBox:
    ...

fallback_cumsum = ...
fallback_cumprod = ...
fallback_logcumsumexp = ...
fallback_cummax = ...
fallback_cummin = ...
@register_lowering(aten.cumsum)
def cumsum(x, axis=..., dtype=...): # -> TensorBox | PyTree:
    ...

@register_lowering(aten.cumprod)
def cumprod(x, axis=..., dtype=...): # -> TensorBox | PyTree:
    ...

@register_lowering(aten.logcumsumexp)
def logcumsumexp(x, dim): # -> TensorBox | PyTree:
    ...

@register_lowering(aten.cummax, type_promotion_kind=None)
def cummax(x, axis=...): # -> tuple[TensorBox, Any] | PyTree | tuple[TensorBox, TensorBox | None]:
    ...

@register_lowering(aten.cummin, type_promotion_kind=None)
def cummin(x, axis=...): # -> tuple[TensorBox, Any] | PyTree | tuple[TensorBox, TensorBox | None]:
    ...

@register_lowering(aten.prod)
def prod(x, axis=..., keepdims=..., *, dtype=...): # -> TensorBox:
    ...

@register_lowering(aten.any)
def reduce_any(x, dim=..., keepdim=...): # -> TensorBox:
    ...

@register_lowering(aten.max, type_promotion_kind=None)
def reduce_max(x, dim=..., keepdim=...): # -> tuple[TensorBox, TensorBox] | TensorBox:
    ...

@register_lowering(aten.min, type_promotion_kind=None)
def reduce_min(x, dim=..., keepdim=...): # -> tuple[TensorBox, TensorBox] | TensorBox:
    ...

reduce_amax = ...
reduce_amin = ...
reduce_argmax = ...
reduce_argmin = ...
add = ...
sort_fallback = ...
@register_lowering(aten.sort.stable, type_promotion_kind=None)
def sort_stable(x, *, stable=..., dim=..., descending=...): # -> tuple[TensorBox, TensorBox] | PyTree | tuple[TensorBox, TensorBox | Any]:
    ...

@register_lowering(aten.sort.default, type_promotion_kind=None)
def sort(x, dim=..., descending=...): # -> tuple[TensorBox, TensorBox] | PyTree | tuple[TensorBox, TensorBox | Any]:
    ...

def register_pointwise_numeric(op, name=..., triton_fallback=...): # -> Callable[..., Any | TensorBox]:
    ...

def register_pointwise_numeric_ldf64(op: torch._ops.OpOverloadPacket): # -> Callable[..., Any | TensorBox]:
    ...

rsqrt = ...
exp = ...
exp2 = ...
expm1 = ...
relu = ...
sigmoid = ...
sqrt = ...
square = ...
sub = ...
abs = ...
bitwise_and = ...
bitwise_left_shift = ...
bitwise_not = ...
bitwise_or = ...
bitwise_right_shift = ...
bitwise_xor = ...
erf = ...
logical_and = ...
logical_not = ...
logical_or = ...
logical_xor = ...
maximum = ...
minimum = ...
neg = ...
abs = ...
reciprocal = ...
sign = ...
gt = ...
foreach_add_list = ...
foreach_add_scalar = ...
foreach_mul_list = ...
foreach_mul_scalar = ...
foreach_div_list = ...
foreach_div_scalar = ...
def register_foreach_inplace(aten_op, outplace_aten_op, outplace_op): # -> None:
    ...

def register_inplace(aten_op, outplace_op): # -> Callable[..., TensorBox | Any]:
    ...

@register_lowering(aten.sym_constrain_range)
def sym_constrain_range(a, min=..., max=...): # -> None:
    ...

@register_lowering(aten.sym_size.int)
def sym_size(a, dim):
    ...

@register_lowering(aten.sym_stride.int)
def sym_stride(a, dim):
    ...

@register_lowering(aten.sym_numel)
def sym_numel(a):
    ...

@register_lowering(torch.sym_sum)
def sym_sum(args): # -> Expr:
    ...

@register_lowering(aten._foobar)
def foobar(self, *args, **kwargs):
    ...

@register_lowering(torch.ops.inductor.resize_storage_bytes_)
def resize_storage_bytes_(variable, new_size):
    ...

@register_lowering(torch.ops.aten.set_.source_Tensor)
def set__source_tensor(self, source_tensor): # -> ShapeAsConstantBuffer | TensorBox:
    ...

if hasattr(torch.ops.fsdp, "copy_"):
    @register_lowering(torch.ops.fsdp.copy_.default)
    def fsdp_copy_(dst, src): # -> TensorBox:
        ...
    
@register_lowering(torch.ops.aten.resize)
def resize(x, size, *, memory_format=...): # -> TensorBox:
    ...

@register_lowering(triton_kernel_wrapper_mutation)
def triton_kernel_wrap_(*, kernel_idx, constant_args_idx, grid, tma_descriptor_metadata, kwargs): # -> dict[Any, TensorBox]:
    ...

@register_lowering(torch.ops.higher_order.cond, type_promotion_kind=None)
def cond(pred, true_fn, false_fn, operands): # -> list[ShapeAsConstantBuffer | TensorBox]:
    ...

@register_lowering(torch.ops.higher_order.while_loop, type_promotion_kind=None)
def while_loop(cond_fn, body_fn, carried_inputs, additional_inputs): # -> list[ShapeAsConstantBuffer | TensorBox]:
    ...

@register_lowering(torch.ops.higher_order.invoke_subgraph, type_promotion_kind=None)
def invoke_subgraph(subgraph_fn: ir.Subgraph, identifier: str, *operands): # -> list[ShapeAsConstantBuffer | TensorBox]:
    ...

@register_lowering(torch._higher_order_ops.invoke_quant, type_promotion_kind=None)
def invoke_quant_tracer(subgraph_fn: ir.Subgraph, *operands, scheme=...): # -> Any | None:
    ...

@register_lowering(associative_scan_op, type_promotion_kind=None)
def associative_scan(combine_fn: ir.Subgraph, xs, additional_inputs: tuple[torch.Tensor]):
    ...

@register_lowering(torch.ops.higher_order.with_effects, type_promotion_kind=None)
def with_effects(token, op, *args, **kwargs): # -> tuple[Any] | tuple[Any, PyTree] | tuple[Any, *tuple[Any, ...]]:
    ...

@register_lowering(inductor_prims.prepare_softmax_online, type_promotion_kind=None)
def prepare_softmax_online(x, dim): # -> tuple[TensorBox, TensorBox]:
    """
    Lowering inductor_prims.prepare_softmax_online to compute max/sum in one pass if no split is needed.
    """
    ...

@contextlib.contextmanager
def force_fallback(op: torch._ops.OpOverload): # -> Generator[None, Any, None]:
    """
    A context manager to force fallback an op. Used in unit test
    for FallbackKernel.
    """
    ...

