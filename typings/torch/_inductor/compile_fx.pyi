"""
This type stub file was generated by pyright.
"""

import contextlib
import enum
import torch.fx
from abc import ABC, abstractmethod
from contextlib import AbstractContextManager
from typing import Any, Callable, Optional, TYPE_CHECKING, TypeVar, Union
from typing_extensions import ParamSpec, Protocol, TypedDict, Unpack, override
from torch import fx
from torch._inductor.cudagraph_utils import BoxedDeviceIndex, PlaceholderInfo
from torch._inductor.output_code import OutputCode
from torch._inductor.utils import BoxedBool, InputType
from torch.fx import GraphModule
from .ir import ExternKernelNode
from collections.abc import Generator, Sequence
from torch._ops import OpOverload
from torch.export.pt2_archive._package_weights import Weights

if TYPE_CHECKING:
    ...
_P = ParamSpec("_P")
_T = TypeVar("_T")
if TYPE_CHECKING or not config.is_fbcode():
    def time_and_log(attr: str) -> Callable[[Callable[_P, _T]], Callable[_P, _T]]:
        ...
    
    def log_optimus_to_scuba(*args: object, **kwargs: object) -> None:
        ...
    
else:
    ...
if TYPE_CHECKING:
    ...
class FxCompileMode(enum.Enum):
    NORMAL = ...
    SERIALIZE = ...
    SUBPROCESS = ...


log = ...
perf_hint_log = ...
pre_grad_graphs_log = ...
post_grad_graphs_log = ...
static_inputs_log = ...
inductor_metrics_log = ...
def get_static_input_idxs(num_fixed: int) -> list[int]:
    ...

def record_original_output_strides(gm: GraphModule) -> None:
    ...

def split_const_gm(gm: GraphModule, skip_constructor: bool = ..., lifted_constant_names: Optional[list[str]] = ..., skip_folding_node_fn: Optional[Callable[[torch.fx.Node], bool]] = ...) -> tuple[GraphModule, dict[str, int]]:
    """
    This function takes an GraphModule input "gm".
    The gm will be split into 2 components,
      1) const_gm, which consists the subgraph of gm that can be constant folded.
      2) gm (being inplace modified,) which returns the graph after constant folding.

    If an additional "lifted_constants" argument is passed in, we will assume the gm has
    been lifted and run the transformation accordingly.

    When a "skip_folding_node_fn" callback is passed, we will skip constant folding on
    the nodes for which the callback returns True.

    const_output_index is a mapping of corresponding node name from gm to the
    output index of const_gm.
    Returns (const_gm, const_output_index)
    """
    ...

def is_tf32_warning_applicable(gm: GraphModule) -> bool:
    ...

def maybe_disable_comprehensive_padding(example_inputs: Sequence[InputType]) -> AbstractContextManager[None, None]:
    """
    For CPU backend, enable comprehensive padding causes some unit tests
    fail due to changing number of generated kernels. Skip for now.
    """
    ...

def maybe_disable_graph_partition(cpp_wrapper: bool, aot_mode: bool) -> AbstractContextManager[None, None]:
    """
    graph partition does not support cpp_wrapper and aot_mode yet.
    """
    ...

def fake_tensor_prop(gm: GraphModule, example_inputs: Sequence[InputType], force_allow_non_fake_inputs: bool = ...) -> torch._subclasses.FakeTensorMode:
    """
    If we can not detect fake mode from the context of inputs, create one.

    The created fake mode will be returned.
    """
    ...

def get_patched_config_dict(config_patches: Optional[Union[str, dict[str, Any]]] = ...) -> dict[str, Any]:
    ...

@contextlib.contextmanager
def with_fresh_cache_if_config() -> Generator[None, None, None]:
    ...

class _CompileFxKwargs(TypedDict, total=False):
    cudagraphs: Optional[BoxedBool]
    static_input_idxs: Sequence[int]
    is_backward: bool
    graph_id: Optional[int]
    cpp_wrapper: bool
    aot_mode: bool
    is_inference: bool
    layout_opt: Optional[bool]
    extern_node_serializer: Optional[Callable[[list[ExternKernelNode]], Any]]
    boxed_forward_device_index: Optional[BoxedDeviceIndex]
    ...


class _CompileFxCallable(Protocol):
    def __call__(self, gm: GraphModule, example_inputs: Sequence[InputType], **kwargs: Unpack[_CompileFxKwargs]) -> OutputCode:
        ...
    


def compile_fx_inner(gm: GraphModule, example_inputs: Sequence[InputType], **kwargs: Unpack[_CompileFxKwargs]) -> OutputCode:
    ...

class _FxCompileStat:
    codegen_and_compile: int = ...
    def __repr__(self) -> str:
        ...
    


class FxCompile(ABC):
    """
    An FxCompile represents a mechanism that can turn a GraphModule into an
    OutputCode.
    """
    _compile_stats: dict[type[FxCompile], _FxCompileStat] = ...
    @abstractmethod
    def codegen_and_compile(self, gm: GraphModule, example_inputs: Sequence[InputType], inputs_to_check: Sequence[int], graph_kwargs: _CompileFxKwargs) -> OutputCode:
        ...
    


class _InProcessFxCompile(FxCompile):
    @override
    def codegen_and_compile(self, gm: GraphModule, example_inputs: Sequence[InputType], inputs_to_check: Sequence[int], graph_kwargs: _CompileFxKwargs) -> OutputCode:
        """
        Generates the OutputCode from the GraphModule and example_inputs.
        """
        ...
    


def fx_codegen_and_compile(gm: GraphModule, example_inputs: Sequence[InputType], inputs_to_check: Sequence[int], **graph_kwargs: Unpack[_CompileFxKwargs]) -> OutputCode:
    ...

def get_input_idxs_to_check(inputs: Sequence[InputType], static_input_idxs: Sequence[int]) -> Sequence[int]:
    """
    This function runs at compile time, and generates a list of indices for which we
    might need to do a copy to preserve alignment requirements.
    """
    ...

def cudagraphify(model: Callable[..., Any], static_input_idxs: Sequence[int] = ..., *, device_index: int, stack_traces: list[Optional[str]], is_backward: bool, is_inference: bool, constants: tuple[torch.Tensor, ...] = ..., placeholders: Sequence[PlaceholderInfo] = ..., mutated_input_idxs: tuple[int, ...] = ...) -> Callable[..., Any]:
    ...

def static_input(x: torch.Tensor) -> torch.Tensor:
    """
    Copy and input while preserving strides
    """
    ...

def index_expanded_dims_and_copy_(dst: torch.Tensor, src: torch.Tensor, expanded_dims: list[int]) -> None:
    "Index into expanded dimensions of both dst and src then copy_"
    ...

def cudagraphify_impl(model: Callable[..., Any], inputs: list[torch.Tensor], static_input_idxs: Sequence[int] = ...) -> Callable[[list[InputType]], Any]:
    """
    Assumes inputs[static_input_idxs[i]] are always the same memory address
    """
    ...

def compile_fx_aot(model_: GraphModule, example_inputs_: list[InputType], inner_compile: _CompileFxCallable = ..., config_patches: Optional[dict[str, str]] = ...) -> Union[list[Union[str, Weights]], str]:
    ...

_graph_counter = ...
def fw_compiler_freezing(aot_autograd_model: GraphModule, aot_example_inputs: Sequence[InputType], dynamo_model: GraphModule, num_example_inputs: int, inner_compile: Callable[..., Any], cudagraphs: BoxedBool, graph_id: int, forward_device: BoxedDeviceIndex) -> Callable[[list[object]], Sequence[torch.Tensor]]:
    ...

def get_cpp_wrapper_config() -> dict[str, object]:
    ...

def get_cuda_device_context(gm: torch.fx.GraphModule) -> AbstractContextManager[None]:
    """
    Returns a cuda device context manager if there is a single device in the graph
    """
    ...

def compile_fx(model_: GraphModule, example_inputs_: Sequence[InputType], inner_compile: Callable[..., OutputCode] = ..., config_patches: Optional[dict[str, Any]] = ..., decompositions: Optional[dict[OpOverload, Callable[..., Any]]] = ..., ignore_shape_env: bool = ...) -> Union[Callable[[list[object]], Sequence[torch.Tensor]], str, list[str], Weights]:
    """
    Main entry point for compiling given FX graph.  Despite the fact that this
    lives in :mod:`torch._inductor`, this function is responsible for calling
    into AOT Autograd (and we will eventually get a callback to
    ``inner_compile`` to perform actual compilation.  In other words, this
    function orchestrates end-to-end compilation for the inductor backend when
    you use :func:`torch.compile`.

    NB: This function TAKES OWNERSHIP of the input ``model_`` and can potentially
    mutate it!  Make a copy if you need to preserve the original GraphModule.
    """
    ...

def graph_returns_tuple(gm: GraphModule) -> bool:
    """True if a FX graph returns a tuple"""
    ...

def make_graph_return_tuple(gm: GraphModule, inputs: Sequence[InputType], compile_gm: Callable[..., Any]) -> Callable[..., Any]:
    """
    Mutate gm so it returns a tuple.  This is only needed for graphs
    not created by torchdynamo that return non-tuples.
    """
    ...

def handle_dynamo_export_graph(gm: GraphModule, inputs: Sequence[InputType], compile_gm: Callable[..., Any]) -> Callable[..., Any]:
    """
    `torch._dynamo.export` embeds pytrees in the FX graph codegen object,
    convert that to a normal FX graph so inductor can compile it.
    """
    ...

