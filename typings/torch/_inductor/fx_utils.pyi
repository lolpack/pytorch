"""
This type stub file was generated by pyright.
"""

import torch
import torch.fx
from typing import Any, Callable, Optional

def matches_module_function_pattern(pattern: tuple[type[torch.nn.modules.Module], Callable[..., Any]], node: torch.fx.node.Node, modules: dict[str, torch.nn.modules.Module]) -> bool:
    ...

class FakeTensorUpdater:
    """
    The main idea here is that it's difficult to maintain accurate fake
    tensors (our primary form of metadata) for each node in our graph as we
    transform it.

    The most reliable way to obtain this information is by rerunning
    faketensor propagation. However, in general, faketensor propagation is
    fairly expensive. So, instead we'd like to only rerun faketensor
    propagation on nodes that have changed.

    In order to detect which nodes have changed, we first hash its node,
    target, and argument lists (which are immutable in FX).

    Then, whenever we call incremental_update, we check which FX nodes have a
    new hash, and recompute the faketensor metadata for that node. Then, we
    continue to recursively compute the faketensors for all users until the
    fake tensors stop changing.
    """
    def __init__(self, graph: torch.fx.Graph) -> None:
        ...
    
    def hash_node(self, node: torch.fx.Node): # -> tuple[Node, Target, int, int]:
        ...
    
    def incremental_update(self): # -> None:
        """Update FakeTensors on self.graph. We will try to do the minimum amount of work."""
        ...
    


def get_storage(t: torch.Tensor) -> int:
    ...

def get_node_storage(node: torch.fx.Node) -> Optional[int]:
    ...

def get_fake(x): # -> Node | Any:
    ...

def get_fake_args_kwargs(x: torch.fx.Node) -> tuple[bool, tuple[Any], dict[str, Any]]:
    """
    First value returns a boolean if any of the input nodes don't have a faketensor.
    """
    ...

def is_node_realized(node: torch.fx.Node) -> bool:
    """Returns true if a node is always realized when lowered to inductor IR.

    NOTE: This may return some false negatives. e.g. it doesn't
    handle buffers realized heuristically during lowering, or
    buffers realized indirectly through view ops.
    """
    ...

def count_flops_fx(node: torch.fx.Node) -> Optional[int]:
    ...

def countable_fx(node: torch.fx.Node) -> bool:
    """
    Whether or not we can count the flops of an FX node.
    """
    ...

