"""
This type stub file was generated by pyright.
"""

import contextlib
import dataclasses
import functools
import torch
from collections.abc import Iterator
from typing import Any, Callable, IO, Optional
from torch import fx as fx
from torch.fx.graph_module import GraphModule
from torch.fx.passes.shape_prop import TensorMetadata
from torch.types import FileLike
from . import ir
from .scheduler import BaseSchedulerNode

log = ...
ir_pre_fusion_log = ...
ir_post_fusion_log = ...
SchedulerNodeList = list[Any]
BufMeta = ...
GRAPHVIZ_COMMAND_SCALABLE = ...
@functools.cache
def has_dot() -> bool:
    ...

def draw_buffers(nodes: list[BaseSchedulerNode], print_graph: bool = ..., fname: Optional[str] = ...) -> None:
    """
    Draw a graph in fname.svg.
    """
    ...

def create_fx_from_snodes(snodes: list[BaseSchedulerNode]) -> fx.Graph:
    """
    Creates a FX Graph from a list of SchedulerNode objects.
    """
    ...

def update_orig_fx_node_name_to_buf_name(nodes: Optional[SchedulerNodeList], node_name_to_buf_name: dict[str, str], parent_buf_name: Optional[str] = ..., n_origins: int = ...) -> None:
    ...

def get_node_name_to_buf_meta(node_name_to_buf_name: dict[str, str]) -> dict[str, BufMeta]:
    ...

def annotate_orig_fx_with_snodes(gm: torch.fx.GraphModule, snodes: SchedulerNodeList) -> None:
    """
    Creates a FX Graph from a list of SchedulerNode objects.
    """
    ...

@contextlib.contextmanager
def enable_aot_logging() -> Iterator[None]:
    ...

_inductor_post_to_pre_grad_nodes: dict[str, Any] = ...
_pre_grad_graph_id: Optional[int] = ...
class DebugContext:
    _counter = ...
    _inductor_triton_kernel_to_post_grad_node_info: dict[str, list[str]] = ...
    @staticmethod
    def create_debug_dir(folder_name: str) -> Optional[str]:
        ...
    
    def __init__(self) -> None:
        ...
    
    def copy(self, new_path: str) -> None:
        ...
    
    def fopen(self, filename: str, write_mode: str = ..., *args: Any, **kwargs: Any) -> IO[Any]:
        ...
    
    @contextlib.contextmanager
    def fopen_context(self, filename: str, write_mode: str = ..., *args: Any, **kwargs: Any) -> Iterator[IO[Any]]:
        ...
    
    def filename(self, suffix: str) -> str:
        ...
    
    def upload_tar(self) -> None:
        ...
    
    def __enter__(self) -> None:
        ...
    
    def __exit__(self, exc_type: Optional[type[BaseException]], exc_val: Optional[BaseException], exc_tb: Optional[Any]) -> None:
        ...
    
    def __getattr__(self, name: str) -> Optional[Callable[..., None]]:
        ...
    


class DebugFormatter:
    def __init__(self, handler: DebugContext) -> None:
        ...
    
    def fx_graph(self, gm: torch.fx.GraphModule, inputs: list[torch.Tensor]) -> None:
        ...
    
    def fx_graph_transformed(self, gm: torch.fx.GraphModule, inputs: list[torch.Tensor]) -> None:
        ...
    
    def ir_pre_fusion(self, nodes: SchedulerNodeList) -> None:
        ...
    
    def ir_post_fusion(self, nodes: SchedulerNodeList) -> None:
        ...
    
    def graph_diagram(self, nodes: SchedulerNodeList) -> None:
        ...
    
    def draw_orig_fx_graph(self, gm: torch.fx.GraphModule, nodes: SchedulerNodeList) -> None:
        ...
    
    def output_code(self, filename: str, extension: str = ...) -> None:
        ...
    
    def log_inductor_triton_kernel_to_post_grad_node_info(self, filename: str = ...) -> tuple[dict[str, list[str]], dict[str, Any]]:
        ...
    
    def log_autotuning_results(self, name: str, input_nodes: list[ir.IRNode], timings: dict[ChoiceCaller, float], elapse: float, precompile_elapse: float, prescreening_elapse: Optional[float]) -> None:
        ...
    


def log_ir_pre_fusion(nodes: SchedulerNodeList) -> None:
    ...

def log_ir_post_fusion(nodes: SchedulerNodeList) -> None:
    ...

@dataclasses.dataclass
class TensorMetadataHolder:
    tensor_metadata: TensorMetadata
    device: torch.device
    ...


save_args_cnt = ...
def create_node_mapping(pre_grad_graph_id: int, post_to_pre_grad_nodes_json: dict[str, Any], triton_kernel_to_post_grad_json: dict[str, Any]) -> dict[str, dict[str, Any]]:
    """Create bidirectional mappings between:

    - pre_grad graph nodes and post_grad graph code nodes, and vice versa
    - triton kernel name and post_grad graph code nodes, and vice versa
    """
    ...

def save_args_for_compile_fx_inner(*args: Any, **kwargs: Any) -> None:
    """
    This function is used to save arguments for a compile_fx_inner function call
    to the file system.  Later on one can replay the compile_fx_inner call
    with the saved arguments using load_args_and_run_compile_fx_inner.
    """
    ...

def load_args_and_run_compile_fx_inner(path: str) -> Any:
    ...

def aot_inductor_minifier_wrapper(func: Callable[..., str], exported_program: torch.export.ExportedProgram, *, inductor_configs: dict[str, Any], package_path: Optional[FileLike] = ...) -> str:
    ...

