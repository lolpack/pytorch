"""
This type stub file was generated by pyright.
"""

import dataclasses
from functools import partial
from threading import Lock
from typing import Any, Callable, TYPE_CHECKING
from collections.abc import Generator
from triton import Config as TritonConfig

if TYPE_CHECKING:
    ...
@dataclasses.dataclass
class BaseConfig:
    """
    Base Gemm configuration used for most backends (CPU, CUDA)
    """
    block_m: int
    block_n: int
    block_k: int
    num_stages: int
    num_warps: int
    ...


@dataclasses.dataclass
class GemmConfig(BaseConfig):
    """
    Gemm configuration used for most backends (CPU, CUDA)
    """
    group_m: int = ...


ConvConfig = BaseConfig
@dataclasses.dataclass
class FlexConfig:
    """
    Base Config class for flex attention
    - FlexAttn forward, backward and flex decode will use this

    NOTE:
    For flex_attn bwd block_m and block_n are reused for block_m1, block_m2, block_n1, block_n2

    """
    block_m: int
    block_n: int
    num_stages: int
    num_warps: int
    ...


@dataclasses.dataclass
class FlexDecodeConfig:
    """
    Config class for flex decoding
    """
    block_n: int
    num_stages: int
    num_warps: int
    ...


@dataclasses.dataclass
class ROCmGemmConfig(GemmConfig):
    """
    ROCm subclass for GEMMs, with AMD backend specific tuneable kernargs
    """
    matrix_instr_nonkdim: int = ...
    waves_per_eu: int = ...
    kpack: int = ...


@dataclasses.dataclass
class ROCmConvConfig(ConvConfig):
    """
    ROCm subclass for Conv, with AMD backend specific tuneable kernargs
    """
    matrix_instr_nonkdim: int = ...
    waves_per_eu: int = ...
    kpack: int = ...


@dataclasses.dataclass
class ROCmFlexConfig(FlexConfig):
    """
    ROCm subclass for FlexAttn, with AMD backend specific tuneable kernargs
    """
    matrix_instr_nonkdim: int = ...
    waves_per_eu: int = ...
    kpack: int = ...


@dataclasses.dataclass
class ROCmFlexDecodeConfig(FlexDecodeConfig):
    """
    ROCm subclass for FlexDecode, with AMD backend specific tuneable kernargs
    """
    matrix_instr_nonkdim: int = ...
    waves_per_eu: int = ...
    kpack: int = ...


class BaseHeuristicSingleton(type):
    """
    Thread-safe implementation of single to be used in the config heuristic subclasses
    to ensure heavy __init__ calls are not repeatedly run
    """
    _instances: dict[type[Any], Any] = ...
    _lock: Lock = ...
    def __call__(cls: BaseHeuristicSingleton, *args: Any, **kwargs: Any) -> BaseConfigHeuristic:
        ...
    


class BaseConfigHeuristic(metaclass=BaseHeuristicSingleton):
    """
    Base class for mm_configs, device specific triton kernels config inherit from here
    """
    def __init__(self) -> None:
        ...
    
    def preprocess_mm_configs(self, m: int, n: int, k: int, configs: list[BaseConfig], has_int8_tensor: bool = ..., scale: int = ..., exclude: Callable[[int, int, int], bool] = ..., dtype_size: int = ...) -> Generator[TritonConfig, None, None]:
        ...
    
    def triton_config(self, num_stages: int, num_warps: int, **kwargs: Any) -> TritonConfig:
        ...
    
    def get_mm_configs(self) -> partial[Generator[TritonConfig, None, None]]:
        ...
    
    def get_exhaustive_mm_configs(self) -> partial[Generator[TritonConfig, None, None]]:
        ...
    
    def get_extra_mm_configs(self) -> partial[Generator[TritonConfig, None, None]]:
        ...
    
    def get_int8_mm_configs(self) -> partial[Generator[TritonConfig, None, None]]:
        ...
    
    def get_mixed_mm_configs(self) -> partial[Generator[TritonConfig, None, None]]:
        ...
    
    def get_persistent_mm_configs(self) -> partial[Generator[TritonConfig, None, None]]:
        ...
    
    def get_scaled_mm_configs(self) -> partial[Generator[TritonConfig, None, None]]:
        ...
    
    def get_scaled_persistent_mm_configs(self) -> partial[Generator[TritonConfig, None, None]]:
        ...
    
    def get_mm_plus_mm_configs(self) -> partial[Generator[TritonConfig, None, None]]:
        ...
    
    def get_conv_configs(self) -> partial[Generator[TritonConfig, None, None]]:
        ...
    
    def get_flex_attn_fwd_configs(self, head_dim: int, dtype: Any) -> list[FlexConfig]:
        ...
    
    def get_flex_attn_bwd_configs(self, head_dim: int, dtype: Any) -> list[FlexConfig]:
        ...
    
    def get_flex_decode_configs(self, head_dim: int, dtype: Any) -> list[FlexDecodeConfig]:
        ...
    


class CPUConfigHeuristic(BaseConfigHeuristic):
    ...


class CUDAConfigHeuristic(BaseConfigHeuristic):
    """
    Child class for CUDA device specific gemm/flex attention/conv/ configs.
    """
    def __init__(self) -> None:
        ...
    
    def get_flex_attn_fwd_configs(self, head_dim: int, dtype: Any) -> list[FlexConfig]:
        ...
    
    def get_flex_attn_bwd_configs(self, head_dim: int, dtype: Any) -> list[FlexConfig]:
        ...
    
    def get_flex_decode_configs(self, head_dim: int, dtype: Any) -> list[FlexDecodeConfig]:
        ...
    


class ROCmConfigHeuristic(BaseConfigHeuristic):
    """
    Child class for ROCm specific gemm/flex attention/conv/ configs.
    """
    def __init__(self) -> None:
        ...
    
    def get_extra_mm_configs(self) -> partial[Generator[TritonConfig, None, None]]:
        ...
    
    def get_int8_mm_configs(self) -> partial[Generator[TritonConfig, None, None]]:
        ...
    
    def get_mixed_mm_configs(self) -> partial[Generator[TritonConfig, None, None]]:
        ...
    
    def get_persistent_mm_configs(self) -> partial[Generator[TritonConfig, None, None]]:
        ...
    
    def get_scaled_mm_configs(self) -> partial[Generator[TritonConfig, None, None]]:
        ...
    
    def get_scaled_persistent_mm_configs(self) -> partial[Generator[TritonConfig, None, None]]:
        ...
    
    def get_mm_plus_mm_configs(self) -> partial[Generator[TritonConfig, None, None]]:
        ...
    
    def get_conv_configs(self) -> partial[Generator[TritonConfig, None, None]]:
        ...
    
    def get_flex_attn_fwd_configs(self, head_dim: int, dtype: Any) -> list[FlexConfig]:
        ...
    
    def get_flex_attn_bwd_configs(self, head_dim: int, dtype: Any) -> list[FlexConfig]:
        ...
    
    def get_flex_decode_configs(self, head_dim: int, dtype: Any) -> list[FlexDecodeConfig]:
        ...
    


class XPUConfigHeuristic(BaseConfigHeuristic):
    """
    Placeholder child class for XPU specific overrides.
    """
    ...


