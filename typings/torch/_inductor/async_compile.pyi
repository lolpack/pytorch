"""
This type stub file was generated by pyright.
"""

import functools
import os
from concurrent.futures import ThreadPoolExecutor
from typing import Any, Callable, Optional, TYPE_CHECKING
from torch._inductor import config
from torch._inductor.codecache import CodeCacheFuture
from torch._inductor.compile_worker.subproc_pool import AnyPool
from torch._inductor.utils import clear_on_fresh_cache
from torch.utils._triton import has_triton_package
from torch._inductor.runtime.hints import HalideMeta

if TYPE_CHECKING:
    ...
_cumulative_compile_time = ...
_t0: Optional[float] = ...
kernel_code_log = ...
log = ...
_triton_kernel_metrics: Optional[dict[str, dict[str, Any]]] = ...
def pre_fork_setup(): # -> None:
    """
    Setup that must be done prior to forking with a process pool.
    """
    ...

def caching_device_properties(): # -> None:
    ...

_IS_WINDOWS = ...
log = ...
_pool_set = ...
def shutdown_compile_workers() -> None:
    """Shut down all outstanding compile-worker pools."""
    ...

def after_fork(): # -> None:
    """Reset pools to initial state without shutting them down"""
    ...

def get_compile_threads() -> int:
    """
    Temporary for internal rollout. Assign config.compile_threads lazily and return it.
    TODO: remove after rollout.
    """
    ...

@clear_on_fresh_cache
class CompiledTritonKernels:
    """
    In memory cache for storing compiled triton kernels.

    Each triton kernel is keyed by the hash of its source code. Each value stored
    in the cache is a return value of AsyncCompile.triton().

    Currently, the cache stores Future objects, but it should be generalizable for any kernels.
    """
    _cache: dict[str, CodeCacheFuture] = ...
    @staticmethod
    def key(kernel_src: str): # -> str:
        """
        Generates a cache key given a triton kernel's full source code.
        This source includes the inductor meta, compilation metadata, the kernel itself, etc.
        `kernel_src` should be the exact string passed to async_compile.triton()'s first argument.
        """
        ...
    
    @staticmethod
    def save(kernel_src: str, future: CodeCacheFuture): # -> None:
        """
        Saves a compiled triton kernel to the cache.
        TODO: We store a LambdaFuture as that's the callable returned by async_compile.triton,
        but the real type we want to return here is actually an abstract triton kernel.

        TODO: Source code here is not just the kernel's source code, but also includes the inductor preamble, etc.
        so it could be less strict.
        """
        ...
    
    @staticmethod
    def get(kernel_src: str) -> Optional[CodeCacheFuture]:
        ...
    
    @staticmethod
    def cache_clear(): # -> None:
        ...
    
    @staticmethod
    def remove_future(kernel_src: str) -> None:
        ...
    


class AsyncCompile:
    def __init__(self) -> None:
        ...
    
    @staticmethod
    @functools.lru_cache(1)
    def pool() -> ThreadPoolExecutor:
        ...
    
    @staticmethod
    @functools.lru_cache(1)
    def process_pool() -> AnyPool:
        ...
    
    @classmethod
    def warm_pool(cls) -> None:
        ...
    
    @classmethod
    def submit(cls, task: Callable[..., Any]) -> Any:
        ...
    
    def use_process_pool(self): # -> Literal[False]:
        ...
    
    def triton(self, kernel_name: str, source_code: str, device_str: str = ...): # -> Any | StaticAutotunerFuture | CachingAutotuner | LambdaFuture:
        """
        Async_compile.triton is more complicated than the other backends because
        we're trying to optimize compile time as much as possible for this hot callsite.

        First of all, the function is cached by CompiledTritonKernels; if there's a kernel
        already compiled, we grab it directly from the cache and return.

        Otherwise, if we have multiple compile threads, we kick off triton compilations on each
        worker process by giving it a kernel and source code to compile. The worker initializes
        a CachingAutotuner, runs triton compilation, and pickles the kernel back to us.
        We use TritonCompileResult to represent the objects being pickled back to us by each
        worker.

        Some maybe not obvious things that are pickled back to us:
        - Most of the time, we can avoid sending back CachingAutotuner.fn and other metadata
          and do not have to pay the cost of loading the triton kernel on the parent. But certain
          cases, like coordesc tuning and dynamic_scale_rblock, require us to reload the function
          in the parent lazily when we require it.
        - The AutotuneCache, if enabled, is constructed on each worker per triton config
          and pickled by to us via `CachingAutotuner.save_cache_hook`.
        """
        ...
    
    def multi_kernel(self, *args, **kwargs) -> Any:
        ...
    
    def cpp(self, source_code: str): # -> Any | LambdaFuture:
        ...
    
    def cpp_pybinding(self, argtypes: list[str], source_code: str): # -> Any | LambdaFuture:
        ...
    
    def cuda(self, source_code, dst_file_ext, aot_compile=...): # -> Any:
        ...
    
    def rocm(self, source_code, dst_file_ext, aot_compile=...): # -> Any:
        ...
    
    def halide(self, meta: HalideMeta, source_code: str): # -> Any | LambdaFuture:
        ...
    
    def wait(self, scope: dict[str, Any]) -> None:
        ...
    


if (os.environ.get("TORCH_TNT_IN_USE", "0") == "1" or os.environ.get("TORCH_WARM_POOL", "1") != "1" or not has_triton_package() or config.is_fbcode()):
    ...
else:
    ...
