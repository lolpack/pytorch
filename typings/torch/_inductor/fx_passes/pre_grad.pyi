"""
This type stub file was generated by pyright.
"""

import torch
from collections.abc import Sequence
from typing import Optional
from ..pattern_matcher import init_once_fakemode

log = ...
efficient_conv_bn_eval_pass = ...
fuse_split_linear_add_pass = ...
fuse_chunk_squeeze_cat_pass = ...
remove_reshape_pass = ...
normalization_pass_aten = ...
merge_splits_pass_aten = ...
split_cat_pass_aten = ...
unbind_stack_pass_aten = ...
merge_getitem_cat_pass_aten = ...
merge_stack_tahn_unbind_pass_aten = ...
mutate_cat_pass_aten = ...
remove_split_with_size_one_pass_aten = ...
def save_inductor_dict(pass_to_compare=...): # -> dict[str | Any, int]:
    ...

def is_same_dict(inductor_dict, optimus_dict): # -> bool:
    ...

def shape_prop(mod) -> None:
    ...

def normalize_node_kwargs_pass(graph): # -> None:
    ...

def fuse_parallel_linear_pass(graph): # -> None:
    ...

def remove_split_ops(graph, shape_prop): # -> None:
    ...

def remove_split_ops_pass(graph): # -> None:
    ...

def fuse_chunk_reshape_unsqueeze_concat_pass(graph): # -> None:
    ...

def fuse_chunk_reshape_concat_pass(graph): # -> None:
    ...

def remove_noop_pass(graph): # -> None:
    ...

def stack_to_unsqueeze_pass(graph): # -> None:
    ...

def merge_concats_pass(graph): # -> None:
    ...

def relu_nan_to_num(graph): # -> None:
    ...

def fuse_split_getitem_squeeze_cat(graph): # -> None:
    ...

def use_triton_dot_compress(graph): # -> None:
    ...

def use_triton_lce_replace_simple_LCE_helper(gm, shape_prop): # -> None:
    ...

def use_triton_lce_replace_simple_LCE(graph): # -> None:
    ...

def use_triton_lce_replace_normal_LCE_helper(gm, shape_prop): # -> None:
    ...

def use_triton_lce_replace_normal_LCE(graph): # -> None:
    ...

def use_matmul_lce_replace_normal_LCE(graph): # -> None:
    ...

def use_matmul_fuse_lce_replace_first_LCE(graph): # -> None:
    ...

@init_once_fakemode
def lazy_init(): # -> None:
    ...

def pre_grad_passes(gm: torch.fx.GraphModule, example_inputs: Sequence[object] = ..., add_passes: Optional[str] = ..., remove_passes: Optional[str] = ...) -> torch.fx.GraphModule:
    """
    Apply passes on the input FX graph using Torch IR.

    WARNING:
    The IR before grad is not functional or normalized, so it is harder
    to write passes on this IR.  Passes must be safe with respect to
    aliasing and mutation and need to handle all possible arg schemas.

    Consider adding a new pass to post_grad.py or joint_graph.py which
    are after functionalization and normalization.
    """
    ...

def fuse_fx(gm: torch.fx.GraphModule, example_inputs) -> torch.fx.GraphModule:
    ...

def fetch_attr(target: str, mod): # -> Any:
    ...

def remove_identity(gm: torch.fx.GraphModule) -> torch.fx.GraphModule:
    """
    Removes all identity layers from the module.
    """
    class IdentityRemover(torch.fx.Transformer):
        ...
    
    

def fuse_conv_bn(gm: torch.fx.GraphModule, inplace=...) -> torch.fx.GraphModule:
    """
    Fuses Convolution/BN layers for inference purposes.
    """
    class ConvBNFusion:
        ...
    
    

class NormalizedLinearNode:
    def __init__(self, node: torch.fx.Node) -> None:
        ...
    
    def get_input(self) -> torch.fx.Node:
        ...
    
    def get_weight(self) -> torch.fx.Node:
        ...
    
    def get_bias(self) -> torch.fx.Node:
        ...
    


class NormalizedMatmulNode:
    def __init__(self, node: torch.fx.Node) -> None:
        ...
    
    def get_input(self) -> torch.fx.Node:
        ...
    
    def get_other(self) -> torch.fx.Node:
        ...
    


def check_permute(node: torch.fx.Node) -> bool:
    ...

def sink_cat_after_pointwise(module: torch.fx.GraphModule) -> torch.fx.GraphModule:
    ...

def linear_permute_fusion(module: torch.fx.GraphModule) -> torch.fx.GraphModule:
    ...

def linear_transpose(input: torch.Tensor, weight: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor:
    ...

def permute_linear_fusion(module: torch.fx.GraphModule) -> torch.fx.GraphModule:
    ...

def permute_matmul_fusion(module: torch.fx.GraphModule) -> torch.fx.GraphModule:
    ...

def transpose_linear(input: torch.Tensor, weight: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor:
    ...

def transpose_matmul(A: torch.Tensor, B: torch.Tensor, Atrans: bool, Btrans: bool) -> torch.Tensor:
    ...

