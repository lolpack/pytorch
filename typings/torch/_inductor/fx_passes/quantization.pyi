"""
This type stub file was generated by pyright.
"""

import torch

aten = ...
prims = ...
quantized_decomposed = ...
quantized = ...
_PER_TENSOR_QUANTIZE_OPS = ...
_VIEW_OPS = ...
def get_dequantize_per_tensor_activation_pattern(is_tensor_overload=...): # -> CallFunction:
    ...

dequantize_per_channel_weight_pattern = ...
dequantize_per_channel_to_bf16_weight_pattern = ...
dequantize_per_channel_clone_weight_pattern = ...
dequantize_per_channel_to_bf16_clone_weight_pattern = ...
def get_qconv_pt2e_pattern(users=...): # -> CallFunction:
    ...

def get_qconv2d_binary_pt2e_pattern(users=...): # -> CallFunction:
    ...

def get_qlinear_pt2e_pattern(x_scale_zp_are_tensors, users=...): # -> CallFunction:
    ...

def get_qlinear_binary_pt2e_pattern(x_scale_zp_are_tensors, users=...): # -> CallFunction:
    ...

dequantize_accum_pattern = ...
def generate_pattern_with_binary(binary_post_op, computation_call, extra_input_pattern, dtype_convert=..., swap_inputs=...): # -> CallFunction:
    ...

def generate_pattern_with_unary(computation_call, unary_post_op): # -> CallFunction:
    ...

def generate_pattern_with_output_quant(computation_call, with_dtype_convert=...): # -> CallFunction:
    ...

_raw_dequantize_per_tensor_activation_pattern = ...
class PostOpAttr:
    def __init__(self, binary_op_name: str = ..., alpha=..., unary_op_name: str = ..., scalars_attr=..., algorithm_attr=...) -> None:
        ...
    


def concat_linear_woq_int4(gm: torch.fx.GraphModule): # -> None:
    """
    Concat Linear optimization pass for WOQ int4
    This pass fuses the original pattern:
    def ...
        return (woq_int4(x, w1, group_size, scale_zp1), woq_int4(x, w2, group_size, scale_zp1) ...)
    into a single operation:
    def ...
        concat_res = woq_int4(x, concat_w, group_size, concat_scale_zp)
        return split(concat_res, split_size_list)
    """
    ...

def quant_lift_up(graph_module: torch.fx.GraphModule): # -> None:
    """
    Lift up the quant node before view like nodes. It can benefit performance
    of Attention like block. For example, we have the pattern as:

             DQ
    DQ       LINEAR
    LINEAR   VIEW
    VIEW     PERMUTE
    PERMUTE  TRANSPOSE
    Q        Q
    DQ       DQ
       Matmul
        DIV
        ADD
      SOFTMAX

    We want to lift up the the quant nodes from matmul before view like nodes
    as the output of Linear node.

             DQ
    DQ       LINEAR
    LINEAR   Q
    Q        VIEW
    VIEW     PERMUTE
    PERMUTE  TRANSPOSE
    DQ       DQ
       Matmul
        DIV
        ADD
      SOFTMAX

    It produces a DQ->LINEAR->Q pattern which can be fused by backend.
    """
    ...

