"""
This type stub file was generated by pyright.
"""

import collections
import torch
from collections.abc import Iterable, Iterator
from typing import Any, Optional
from torch.utils._ordered_set import OrderedSet

has_fbgemm = ...
aten = ...
log = ...
DEFAULT_BETA = ...
DEFAULT_ALPHA = ...
MIN_FUSE_SET_SIZE = ...
MAX_FUSE_SET_SIZE = ...
MAX_FUSE_SEARCH_DEPTH = ...
MAX_FUSE_TENSOR_SIZE_GROUP_LINEAR = ...
FUSE_NODES_WITH_SAME_PARENT = ...
SHAPE_BROADCAST_BATCH_LINEAR = ...
Fuse_NODES_WITH_SAME_USERS = ...
SEARCH_EXCLUSIONS = ...
default_graph_search_options = ...
graph_search_options = ...
def update_stack_example_value(node, metadata, dim=..., op=...): # -> None:
    """
    Update the example value of the node in the graph to enable followup split cat opt.
    """
    ...

def update_pointwise_example_value(pointwise_node, input, other, op): # -> None:
    """
    Update the example value of the add node in the graph to enable followup split cat opt.
    """
    ...

class GroupBatchFusionBase:
    def __init__(self, **kwargs) -> None:
        ...
    
    def match(self, node):
        ...
    
    def fuse(self, graph, subset):
        ...
    


PRE_GRAD_FUSIONS: dict[str, GroupBatchFusionBase] = ...
POST_GRAD_FUSIONS: dict[str, GroupBatchFusionBase] = ...
def register_fusion(name: str, pre_grad=...): # -> Callable[..., GroupBatchFusionBase]:
    ...

def list_group_batch_fusions(pre_grad=...) -> list[str]:
    ...

def decompose_stack(graph: torch.fx.GraphModule, input_tensors: list[Any]) -> Any:
    ...

class GroupFusion(GroupBatchFusionBase):
    """
    Fuse ops in a group way, e.g, fuse mm/addmm of arbitrary input shapes with fbgemm.gmm.
    """
    ...


class BatchFusion(GroupBatchFusionBase):
    """
    Fuse ops in a batch way, e.g, fuse mm/addmm of same input shapes with bmm.
    """
    ...


class BatchPointwiseOpsFusionFactory(BatchFusion):
    def __init__(self, op, **kwargs) -> None:
        ...
    


@register_fusion("batch_linear_post_grad", pre_grad=False)
class PostGradBatchLinearFusion(BatchFusion):
    """
    Fuse ops in a batch way in post grad (aten level).
    """
    def match(self, node: torch.fx.Node) -> Optional[tuple[str, int, int, int, bool, str]]:
        ...
    
    def fuse(self, graph: torch.fx.GraphModule, subset: list[torch.fx.Node]): # -> None:
        ...
    


@register_fusion("group_linear", pre_grad=False)
class GroupLinearFusion(GroupFusion):
    def match(self, node: torch.fx.Node) -> Optional[tuple[str, bool]]:
        ...
    
    def fuse(self, graph: torch.fx.GraphModule, subset: list[torch.fx.Node]): # -> None:
        ...
    


class BatchPointwiseMathOpsPostGradFusion(BatchPointwiseOpsFusionFactory):
    """
    Batch pointwise math operator (e.g., add, mul) in post grad pass.
    """
    def __init__(self, op, **kwargs) -> None:
        ...
    
    def match(self, node: torch.fx.Node): # -> tuple[Any, str, str, str, str, str, str] | None:
        ...
    
    def fuse(self, graph: torch.fx.GraphModule, subset: list[torch.fx.Node]): # -> None:
        ...
    


@register_fusion("batch_linear_lhs")
class BatchLinearLHSFusion(BatchFusion):
    """
    Batch linear left-hand side fusion. This pass tries to fuse the following patterns:

        torch.nn.functional.linear(x, w1), linear(x, w2),... * linear(x, wn)
        -> torch.mm(x, torch.cat([w1, w2,... * wn]).transpose(0, 1))

    We have a separate pass to eliminate contiguous transpose in a generic way.
    """
    def match(self, node: torch.fx.Node) -> Optional[tuple[str, bool, Any]]:
        ...
    
    def fuse(self, graph: torch.fx.GraphModule, subset: list[torch.fx.Node]): # -> None:
        ...
    


def is_linear_node_can_be_fused(node: torch.fx.Node): # -> bool:
    ...

@register_fusion("batch_linear")
class PreGradBatchLinearFusion(BatchFusion):
    """
    Batch linear fusion in pre grad pass.
    Fuse linear with same size with torch.baddmm
    """
    def match(self, node: torch.fx.Node): # -> tuple[Literal['batch_linear'], Argument, str, str, bool, str] | None:
        ...
    
    def fuse(self, graph: torch.fx.GraphModule, subset: list[torch.fx.Node]): # -> None:
        ...
    


@register_fusion("batch_layernorm")
class BatchLayernormFusion(BatchFusion):
    """
    Batch layer norm fusion in pre grad pass
    """
    def match(self, node: torch.fx.Node): # -> tuple[Literal['batch_layernorm'], str, str, str, str, str, str] | None:
        ...
    
    def fuse(self, graph: torch.fx.GraphModule, subset: list[torch.fx.Node]): # -> None:
        ...
    


class BatchPointwiseOpsPreGradFusion(BatchPointwiseOpsFusionFactory):
    """
    Batch pointwise ops (e.g., sigmoid, relu, tanh) fusion in pre grad pass.
    We fuse it in random place, and the introduced stack node may be merged in split cat.
    """
    def __init__(self, op, **kwargs) -> None:
        ...
    
    def match(self, node: torch.fx.Node): # -> tuple[Any, str, str, str] | None:
        ...
    
    def fuse(self, graph: torch.fx.GraphModule, subset: list[torch.fx.Node]): # -> None:
        ...
    


class BatchPointwiseOpsPostGradFusion(BatchPointwiseOpsFusionFactory):
    """
    Batch pointwise ops (e.g., sigmoid, relu, tanh) fusion in post grad pass.
    The introduced stack node may be merged in split cat.
    """
    def __init__(self, op, **kwargs) -> None:
        ...
    
    def match(self, node: torch.fx.Node): # -> tuple[Any, str, str, str] | None:
        ...
    
    def fuse(self, graph: torch.fx.GraphModule, subset: list[torch.fx.Node]): # -> None:
        ...
    


class BatchMathOpsPreGradFusion(BatchPointwiseOpsFusionFactory):
    """
    Batch simple match related ops such as nan_to_num in pre grad pass.
    """
    def __init__(self, op, **kwargs) -> None:
        ...
    
    def match(self, node: torch.fx.Node): # -> str | None:
        ...
    
    def fuse(self, graph: torch.fx.GraphModule, subset: list[torch.fx.Node]): # -> None:
        ...
    


@register_fusion("batch_tanh")
class BatchTanhPreGradFusion(BatchPointwiseOpsPreGradFusion):
    def __init__(self, **kwargs) -> None:
        ...
    


@register_fusion("batch_sigmoid")
class BatchSigmoidPreGradFusion(BatchPointwiseOpsPreGradFusion):
    def __init__(self, **kwargs) -> None:
        ...
    


@register_fusion("batch_relu")
class BatchReLuPreGradFusion(BatchPointwiseOpsPreGradFusion):
    def __init__(self, **kwargs) -> None:
        ...
    


@register_fusion("batch_detach")
class BatchDetachPreGradFusion(BatchMathOpsPreGradFusion):
    def __init__(self, **kwargs) -> None:
        ...
    


@register_fusion("batch_nan_to_num")
class BatchNanToNumPreGradFusion(BatchMathOpsPreGradFusion):
    def __init__(self, **kwargs) -> None:
        ...
    


@register_fusion("batch_clamp")
class BatchClampPreGradFusion(BatchMathOpsPreGradFusion):
    def __init__(self, **kwargs) -> None:
        ...
    


@register_fusion("batch_aten_tanh", pre_grad=False)
class BatchTanhPostGradFusion(BatchPointwiseOpsPostGradFusion):
    def __init__(self, **kwargs) -> None:
        ...
    


@register_fusion("batch_aten_sigmoid", pre_grad=False)
class BatchSigmoidPostGradFusion(BatchPointwiseOpsPostGradFusion):
    def __init__(self, **kwargs) -> None:
        ...
    


@register_fusion("batch_aten_relu", pre_grad=False)
class BatchReLuPostGradFusion(BatchPointwiseOpsPostGradFusion):
    def __init__(self, **kwargs) -> None:
        ...
    


@register_fusion("batch_aten_add", pre_grad=False)
class BatchAddPostGradFusion(BatchPointwiseMathOpsPostGradFusion):
    def __init__(self, **kwargs) -> None:
        ...
    


@register_fusion("batch_aten_sub", pre_grad=False)
class BatchSubPostGradFusion(BatchPointwiseMathOpsPostGradFusion):
    def __init__(self, **kwargs) -> None:
        ...
    


@register_fusion("batch_aten_div", pre_grad=False)
class BatchDivPostGradFusion(BatchPointwiseMathOpsPostGradFusion):
    def __init__(self, **kwargs) -> None:
        ...
    


@register_fusion("batch_aten_mul", pre_grad=False)
class BatchMulPostGradFusion(BatchPointwiseMathOpsPostGradFusion):
    def __init__(self, **kwargs) -> None:
        ...
    


class _OrderedSet:
    def __init__(self, param=...) -> None:
        ...
    
    def __contains__(self, o) -> bool:
        ...
    
    def __len__(self) -> int:
        ...
    
    def append(self, o): # -> None:
        ...
    
    def __iter__(self): # -> Iterator[Any]:
        ...
    


def find_independent_subset_greedy(node_list: Iterable[torch.fx.Node], graph_search_options: dict[str, Any]) -> Iterator[Iterable[torch.fx.Node]]:
    """
    Yields a list of subsets of `node_list` where no element in the subset
    depends on any other element in the subset. This results in a set of
    independent nodes which can be fused together.

    The order of `node_list` is preserved within each subset so we can benefit
    from split-cat elimination in later passes.

    During iteration it is only safe to mutate the graph by changing the nodes
    that have been returned.

    graph_search_options:
      - min_fuse_set_size: Minimum size of the subset to consider. Subsets below
        this size will be ignored.
      - max_fuse_set_size: Maximum size of the subset to consider. Subsets will
        be broken to be at most this size.
    """
    ...

def get_fusion_candidates(rule: GroupBatchFusionBase, root_node: torch.fx.Node, fused_set: OrderedSet[torch.fx.Node]) -> collections.defaultdict[Any, list[torch.fx.Node]]:
    """
    Search fusion candidates for a specific rule using BFS starting from the root node.
    We only search the subgraph within graph_search_options["max_fuse_search_depth"].
    """
    ...

def apply_group_batch_fusion(graph: torch.fx.GraphModule, rule: GroupBatchFusionBase): # -> None:
    ...

def generate_fusion_from_config(config_options: dict[str, Any], pre_grad=...): # -> list[GroupBatchFusionBase]:
    ...

def group_batch_fusion_passes(graph: torch.fx.Graph, pre_grad=...): # -> None:
    ...

