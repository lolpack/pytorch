"""
This type stub file was generated by pyright.
"""

import torch
from ..ir import Subgraph, TensorBox
from ..pattern_matcher import Arg, CallFunction, Match, register_graph_pattern
from ..select_algorithm import SymbolicGridFn

B2B_GEMM_PASS = ...
@SymbolicGridFn
def b2b_gemm_grid(M, P, meta, *, cdiv): # -> tuple[Any, Literal[1], Literal[1]]:
    ...

b2b_gemm_left_template = ...
b2b_gemm_right_template = ...
def load_ratio_left(M: int, N: int, O: int, P: int, m: int, n: int, o: int, p: int) -> float:
    """
    compute the ratio of estimated numbers of loads in baseline and b2bgemm
    M, N, O, P are matrix sizes
    m, n, o, p are block sizes
    |       | baseline (lower bound)        | b2bgemm
    | load  | M * N + N * O + M * O + O * P | M / m * P / p * O / o * (o * p + N / n * (m * n + n * o))
    | store | M * O + M * P                 | M * P
    b2bgemm is always better on stores, but for loads we need to find out beneficial cases using this function
    """
    ...

def load_ratio_right(M: int, N: int, O: int, P: int, m: int, n: int, o: int, p: int) -> float:
    """
    compute the ratio of estimated numbers of loads in baseline and b2bgemm
    M, N, O, P are matrix sizes
    m, n, o, p are block sizes
    |       | baseline (lower bound)        | b2bgemm
    | load  | N * O + O * P + M * N + N * P | M / m * P / p * N / n * (m * n + O / o * (n * o + o * p))
    | store | N * P + M * P                 | M * P
    b2bgemm is always better on stores, but for loads we need to find out beneficial cases using this function
    """
    ...

b2b_gemm_configs = ...
def is_b2b_gemm_good_on(is_left_assoc: bool, A_node: torch.fx.Node, B_node: torch.fx.Node, C_node: torch.fx.Node) -> bool:
    """
    checks whether the sizes are good for b2b_gemm
    """
    ...

def unoptimized_b2b_gemm(is_left_assoc: bool, subgraph: Subgraph, A: torch.Tensor, B: torch.Tensor, C: torch.Tensor, *, out: torch.Tensor) -> torch.Tensor:
    """
    The unoptimized version is used as a fallback when the b2b_gemm kernel is not beneficial.
    """
    ...

unoptimized_choice = ...
def build_subgraph_buffer(args: list[TensorBox], subgraph: Subgraph): # -> PyTree:
    """
    This function is adapted from ../kernel/flex_attention.py.
    The goal is to take in the required args and produce the subgraph buffer
    The subgraph buffer is a ComputedBuffer that will be inlined into the triton template

    Args:
        args: The args that are passed into the subgraph
        subgraph: The Subgraph ir for which to produce the output node
    """
    ...

def create_placeholder(name: str, dtype: torch.dtype, device: torch.device) -> TensorBox:
    """
    Creates a placeholder input buffers for producing subgraph_output
    """
    ...

def tuned_b2b_gemm(is_left_assoc: bool, subgraph: Subgraph, A: torch._inductor.ir.TensorBox, B: torch._inductor.ir.TensorBox, C: torch._inductor.ir.TensorBox, *, layout=...) -> torch._inductor.ir.TensorBox:
    ...

@register_graph_pattern(CallFunction(torch.ops.aten.mm, Arg(), Arg()), pass_dict=B2B_GEMM_PASS)
def b2b_gemm_handler(match: Match, mat1: torch.fx.Node, mat2: torch.fx.Node) -> None:
    ...

