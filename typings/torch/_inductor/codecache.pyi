"""
This type stub file was generated by pyright.
"""

import dataclasses
import functools
import hashlib
import pickle
import torch
from ctypes import CDLL, c_void_p
from functools import lru_cache
from pathlib import Path
from types import ModuleType
from typing import Any, Callable, Generic, Optional, TYPE_CHECKING, TypeVar, Union
from typing_extensions import Self, override
from torch import SymInt, Tensor
from torch._inductor import config
from torch._inductor.utils import clear_on_fresh_cache
from torch._subclasses.fake_tensor import TensorMetadata
from torch.compiler._cache import CacheArtifact, CacheArtifactFactory
from torch.export.pt2_archive._package_weights import Weights
from .output_code import CompiledFxGraph, CompiledFxGraphConstants
from .remote_cache import JsonDataTy, RemoteCache
from collections.abc import Generator, KeysView, Sequence
from concurrent.futures import Future
from .compile_fx import _CompileFxKwargs
from .graph import GraphLowering
from .ir import ChoiceCaller
from .runtime.hints import HalideMeta
from .runtime.triton_heuristics import CachingAutotuner
from .utils import InputType

if config.is_fbcode():
    ...
else:
    def log_global_cache_errors(*args: Any, **kwargs: Any) -> None:
        ...
    
    def log_global_cache_stats(*args: Any, **kwargs: Any) -> None:
        ...
    
    def log_global_cache_vals(*args: Any, **kwargs: Any) -> None:
        ...
    
    def use_global_cache() -> bool:
        ...
    
T = TypeVar("T")
if TYPE_CHECKING:
    ...
_IS_WINDOWS = ...
LOCK_TIMEOUT = ...
output_code_log = ...
log = ...
def use_re_build() -> bool:
    """
    Use for CUTLASS compilation only right now.
    """
    ...

def get_cpp_wrapper_cubin_path_name() -> str:
    ...

def get_kernel_bin_format(device: str) -> str:
    ...

@functools.cache
def get_global_cache_path_impl(global_cache_dir: str) -> Optional[Path]:
    ...

class CacheBase:
    @staticmethod
    @functools.cache
    def get_system() -> dict[str, Any]:
        ...
    
    @staticmethod
    @clear_on_fresh_cache
    @functools.cache
    def get_local_cache_path() -> Path:
        ...
    
    @staticmethod
    def get_global_cache_path() -> Optional[Path]:
        ...
    
    def __init__(self) -> None:
        ...
    
    def get_local_cache(self) -> dict[str, Any]:
        ...
    
    def update_local_cache(self, local_cache: dict[str, Any]) -> None:
        ...
    


class LocalCache(CacheBase):
    def lookup(self, *keys: str) -> Optional[dict[str, Any]]:
        ...
    
    def set_value(self, *keys: str, value: Any) -> None:
        ...
    


class PersistentCache(CacheBase):
    @functools.cache
    def get_global_cache(self) -> dict[str, Any]:
        ...
    
    def lookup(self, choices: list[ChoiceCaller], op: str, inputs: str, benchmark: Optional[Callable[[Any], dict[ChoiceCaller, float]]]) -> dict[ChoiceCaller, float]:
        """
        Check to see if we have benchmarked the given choice callers. For each
        choice caller:

            1. Check global_cache[op][inputs][choice][precision], return benchmark if cached.
            2. Check local_cache[op][inputs][choice][precision], return benchmark if cached.
            3. If benchmark is not None:
                a. `max_autotune_gemm=True`: benchmark the choice, update
                    local_cache[op][inputs][choice], and return the benchmark.
                b. `max_autotune_gemm=False`: don't benchmark the choice, return nothing.
        """
        ...
    


def get_lock_dir() -> str:
    ...

def sha256_hash(data: bytes) -> str:
    ...

def code_hash(code: Union[str, bytes], extra: Union[str, bytes] = ...) -> str:
    ...

def get_path(basename: str, extension: str, specified_dir: str = ...) -> tuple[str, str, str]:
    ...

def get_hash(content: Union[str, bytes], extra: str = ..., hash_type: str = ...) -> str:
    ...

def write(content: Union[str, bytes], extension: str, extra: str = ..., hash_type: str = ..., specified_dir: str = ..., key: Optional[str] = ...) -> tuple[str, str]:
    ...

def write_text(text: str) -> str:
    """
    Write the `text` to a file and return the path computed based on the hash.
    """
    ...

def write_atomic(path_: str, content: Union[str, bytes], make_dirs: bool = ..., encode_utf_8: bool = ...) -> None:
    ...

@dataclasses.dataclass
class TensorMetadataAndValues:
    """
    TensorMetadata plus the elements as a list of raw values.
    Used for hashing inlined constants.
    """
    tensor_metadata: TensorMetadata
    values: list[Any]
    ...


def extract_tensor_metadata_for_cache_key(t: Tensor) -> TensorMetadata:
    """
    Extracts the tensor metadata and removes fields of the TensorMetadata
    that are not needed for caching
    """
    ...

class FxGraphCachePickler(pickle.Pickler):
    """
    Custom pickler to customize the pickling of some objects (Tensors), only for the
    purpose of computing a hash for keying into the FxGraphCache. Tensors contain
    objects that don't pickle and/or vary between runs, and we want to capture the
    data that allow us to compute a stable, but safe hash.
    """
    def __init__(self, gm: torch.fx.GraphModule, has_user_defined_triton_kernels: bool = ...) -> None:
        """
        Create an FX graph pickler. If include_non_inlined=True, then pickling will
        include the _values_ for all Tensors. (Note that any tensors are constants
        attached as attributes to the GraphModule). Otherwise, pickling will include
        only the metadata for these tensors.
        """
        ...
    
    def dumps(self, obj: Any) -> bytes:
        """
        Pickle an object and return a byte string.
        """
        ...
    
    def get_hash(self, obj: Any) -> str:
        """
        Serialize an object and return a hash of the bytes.
        """
        ...
    
    def debug_lines(self, inp: FxGraphHashDetails) -> list[str]:
        """
        Get a printable string describing in more detail all the attributes
        comprising an object. Useful for debugging when one graph hashes
        to a different value than another.
        """
        ...
    


def build_code_hash(roots: list[str] | None, prefix: str, hasher: hashlib._Hash) -> None:
    ...

def torch_key_cache(func: Callable[[], bytes]) -> Callable[[], bytes]:
    """
    This function is a reimplementation of functools.lru_cache with a
    set function that allows prepopulating the cache.
    """
    ...

@torch_key_cache
def torch_key() -> bytes:
    """
    Compute a key that contains relevant information about torch source files
    """
    ...

def get_inductor_root() -> str:
    ...

@dataclasses.dataclass
class OrderedSetHolder:
    """
    See FxGraphHashDetails. Holds a sorted list to support stable hashing
    of set kwargs.
    """
    items: list[Any]
    ...


class BypassFxGraphCache(Exception):
    """
    Exception to indicate that the FxGraphCache should be bypassed.
    """
    ...


class FxGraphHashDetails:
    """
    Object to capture all the details for a compiled FX graph relevant to computing
    a safe and stable cache key.
    """
    EXCLUDED_KWARGS = ...
    def __init__(self, gm: torch.fx.GraphModule, example_inputs: Sequence[InputType], fx_kwargs: _CompileFxKwargs, inputs_to_check: Sequence[int]) -> None:
        ...
    


def compiled_fx_graph_hash(gm: torch.fx.GraphModule, example_inputs: Sequence[InputType], fx_kwargs: _CompileFxKwargs, inputs_to_check: Sequence[int]) -> tuple[str, list[str]]:
    """
    Generate a unique hash of the FX graph for caching.
    """
    ...

def add_ephemeral_timeout_increase_for_distributed(time_saved_ns: int) -> int:
    """
    Ephemerally increases the NCCL timeout when compiling for a distributed job
    Returns amount of seconds increased
    """
    ...

class GuardedCache(Generic[T]):
    """
    Mixin for caches that have guards associated with their entries.
    """
    @classmethod
    def iterate_over_candidates(cls: type[GuardedCache[T]], local: bool, remote_cache: Optional[RemoteCache[JsonDataTy]], key: str) -> Generator[tuple[T, bytes], None, None]:
        ...
    
    @classmethod
    def find_guarded_entry(cls: type[GuardedCache[T]], key: str, local: bool, remote_cache: Optional[RemoteCache[JsonDataTy]], evaluate_guards: Callable[[str, Union[list[int], list[torch.SymInt]]], bool], hints: list[int]) -> tuple[Optional[T], Optional[bytes], dict[str, str]]:
        """
        Find the first cache entry in iterate_over_candidates that passes `evaluate_guards`.

        Args:
            key: The cache key to look up
            local: Whether to check the local cache
            remote_cache: The remote cache to check, if any
            evaluate_guards: Function that evaluates whether a guard passes the check,
                given a list of hint values and the guard expression.
            hints: List of symint hints paired with evaluate_guards

        Returns:
            A tuple of (graph, pickled_content) if found, or (None, None) if not found
        """
        ...
    


@CacheArtifactFactory.register
class InductorCacheArtifact(CacheArtifact):
    @override
    def populate_cache(self) -> None:
        ...
    
    @override
    @staticmethod
    def type() -> str:
        ...
    


class FxGraphCache(GuardedCache[CompiledFxGraph]):
    """
    Supports caching and reusing compiled Fx graphs.

    The overall strategy is as follows:
    - This cache stores entries on disk. When saving an entry, we can't
      serialize callables (that could be C++, Triton, etc.), so we serialize
      their own disk cache location. We then recreate the compiled artifact
      after fetching from disk.
    - For indexing the cache, we gather the fields relevant to identifying an
      FxGraph (the graph module, graph inputs, system settings etc.) into an
      FxGraphCacheDetails object, pickle it, and compute a hash for the key.
      See FxGraphCachePickler.
    - Among the metadata we store, we also include a guards expression that's
      appropriate for validating any symbols for Tensor arguments that have
      symbolic bounds. On cache lookup then, we evaluate those guards in the
      current context to validate that a cached entry can be served.
    - A given graph could have multiple compiled versions, corresponding to
      different sets of guards. Therefore, we store cache entries in the form:
          <temp dir>/<fx graph hash>/<serialized metadata>
    - On lookup, we compute the key from the graph details, iterate over all
      leaf files in the corresponding subdirectory, deserialize the entry, and
      evaluate its guards expression. If the evaluation succeeds, we have a
      cache hit. If it fails, we compile the graph and store a new entry.
    - Finally, on a cache hit, we need to make sure any guards that would
      have been created during compilation are added to the current context.
    """
    @staticmethod
    def cache_hit_post_compile(graph: CompiledFxGraph, cache_info: dict[str, Any], constants: CompiledFxGraphConstants) -> tuple[Optional[CompiledFxGraph], dict[str, Any]]:
        """
        Cache specific post compile steps that need to run if we find a graph in the cache
        This includes putting bundled triton artifacts in the right place,
        reloading the PyCodeCache artifact, etc.

        These don't always happen (i.e. on a cache miss, so they are in a separate function from
        CompiledFxGraph.post_compile)
        """
        ...
    
    @staticmethod
    def prepare_key(gm: torch.fx.GraphModule, example_inputs: Sequence[InputType], fx_kwargs: _CompileFxKwargs, inputs_to_check: Sequence[int], remote: bool) -> tuple[Optional[tuple[str, list[str]]], dict[str, Any]]:
        """
        Checks that the inductor input is cacheable, then computes
        and returns the cache key for the input.
        Returns (key_info, cache_info) where:
        - key_info is (hash_key, debug_lines), and
        - cache_info will contain debug info in the event of BypassFxGraphCache.

        NB: It is possible to have this function return a union instead. But
        I personally believe it is more annoying/difficult to read in that format.
        """
        ...
    
    @staticmethod
    def get_remote_cache() -> Optional[RemoteCache[JsonDataTy]]:
        """
        Attempts to load the remote cache, returns None on error.
        """
        ...
    
    @staticmethod
    def load_with_key(key: str, debug_lines: list[str], example_inputs: Sequence[InputType], local: bool, remote_cache: Optional[RemoteCache[JsonDataTy]], is_backward: bool, constants: CompiledFxGraphConstants, evaluate_guards: Optional[Callable[[str, Union[list[int], list[torch.SymInt]]], bool]] = ...) -> tuple[Optional[CompiledFxGraph], dict[str, Any]]:
        """
        Lookup the graph with the given key, and return results and metadata.
        Doesn't do any logging on its own, because AOTAutograd handles a cache miss
        differently from FXGraphCache.
        """
        ...
    
    @staticmethod
    def clear() -> None:
        """
        Clear out the on-disk cache.
        """
        ...
    


@functools.cache
def split_aot_inductor_output_path(path: str) -> tuple[str, str]:
    """Returns the path where the AOT Inductor compiled kernels are stored."""
    ...

@clear_on_fresh_cache
class CudaKernelParamCache:
    cache: dict[str, dict[str, Any]] = ...
    cache_clear = ...
    @classmethod
    def set(cls, key: str, params: dict[str, Optional[str]], cubin: str, bin_type: str, asm: Optional[str] = ..., asm_type: Optional[str] = ...) -> None:
        ...
    
    @classmethod
    def get(cls, key: str) -> Optional[dict[str, Any]]:
        ...
    
    @classmethod
    def get_keys(cls) -> KeysView[str]:
        ...
    


class AotCodeCompiler:
    """
    Compile AOT Inductor generated code.
    """
    @classmethod
    def compile(cls, graph: GraphLowering, wrapper_code: str, kernel_code: str, serialized_extern_kernel_nodes: Optional[str], *, device_type: str, additional_files: list[str]) -> Union[list[Union[str, Weights]], str]:
        """
        Returns the .so path, or returns a list of files that were generated if
        config.aot_inductor.package=True.
        """
        ...
    


_libgomp: Optional[CDLL] = ...
def custom_op_wrapper(op: str, *args: Any) -> Union[list[c_void_p], c_void_p, None]:
    ...

_HEADER_DIR = ...
_HEADER_LOCK_DIR = ...
@clear_on_fresh_cache
class CppCodeCache:
    """Compiles and caches C++ libraries.  Users of this class supply the source code to
    be compiled, while compilation flags are set by CppBuilder."""
    cache: dict[str, Callable[[], Union[CDLL, ModuleType]]] = ...
    cache_clear = ...
    cpp_compile_command_flags: dict[str, Any] = ...
    @classmethod
    def load_async(cls, main_code: str, device_type: str = ..., submit_fn: Any = ..., extra_flags: Sequence[str] = ..., optimized_code: Optional[str] = ...) -> Any:
        """Compile and load a C++ library.  Returns a callable that returns the loaded
        library."""
        ...
    
    @classmethod
    def load(cls, *args: Any, **kwargs: Any) -> Any:
        ...
    


@clear_on_fresh_cache
class CppPythonBindingsCodeCache(CppCodeCache):
    cache: dict[str, Callable[[], Union[CDLL, ModuleType]]] = ...
    cache_clear = ...
    cpp_compile_command_flags = ...
    entry_function = ...
    call_entry_function = ...
    extra_parse_arg = ...
    suffix_template = ...
    @classmethod
    def load_pybinding_async(cls, argtypes: Sequence[str], main_code: str, device_type: str = ..., num_outputs: int = ..., submit_fn: Any = ..., extra_flags: Sequence[str] = ..., kernel_code: Optional[str] = ...) -> Any:
        """
        Wrap a C++ function in fast Python bindings.

        Args:
            argtypes: The types of args to ENTRY_FUNCTION(), e.g. ["float*", "long"]
            main_code: C++ source code containing ENTRY_FUNCTION().  Will be built at
                -O3 if kernel_code is None (to maximize performance in any kernels that
                are present), or -O1 otherwise (to minimize compile time).
            kernel_code: If present, C++ source code that will be built at -O3 and
                linked to main_code.

        Returns:
            A python version of ENTRY_FUNCTION()
        """
        ...
    
    @classmethod
    def load_pybinding(cls, *args: Any, **kwargs: Any) -> Any:
        ...
    


@clear_on_fresh_cache
class CppWrapperCodeCache(CppPythonBindingsCodeCache):
    cache: dict[str, Callable[[], Union[CDLL, ModuleType]]] = ...
    cache_clear = ...
    cpp_compile_command_flags = ...
    entry_function = ...
    call_entry_function = ...
    extra_parse_arg = ...


@clear_on_fresh_cache
class HalideCodeCache(CppPythonBindingsCodeCache):
    cache: dict[str, Callable[[], Union[ModuleType, CDLL]]] = ...
    cache_clear = ...
    _standalone_runtime_path: Optional[str] = ...
    prefix = ...
    glue_template_cpp = ...
    glue_template_cuda = ...
    standalone_runtime_cuda_init = ...
    @classmethod
    @functools.cache
    def config_hash(cls) -> str:
        ...
    
    @staticmethod
    @functools.cache
    def find_libautoschedule(name: str) -> str:
        ...
    
    @staticmethod
    @functools.cache
    def find_header(name: str) -> str:
        ...
    
    @classmethod
    def generate_halide_async(cls, meta: HalideMeta, source_code: str, submit_fn: Any = ...) -> Callable[[], Any]:
        ...
    
    @classmethod
    def generate_halide(cls, *args: Any, **kwargs: Any) -> Callable[[], Any]:
        ...
    
    @classmethod
    def build_standalone_runtime(cls) -> str:
        ...
    


def touch(filename: str) -> None:
    ...

@clear_on_fresh_cache
class PyCodeCache:
    modules: list[ModuleType] = ...
    modules_no_attr: dict[str, ModuleType] = ...
    linemaps: dict[str, list[tuple[Any, ...]]] = ...
    @classmethod
    def write(cls, source_code: str, extra: str = ...) -> tuple[str, str]:
        ...
    
    @classmethod
    def load(cls, source_code: str, extra: str = ...) -> ModuleType:
        ...
    
    @classmethod
    def load_by_key_path(cls, key: str, path: str, linemap: Optional[list[tuple[int, str]]] = ..., attrs: Optional[dict[str, Any]] = ...) -> ModuleType:
        ...
    
    @classmethod
    def cache_clear(cls, purge: bool = ...) -> None:
        """
        Clear the in-memory module cache. If purge=True, also delete all the
        corresponding on-disk source files.
        """
        ...
    
    @classmethod
    @functools.cache
    def stack_frames_for_code(cls, path: str, lineno: int) -> Optional[list[dict[str, Any]]]:
        ...
    


@torch_key_cache
def cutlass_key() -> bytes:
    """
    Compute a key representing the state of the CUTLASS library.

    Note: OSS and fbcode will have different keys.
    """
    ...

def cuda_compile_command(src_files: list[str], dst_file: str, dst_file_ext: str, extra_args: Optional[list[str]] = ...) -> str:
    ...

class DLLWrapper:
    """A wrapper for a dynamic library."""
    def __init__(self, lib_path: str) -> None:
        ...
    
    def close(self) -> None:
        ...
    
    def __getattr__(self, name: str) -> Callable[..., None]:
        ...
    
    def __enter__(self) -> Self:
        ...
    
    def __exit__(self, *args: Any) -> None:
        ...
    
    def __del__(self) -> None:
        ...
    


@lru_cache
def binary_error_path(output_path: str) -> str:
    """
    standard format for the error path
    """
    ...

@clear_on_fresh_cache
class CUDACodeCache:
    """
    A cache for managing the compilation and loading of CUDA source code specifically for CUTLASS.
    This class handles writing source code to files, compiling them into shared objects, and caching
    the results to avoid redundant compilations. It also manages error handling and logging for the
    compilation process.
    """
    @dataclasses.dataclass
    class CacheEntry:
        input_path: str
        output_path: str
        error_json: Optional[str] = ...
    
    
    cache: dict[str, CacheEntry] = ...
    aot_kernels_o: list[str] = ...
    _SOURCE_CODE_SUFFIX = ...
    @staticmethod
    def cache_clear() -> None:
        ...
    
    @staticmethod
    @lru_cache(maxsize=4)
    def get_kernel_binary_remote_cache(caching_enabled: bool, caching_available: bool) -> Optional[Any]:
        """
        Get or create the class instance of the CUTLASSKernelBinaryRemoteCache.

        Args:
            caching_enabled: Whether binary remote caching is enabled
            caching_available: Whether we're in fbcode environment

        Returns:
            CUTLASSKernelBinaryRemoteCache: The class instance of the kernel binary remote cache
        """
        ...
    
    @classmethod
    def write(cls, source_code: str, dst_file_ext: str) -> tuple[str, str]:
        """
        Writes source code into a file with dst_file_ext as the file extension.
        Returns the hash key of source code, and the path to the file.
        """
        ...
    
    @classmethod
    def compile(cls, source_code: str, dst_file_ext: str, extra_args: Optional[list[str]] = ...) -> tuple[str, str, str]:
        """
        Compiles CUDA source_code into a file with dst_file_ext extension.
        If dst_file_ext is "so", first compiles to ".o" and then links to ".so".
        Returns a tuple of dst_file_path, hash_key, source_code_path
        """
        ...
    
    @classmethod
    def load(cls, source_code: str, dst_file_ext: str) -> tuple[DLLWrapper, str, str]:
        """
        Compiles source code and loads the generated .so file.
        Returns a tuple of DLLWrapper, hash_key, source_code_path
        """
        ...
    


@clear_on_fresh_cache
class ROCmCodeCache:
    @dataclasses.dataclass
    class CacheEntry:
        input_path: str
        output_path: str
        ...
    
    
    cache: dict[str, CacheEntry] = ...
    aot_kernels_o: list[str] = ...
    _SOURCE_CODE_SUFFIX = ...
    _logged_compiler_version = ...
    @staticmethod
    def cache_clear() -> None:
        ...
    
    @classmethod
    def write(cls, source_code: str, dst_file_ext: str) -> tuple[str, str]:
        """
        Writes source code into a file with dst_file_ext as the file extension.
        Returns the hash key of source code, and the path to the file.
        """
        ...
    
    @classmethod
    def compile(cls, source_code: str, dst_file_ext: str, extra_args: Optional[list[str]] = ...) -> tuple[str, str, str]:
        """
        Compiles source_code into a file with dst_file_ext extension,
        using the compile command specific for the ROCm platform.
        Returns a tuple of dst_file_path, hash_key, source_code_path
        """
        ...
    
    @classmethod
    def load(cls, source_code: str, dst_file_ext: str) -> tuple[DLLWrapper, str, str]:
        """
        Compiles source code and loads the generated .so file.
        Returns a tuple of DLLWrapper, hash_key, source_code_path
        """
        ...
    


class CodeCacheFuture:
    def result(self) -> Callable[..., Any]:
        ...
    


class LambdaFuture(CodeCacheFuture):
    def __init__(self, result_fn: Callable[..., Any], future: Optional[Future[Any]] = ...) -> None:
        ...
    
    def result(self) -> Callable[..., Any]:
        ...
    


class StaticAutotunerFuture(CodeCacheFuture):
    """
    A statically launchable CachingAutotuner, loaded from TritonBundler
    """
    def __init__(self, static_autotuner: CachingAutotuner) -> None:
        ...
    
    def result(self) -> CachingAutotuner:
        ...
    


