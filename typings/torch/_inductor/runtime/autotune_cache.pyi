"""
This type stub file was generated by pyright.
"""

import dataclasses
from typing import Any, Optional, TYPE_CHECKING
from typing_extensions import override
from torch.compiler._cache import CacheArtifact, CacheArtifactFactory
from ..remote_cache import JsonDataTy, RemoteCache, RemoteCacheBackend
from .triton_compat import Config

"""
PyTorch Inductor Autotuning Cache System

This module implements a caching system for autotuning configurations in PyTorch's Inductor compiler.
It provides mechanisms to store and retrieve optimal kernel configurations both locally and remotely,
which significantly speeds up compilation by reusing previously discovered optimal parameters.

The caching system includes:
- Local filesystem caching for individual machine reuse
- Remote caching for sharing optimizations across machines
- Bundled caching to efficiently store multiple related configurations
- Cache invalidation based on PyTorch versions and backend changes
- Serialization/deserialization support for worker processes

Key components:
- AutotuneCache: Main class for managing cache access and storage
- AutotuneCacheBundler: Bundles multiple cache entries for efficient storage
- LocalAutotuneCache: Handles filesystem-based caching
- _LocalAutotuneCacheBackend: Low-level file operations for cache storage
- AutotuneCacheArtifact: Integration with PyTorch's artifact system

This caching system is critical for performance as it eliminates the need to re-run
expensive autotuning operations when the same kernels are compiled multiple times.
"""
if TYPE_CHECKING:
    ...
log = ...
_InductorMetaTy = dict[str, object]
def inductor_meta_from_config() -> _InductorMetaTy:
    ...

@CacheArtifactFactory.register
class AutotuneCacheArtifact(CacheArtifact):
    @override
    def populate_cache(self) -> None:
        ...
    
    @override
    @staticmethod
    def type() -> str:
        ...
    
    @override
    @staticmethod
    def encode(content: JsonDataTy) -> bytes:
        ...
    


@dataclasses.dataclass
class AutotuneCache:
    configs_hash: str
    local_cache: Optional[tuple[RemoteCache[JsonDataTy], str]] = ...
    remote_cache: Optional[tuple[RemoteCache[JsonDataTy], str]] = ...
    @staticmethod
    def create(inductor_meta: _InductorMetaTy, filename: str, configs_hash: str) -> Optional[AutotuneCache]:
        ...
    
    def read_best(self, inductor_meta: _InductorMetaTy, configs: list[Config]) -> Optional[Config]:
        ...
    
    def __getstate__(self) -> dict[str, Any]:
        ...
    
    def __setstate__(self, state: dict[str, Any]) -> None:
        ...
    
    def save(self, config: Config, time_taken_ns: int, found_by_coordesc: bool = ..., triton_cache_hash: Optional[str] = ...) -> None:
        ...
    


class _AutotuneCacheBundlerImpl:
    """
    Caches a set of LocalAutotuneCacheBackend entries together in a single
    cache.
    """
    _key: str
    _cache: RemoteCache[JsonDataTy]
    _entries: dict[str, JsonDataTy]
    def end_compile(self) -> None:
        ...
    
    def put(self, basename: str, data: JsonDataTy) -> None:
        ...
    
    def __init__(self, key: str, cache: RemoteCache[JsonDataTy]) -> None:
        ...
    
    def sync(self) -> None:
        ...
    


class AutotuneCacheBundler:
    _bundler: Optional[_AutotuneCacheBundlerImpl] = ...
    def __init__(self) -> None:
        ...
    
    @classmethod
    def begin_compile(cls, inductor_meta: _InductorMetaTy, *, code: Optional[str] = ..., code_hash: Optional[str] = ...) -> None:
        ...
    
    @classmethod
    def end_compile(cls) -> None:
        ...
    
    @classmethod
    def sync(cls) -> None:
        ...
    
    @classmethod
    def put(cls, filename: str, data: JsonDataTy) -> None:
        ...
    


class _LocalAutotuneCacheBackend(RemoteCacheBackend[bytes]):
    ...


class LocalAutotuneCache(RemoteCache[JsonDataTy]):
    def __init__(self) -> None:
        ...
    


