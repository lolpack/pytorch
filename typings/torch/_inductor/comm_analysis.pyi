"""
This type stub file was generated by pyright.
"""

import functools
from enum import IntEnum
from . import ir

class NCCL_COLL(IntEnum):
    ALL_REDUCE = ...
    ALL_GATHER = ...
    REDUCE_SCATTER = ...


class NVIDIA_GPU_TYPE(IntEnum):
    VOLTA = ...
    AMPERE = ...
    HOPPER = ...


@functools.lru_cache
def get_gpu_type() -> NVIDIA_GPU_TYPE:
    ...

def get_collective_type(node: ir.IRNode) -> NCCL_COLL:
    ...

def get_collective_input_size_bytes(node: ir.IRNode) -> int:
    ...

def get_collective_group_size(node: ir.IRNode) -> int:
    ...

class NCCL_HW(IntEnum):
    NVLINK = ...
    PCI = ...
    NET = ...


class NCCL_ALGO(IntEnum):
    TREE = ...
    RING = ...


class NCCL_PROTO(IntEnum):
    LL = ...


baseLat = ...
hwLat = ...
llMaxBws = ...
def estimate_nccl_collective_runtime(node: ir.IRNode) -> float:
    """
    Returns estimated NCCL collective runtime in nanoseconds (ns).

    The following heuristics are copied from https://github.com/NVIDIA/nccl/blob/master/src/graph/tuning.cc.
    We aim to estimate the runtime as accurately as possible.

    Assumptions:
    - only ring algorithm (NCCL_ALGO_RING) is used
    - only Low-Latency protocol (NCCL_PROTO_LL) is used, i.e. Simple or LL128 is not used
    - 8 gpus per node  # TODO: Need to find a way to get accurate "gpus per node" and "# nodes" info.
    - collective is one of: allreduce, reducescatter, allgather
    """
    ...

