"""
This type stub file was generated by pyright.
"""

import abc
import dataclasses
import torch
import torch._ops
import torch.fx
from typing import Any, TYPE_CHECKING
from torch import _prims_common
from torch._prims_common import ELEMENTWISE_TYPE_PROMOTION_KIND
from torch.onnx._internal.fx import _pass
from torch.utils import _python_dispatch
from collections.abc import Mapping, Sequence

if TYPE_CHECKING:
    ...
logger = ...
@dataclasses.dataclass
class TypePromotionSnapshot:
    """Type promotion snapshot for a fx node and its inputs.

    Contains the promoted dtype for args and kwargs that needs promoting.
    Contains the expected node output dtype.
    """
    args_dtypes: Mapping[int, torch.dtype]
    kwargs_dtypes: Mapping[str, torch.dtype]
    out_dtype: torch.dtype
    ...


class TypePromotionRule(abc.ABC):
    """Base class for type promotion rule per 'torch.ops.{namespace}.{op_name}'."""
    def __init__(self, namespace: str, op_name: str) -> None:
        ...
    
    @abc.abstractmethod
    def __hash__(self) -> int:
        ...
    
    @abc.abstractmethod
    def __repr__(self): # -> None:
        ...
    
    @abc.abstractmethod
    def __eq__(self, other: object) -> bool:
        ...
    
    def is_valid(self) -> bool:
        """Check if the rule is valid."""
        ...
    
    @abc.abstractmethod
    def preview_type_promotion(self, args: tuple, kwargs: dict) -> TypePromotionSnapshot:
        """Preview type promotion results for provided set of args and kwargs.

        Returns a TypePromotionSnapshot object that contains the promoted dtypes for
        the arguments and the expected output dtype.
        """
        ...
    


class ElementwiseTypePromotionRule(TypePromotionRule):
    """Defines how to perform elementwise type promotion for 'torch.ops.{namespace}.{op_name}'."""
    _USE_OPMATH: bool = ...
    def __init__(self, namespace: str, op_name: str, promote_args_positions: Sequence[int], promote_kwargs_names: Sequence[str], promotion_kind: _prims_common.ELEMENTWISE_TYPE_PROMOTION_KIND) -> None:
        """Constructs a TypePromotionRule for elementwise operators.

        Args:
            namespace: Namespace of the op. E.g. 'aten' in 'torch.ops.aten.add'.
            op_name: Name of the op. E.g. 'add' in 'torch.ops.aten.add'.
            promote_args_positions: Positions of args to promote.
            promote_kwargs_names: Names of kwargs to promote.
            promotion_kind: Type promotion kind. Refer to [_prims_common.elementwise_dtypes](https://github.com/pytorch/pytorch/blob/main/torch/_prims_common/__init__.py) for detail.  # noqa: B950
        """
        ...
    
    def __repr__(self): # -> str:
        ...
    
    def __eq__(self, other: object, /) -> bool:
        ...
    
    def __hash__(self) -> int:
        ...
    
    def preview_type_promotion(self, args: tuple, kwargs: dict) -> TypePromotionSnapshot:
        ...
    


class DivElementwiseTypePromotionRule(ElementwiseTypePromotionRule):
    """Reference type promotion rule from torch._refs.div.

    Rule depends on the value of the `rounding_mode` argument.
    """
    def __init__(self) -> None:
        ...
    
    def preview_type_promotion(self, args: tuple, kwargs: dict) -> TypePromotionSnapshot:
        ...
    


class ReductionTypePromotionRule(TypePromotionRule):
    def __init__(self, namespace: str, op_name: str, promotion_kind: _prims_common.REDUCTION_OUTPUT_TYPE_KIND) -> None:
        """Constructs a TypePromotionRule for reduction operators.

        Args:
            namespace: Namespace of the op. E.g. 'aten' in 'torch.ops.aten.sum'.
            op_name: Name of the op. E.g. 'sum' in 'torch.ops.aten.sum'.
            promotion_kind: Type promotion kind. Refer to [_prims_common.reduction_dtypes]((https://github.com/pytorch/pytorch/blob/main/torch/_prims_common/__init__.py)) for detail.  # noqa: B950
        """
        ...
    
    def __repr__(self): # -> str:
        ...
    
    def __eq__(self, other: object, /) -> bool:
        ...
    
    def __hash__(self) -> int:
        ...
    
    def preview_type_promotion(self, args: tuple, kwargs: dict) -> TypePromotionSnapshot:
        ...
    


class AllOrAnyReductionTypePromotionRule(ReductionTypePromotionRule):
    """Reference type promotion rule from torch.ops.aten.all or torch.ops.aten.any.

    This is a special case where computation dtype is always torch.bool.
    The result dtype is always uint8 if `dtype` kwarg is uint8, otherwise torch.bool.
    """
    def __init__(self, op_name: str) -> None:
        ...
    
    def preview_type_promotion(self, args: tuple, kwargs: dict) -> TypePromotionSnapshot:
        ...
    


class SumLikeReductionTypePromotionRule(ReductionTypePromotionRule):
    """Reference type promotion rule from torch.ops.aten.sum.

    This is a special case where computation dtype is always torch.int64 for integral arg,
    unless overridden by `dtype` kwarg.
    """
    def preview_type_promotion(self, args: tuple, kwargs: dict) -> TypePromotionSnapshot:
        ...
    


_GENERATED_ATEN_TYPE_PROMOTION_RULE_SET = ...
_EXTRA_TYPE_PROMOTION_RULE_SET = ...
class ElementwiseTypePromotionRuleSetGenerator:
    """Hackly distilling info from reference ops decorated with elementwise type promotion rule.

    The goal is to retrieve the decorator

    ```python
        @elementwise_type_promotion_wrapper(
            type_promoting_args=("a", "b"),
            type_promotion_kind=type_promotion_kind,
        )
    ```

    from the reference ops. It provides info as for which arguments are promoted
    and what kind of promotion is applied.
    """
    @classmethod
    def generate_from_torch_refs(cls) -> set[ElementwiseTypePromotionRule]:
        """Parse type promotion rules from reference ops under torch._C._refs."""
        ...
    


class TypePromotionTable:
    """Type promotion table for torch.ops."""
    def __init__(self) -> None:
        ...
    
    def add_rule(self, rule: TypePromotionRule) -> None:
        """Add a type promotion rule for a python op in a torch.ops module.

        Args:
            rule: Type promotion rule.
            module: Module containing the op. E.g. torch.ops.aten.

        Raises:
            ValueError: If the rule is invalid.
        """
        ...
    
    def get_rule(self, py_op: torch._ops.OpOverloadPacket) -> TypePromotionRule | None:
        """Get type promotion rule for a python op under 'torch.ops.<namespace>'."""
        ...
    


def get_type_promotion_rule(node: torch.fx.Node, type_promotion_table: TypePromotionTable) -> TypePromotionRule | None:
    """Get type promotion rule for a node.

    Args:
        node: Node to get type promotion rule for.
        type_promotion_table: Type promotion table.

    Returns:
        Type promotion rule for the node. None if no rule is found or if the node is not
        representing a torch operator.
    """
    ...

class _OpTraceDispatchMode(_python_dispatch.TorchDispatchMode):
    """Trace ops that were dispatched.

    Utilize the dispatch mechanism in [`__torch_dispatch__`](https://dev-discuss.pytorch.org/t/what-and-why-is-torch-dispatch/557)
    to trace op overloads that were dispatched to. This is used to find the compatible
    op overload for a given op overload packet for different set of args and kwargs.
    """
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    def __torch_dispatch__(self, func, types, args=..., kwargs=...):
        ...
    


def find_compatible_op_overload(op: torch._ops.OpOverloadPacket, args: tuple, kwargs: dict) -> torch._ops.OpOverload:
    """Find compatible OpOverload for an OpOverloadPacket using provided args and kwargs.

    Each "call_function" fx.Node in the fx.GraphModule has a target that represents a torch._ops.OpOverload.
    The OpOverload contains an OpOverloadPacket that holds all the available overloads for the operation.

    During the type promotion pass, there are cases where the types of the args and kwargs may change,
    such as promoting Python numbers to tensors. Consequently, the original OpOverload might not be
    compatible with the updated args and kwargs. This function is used to identify the compatible
    OpOverload for the given args and kwargs.

    Args:
        op: OpOverloadPacket to find compatible OpOverload for.
        args: The positional arguments to consider for compatibility.
        kwargs: The keyword arguments to consider for compatibility.

    Returns:
        torch._ops.OpOverload: The compatible OpOverload found for the given args and kwargs.

    Raises:
        RuntimeError: If no compatible op overload is found.

    Examples:
        >>> import torch
        >>> packet = torch.ops.aten.pow
        >>> args = (torch.tensor([1.0, 2.0]), 2)
        >>> find_compatible_op_overload(packet, args, {})._overloadname
        'Tensor_Scalar'
        >>> args = (torch.tensor([1.0, 2.0]), torch.tensor(2.0))
        >>> find_compatible_op_overload(packet, args, {})._overloadname
        'Tensor_Tensor'
    """
    ...

class _TypePromotionInterpreter(torch.fx.Interpreter):
    """Interpreter that inserts type promotion for each node."""
    def __init__(self, module: torch.fx.GraphModule, type_promotion_table: TypePromotionTable) -> None:
        ...
    
    def run_node(self, n: torch.fx.Node) -> Any:
        """This method is an override which inserts type promotion nodes as needed.

        For each `call_function` node, an initial check is conducted to determine if a type
        promotion rule is applicable. If a relevant rule exists, type casting nodes are
        introduced for the corresponding arguments. The OpOverload of the node is updated
        to one that accommodates the promoted types. Should the output type be different,
        type casting node is inserted for this output.

        The call `super().run_node(node)` is guaranteed to be invoked for each node.
        In the case of new or modified nodes, the result of `super().run_node(node)` is
        used to update its `node.meta["val"]` value.
        """
        ...
    


class InsertTypePromotion(_pass.Transform):
    """Explicitly insert type promotion ops to the graph.

    Underneath, the main pass is driven by `_TypePromotionInterpreter`, which is a subclass
    of `torch.fx.Interpreter` to interpret the fx.Graph and perform the insertion of type
    promotion operations.

    By re-running the new and modified nodes using the interpreter, we can update the
    metadata, specifically the fake tensor stored under node.meta["val"], and ensure it
    reflects the latest changes.
    """
    def __init__(self, module: torch.fx.GraphModule, type_promotion_table: TypePromotionTable | None = ...) -> None:
        ...
    


