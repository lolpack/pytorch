"""
This type stub file was generated by pyright.
"""

import dataclasses
import torch
import torch.fx
import onnx
import onnxruntime
from collections.abc import Mapping, Sequence
from typing import Any, Callable, Final, Optional, TYPE_CHECKING, Union
from typing_extensions import TypeAlias
from torch.fx._compatibility import compatibility
from torch.fx.passes.operator_support import OperatorSupport
from onnxruntime.capi import _pybind_state as ORTC

if TYPE_CHECKING:
    ...
_SUPPORT_ONNXRT: Optional[bool] = ...
__all__ = ["is_onnxrt_backend_supported", "torch_compile_backend", "OrtExecutionProvider", "OrtBackendOptions", "OrtBackend"]
def is_onnxrt_backend_supported() -> bool:
    """Returns ``True`` if ONNX Runtime dependencies are installed and usable
    to support TorchDynamo backend integration; ``False`` otherwise.

    Example::

        # xdoctest: +REQUIRES(env:TORCH_DOCTEST_ONNX)
        >>> import torch
        >>> if torch.onnx.is_onnxrt_backend_supported():
        ...     @torch.compile(backend="onnxrt")
        ...     def f(x):
        ...             return x * x
        ...     print(f(torch.randn(10)))
        ... else:
        ...     print("pip install onnx onnxscript onnxruntime")
        ...
    """
    ...

_dumped_onnx_model: dict[str, int] = ...
logger = ...
class OrtOperatorSupport(OperatorSupport):
    """Operator support for ONNXRuntime backend.

    It has two-level of support decision. One is via support_dict and the other one
    is via extra_support_dict. The logic of using support_dict is implemented in
    OrtOperatorSupport and extra_support_dict is used by OperatorSupport.is_node_supported.
    """
    def __init__(self, support_dict: set[Any], extra_support_dict: dict[str, Any]) -> None:
        ...
    
    def is_node_supported(self, submodules: Mapping[str, torch.nn.Module], node: torch.fx.Node) -> bool:
        ...
    


class OrtExecutionInfoPerSession:
    """Information required to execute torch.fx.GraphModule using onnxruntime.InferenceSession"""
    def __init__(self, session: onnxruntime.InferenceSession, input_names: tuple[str, ...], input_value_infos: tuple[onnx.ValueInfoProto, ...], output_names: tuple[str, ...], output_value_infos: tuple[onnx.ValueInfoProto, ...], input_devices: tuple[ORTC.OrtDevice, ...], output_devices: tuple[ORTC.OrtDevice, ...], example_outputs: Union[tuple[torch.Tensor, ...], torch.Tensor]) -> None:
        ...
    
    def is_supported(self, *args): # -> bool:
        ...
    


@dataclasses.dataclass
class OrtExecutionInfoForAllGraphModules:
    def __init__(self) -> None:
        ...
    
    def search_reusable_session_execution_info(self, graph_module: torch.fx.GraphModule, *args): # -> OrtExecutionInfoPerSession | None:
        ...
    
    def cache_session_execution_info(self, graph_module: torch.fx.GraphModule, info: OrtExecutionInfoPerSession): # -> None:
        ...
    


OrtExecutionProvider: TypeAlias = Union[str, tuple[str, Mapping[str, Any]]]
@dataclasses.dataclass(frozen=True)
@compatibility(is_backward_compatible=False)
class OrtBackendOptions:
    """Options for constructing an ``OrtBackend``, the ONNX Runtime
    backend (``"onnxrt"``) for ``torch.compile``.

    Example::

        >>> @torch.compile(
        ...     backend="onnxrt",
        ...     options=torch.onnx._OrtBackendOptions(...),
        ... )
        ... def ort_function(x):
        ...     return x ** x
    """
    preferred_execution_providers: Optional[Sequence[OrtExecutionProvider]] = ...
    infer_execution_providers: bool = ...
    default_execution_providers: Optional[Sequence[OrtExecutionProvider]] = ...
    preallocate_output: bool = ...
    use_aot_autograd: bool = ...
    ort_session_options: Optional[onnxruntime.SessionOptions] = ...
    pre_ort_model_transforms: Optional[Sequence[Callable[[onnx.ModelProto], None]]] = ...


@compatibility(is_backward_compatible=False)
class OrtBackend:
    """A backend compiles (sub-)graphs in torch.fx.GraphModule to onnxruntime.InferenceSession calls.

    The compiler entry point is OrtBackend.compile, which
        1. partitions the original graph into supported sub-graphs (type: torch.fx.GraphModule) and unsupported
           sub-graphs.
        2. For each supported sub-graph, it replaces its _wrapped_call function with _ort_accelerated_call.
        3. Inside _ort_accelerated_call, it creates onnxruntime.InferenceSession and calls it to execute the sub-graph.
    """
    def __init__(self, options: Optional[OrtBackendOptions] = ...) -> None:
        ...
    
    def compile(self, graph_module: torch.fx.GraphModule, args) -> torch.fx.GraphModule:
        ...
    
    def __call__(self, graph_module: torch.fx.GraphModule, args) -> torch.fx.GraphModule:
        """If ``OrtBackendOptions.use_aot_autograd`` is ``True``, the `auto_autograd` compiler
        will be invoked, wrapping this ``OrtBackend`` instance's ``compile`` method. Otherwise,
        the ``compile`` method is invoked directly."""
        ...
    
    __instance_cache_max_count: Final = ...
    __instance_cache: Final[list[OrtBackend]] = ...
    @staticmethod
    def get_cached_instance_for_options(options: Optional[Union[OrtBackendOptions, Mapping[str, Any]]] = ...) -> OrtBackend:
        """Returns a possibly cached instance of an ``OrtBackend``. If an existing
        backend was created previously through this function with the same options,
        it will be returned. Otherwise a new backend will be created, cached, and
        returned.

        Note: if ``options`` sets ``ort_session_options``, a new ``OrtBackend``
        will always be returned, since ``onnxruntime.SessionOptions`` cannot
        participate in caching."""
        ...
    
    @staticmethod
    def clear_cached_instances(): # -> None:
        ...
    
    @staticmethod
    def get_cached_instances(): # -> tuple[Any, ...]:
        ...
    


@compatibility(is_backward_compatible=False)
def torch_compile_backend(graph_module: torch.fx.GraphModule, args, *, options: Optional[Union[OrtBackendOptions, Mapping[str, Any]]] = ...): # -> GraphModule:
    ...

