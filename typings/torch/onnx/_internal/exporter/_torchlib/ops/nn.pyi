"""
This type stub file was generated by pyright.
"""

from typing import Optional, TYPE_CHECKING
from torch.onnx._internal.exporter._torchlib._tensor_typing import TFloat, TReal
from torch.onnx._internal.exporter._torchlib._torchlib_registry import onnx_impl

"""torch.ops.aten operators under the `core` module."""
if TYPE_CHECKING:
    ...
aten = ...
_INT64_MAX = ...
_INT64_MIN = ...
@onnx_impl(aten.gelu.default, trace_only=True, opset_introduced=20)
def aten_gelu_opset20(self: TReal, approximate: str = ...) -> TReal:
    """gelu(Tensor self, *, bool approximate=False) -> Tensor"""
    ...

@onnx_impl(aten.group_norm.default, trace_only=True, opset_introduced=21)
def aten_group_norm(input: TFloat, num_groups: int, weight: Optional[TFloat] = ..., bias: Optional[TFloat] = ..., eps: float = ..., cudnn_enabled: bool = ...) -> TFloat:
    """group_norm(Tensor input, int num_groups, Tensor? weight=None, Tensor? bias=None, float eps=1e-05, bool cudnn_enabled=True) -> Tensor"""
    ...

@onnx_impl(aten.scaled_dot_product_attention.default, trace_only=True, opset_introduced=23)
def aten_scaled_dot_product_attention_23(query: TFloat, key: TFloat, value: TFloat, attn_mask: Optional[TFloat] = ..., dropout_p: float = ..., is_causal: bool = ..., scale: Optional[float] = ..., enable_gqa: bool = ...) -> TFloat:
    """scaled_dot_product_attention(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0.0, bool is_causal=False, *, float? scale=None, bool enable_gqa=False) -> Tensor

    Reference:
        1. https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html
        2. https://onnx.ai/onnx/operators/onnx__Attention.html

    Attempts to convert SDPA to Attention onnx op and fallbacks to an onnx graph equivivalent to the following PyTorch code::
        scale_factor = 1 / math.sqrt(Q.size(-1)) if scale is None else scale
        attn_mask = (
            torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)
            if is_causal
            else attn_mask
        )
        attn_mask = (
            attn_mask.masked_fill(not attn_mask, -float("inf"))
            if attn_mask.dtype == torch.bool
            else attn_mask
        )
        attn_weight = torch.softmax(
            (Q @ K.transpose(-2, -1) *  attn_mask, dim=-1
        )
        attn_weight = torch.dropout(attn_weight, dropout_p)
        return attn_weight @ V

    where Q, K, V are the query, key, and value tensors, respectively.
    L is the target sequence length, S is the source sequence length, and E is the embedding size.
    """
    ...

