"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn
from collections.abc import Sequence
from typing import Any, Callable, NewType, Optional, Protocol, TypeVar
from torch._inductor.cudagraph_utils import BoxedDeviceIndex
from torch._inductor.output_code import OutputCode
from torch._inductor.utils import BoxedBool, InputType
from torch._subclasses import FakeTensorMode
from torch.fx.experimental.symbolic_shapes import ShapeEnv
from ._aot_autograd.schemas import AOTConfig, GraphSignature, ViewAndMutationMeta

static_inputs_log = ...
zip = ...
AOT_COUNTER = ...
aot_autograd_decompositions = ...
FakifiedFlatArgs = NewType("FakifiedFlatArgs", list[Any])
TOutputCode = TypeVar("TOutputCode", bound=OutputCode)
class AOTDispatchCompiler(Protocol):
    """
    Represents a fw or bw_compiler passed to AOTAutograd.
    """
    def __call__(self, gm: torch.fx.GraphModule, example_inputs: Sequence[InputType]) -> Any:
        ...
    


class SerializableAOTDispatchCompiler(AOTDispatchCompiler):
    """
    Represents an AOTDispatchCompiler that returns an OutputCode, and is
    therefore cacheable. SerializableAOTDispatchCompiler always return an OutputCode.
    A _CompileFxCallable usually gets converted into an AOTDispatchCompiler after binding all of
    the kwargs in _CompileFxKwargs.
    """
    def __init__(self, output_code_ty: type[TOutputCode], compiler_fn: Callable[[torch.fx.GraphModule, Sequence[InputType]], TOutputCode]) -> None:
        ...
    
    def __call__(self, gm: torch.fx.GraphModule, example_inputs: Sequence[InputType]) -> OutputCode:
        ...
    


def process_inputs(flat_args: list[Any], aot_config: AOTConfig, fake_mode: FakeTensorMode, shape_env: Optional[ShapeEnv], ignore_shape_env: bool = ...) -> FakifiedFlatArgs:
    ...

def construct_fake_mode(flat_args: list[Any], aot_config: AOTConfig) -> tuple[FakeTensorMode, Optional[ShapeEnv]]:
    ...

def create_aot_dispatcher_function(flat_fn, fake_flat_args: FakifiedFlatArgs, aot_config: AOTConfig, fake_mode: FakeTensorMode, shape_env: Optional[ShapeEnv]) -> tuple[Callable, ViewAndMutationMeta]:
    ...

def aot_function(fn: Callable, fw_compiler: Callable, bw_compiler: Optional[Callable] = ..., partition_fn: Callable = ..., decompositions: Optional[dict] = ..., num_params_buffers: int = ..., keep_inference_input_mutations: bool = ..., inference_compiler: Optional[Callable] = ..., *, dynamic=..., enable_log=...) -> Callable:
    """
    Traces the forward and backward graph of :attr:`fn` using torch dispatch
    mechanism, and then compiles the generated forward and backward graphs
    through :attr:`fw_compiler` and :attr:`bw_compiler`.

    :func:`aot_function` traces the forward and backward graph ahead of time,
    and generates a joint forward and backward graph.  :attr:`partition_fn` is
    then used to separate out forward and backward graphs. The partitioner
    function can be used to perform optimizations such as recomputation. One can
    set `decompositions` dictionary to decompose the operators into a sequence
    of core or simpler operators supported by the backend compilers.

    .. warning::
        This API is experimental and likely to change.

    Args:
        fn (Callable): A Python function that takes one ore more arguments. Must
            return one or more Tensors.
        fw_compiler (Callable): A Python function that accepts an Fx graph with
            Aten ops and input args, and returns a Callable that semantically is
            equivalent to the input Fx graph.
        bw_compiler (Optional[Callable]): A Python function that accepts an
            Fx graph with Aten ops and input args, and returns a Callable that
            semantically is equivalent to the input Fx graph.  Default: None
            (when None, it defaults to the :attr:`fw_compiler`)
        partition_fn (Callable): A Python function that takes a joint forward
            and backward graph, and partitions it into separate forward and
            backward graphs.
        decompositions (Dict): A dictionary to define the decomposition of
            larger Aten ops into simpler or core Aten ops.
        inference_compiler (Optional[Callable]): A Python function that accepts an
            Fx graph with Aten ops and input args, and returns a Callable that
            semantically is equivalent to the input Fx graph. inference_compiler is invoked
            if no autograd is needed. Default: None
            (when None, it defaults to the :attr:`fw_compiler`)
    Returns:
        Returns a ``Callable`` that retains the eager behavior of the original
        :attr:`fn`, but with forward and backward graph compiled via
        :attr:`fw_compile` and :attr:`bw_compile`.

    A simple example usage of :func:`aot_function` is as follows. This example
    will print the forward and backward graphs of the function ``fn``

        >>> fn = lambda x: x.sin().cos()
        >>> def print_compile_fn(fx_module, args):
        >>>     print(fx_module)
        >>>     return fx_module
        >>> aot_fn = aot_function(fn, print_compile_fn)
        >>> x = torch.randn(4, 5, requires_grad=True)
        >>> aot_fn(x)
    """
    ...

def aot_module(mod: nn.Module, *args, **kwargs) -> nn.Module:
    """
    Traces the forward and backward graph of :attr:`mod` using torch dispatch
    tracing mechanism. It is wrapper function, that underneath uses
    :func:`aot_function` to perform tracing and compilation.

    :func:`aot_module` lifts the parameters and buffers of ``nn.Module`` as inputs
    to a new callable which is then compiled through :func:`aot_function`.

    .. warning::
        This API is experimental and likely to change.

    Args:
        mod (Callable): A ``nn.Module`` module.
        args : args to be passed to :func:`aot_function`
        kwargs : kwargs to be passed to :func:`aot_function`

    Returns:
        Returns a ``nn.Module`` that retains the eager behavior of the original
        :attr:`mod`, but with forward and backward graph compiled.

    """
    class AOTModule(nn.Module):
        ...
    
    

def aot_module_simplified(mod: nn.Module, args, fw_compiler: AOTDispatchCompiler, bw_compiler: Optional[AOTDispatchCompiler] = ..., partition_fn: Callable = ..., decompositions: Optional[dict] = ..., keep_inference_input_mutations=..., inference_compiler: Optional[AOTDispatchCompiler] = ..., cudagraphs: Optional[BoxedBool] = ..., boxed_forward_device_index: Optional[BoxedDeviceIndex] = ..., ignore_shape_env: bool = ...) -> nn.Module:
    """
    This is the simplified or low overhead version of aot_module. For frontends
    like TorchDynamo, the input functions/modules to AOT are static and have
    unpacked inputs/outputs. This gives us an opportunity to remove the
        (1) pytree overhead to parse inputs/outputs,
        (2) AOT Autograd cache,
        (3) Reading of params/buffers in every forward call

    :func:`aot_module_simplified` removes these overheads.
    """
    ...

def aot_export_module(mod: nn.Module, args, *, decompositions: Optional[dict] = ..., trace_joint: bool, output_loss_index: Optional[int] = ..., pre_dispatch: bool = ..., dynamic_shapes: Optional[bool] = ..., kwargs=...) -> tuple[torch.fx.GraphModule, GraphSignature]:
    """
    This function takes in a module, and returns:
    (1) an FX graph that can be exported
    (2) some metadata about the graph

    If `trace_joint=True` we will return a joint graph of the forward + backward.

    The traced FX graph will have the following properties compared to the original module:
    (1) Inputs and outputs to the module will be pytree-flattened
    (2) Parameters and buffers on the module will be lifted into graph inputs,
        graph_inputs = (*parameters, *buffers, *user_inputs)
    (3) The graph will be fully functionalized
    (4) Any input mutations will be converted into additional outputs in the graph,
        meaning whoever calls this graph is responsible for applying the mutations
        back to the original inputs.
    (5) If is_joint is provided the graph will return parameter gradients in addition to user outputs.
        The graph output will look like:
        graph_outputs = (*updated_inputs, *user_outputs, *param_gradients)

    There are also several restrictions on what modules can use this API. In particular:
    (1) If trace_joint is specified, we expect the loss function to be **fused**
        into the module forward. One of the outputs to the forward must be a scalar loss,
        which is specified with `output_loss_index`.
        All other outputs to the forward are presumed to not require gradients.
    (2) This API cannot capture optimizers (although in theory we could build an API for this).
    (3) Metadata mutations on params/buffers/inputs are banned.
    (4) Data mutations on anything that requires gradients are banned (parameters)
    (5) If an input is mutated, it is not allowed to alias any other inputs.
    (6) Parameters must not be duplicated.
    """
    ...

def aot_export_joint_simple(func: Callable, args, *, trace_joint: bool, num_params_buffers: int = ..., decompositions: Optional[dict] = ...) -> torch.fx.GraphModule:
    """
    A simplified version of export. Used by higher order operators.

    This function makes a high-level "no calling convention changes" guarantee:
    - If no inputs require grad (so we export an inference graph),
      there are *no* calling convention change between the exported graph, and "func".
    - If at least one input requires grad (so we trace out and export a joint fw-bw graph),
      Then if you were partition the graph into a separate forward and backward graph,
      The forward graph will have no calling convention changes compared to "func".

    The above also relies on some strong restrictions around which functions this API accepts:
    (1) `args` cannot contain any pytrees (they must have been pytree_flattened already)
    (2) `func` cannot mutate any inputs
    (3) The outputs of `func` cannot alias any inputs.

    Note: this function is only lightly tested today. It will probably be tested more heavily by higher order ops.
    """
    ...

compiled_function = ...
compiled_module = ...
