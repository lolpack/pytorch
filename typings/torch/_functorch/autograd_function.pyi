"""
This type stub file was generated by pyright.
"""

import torch
from typing import NamedTuple
from torch._C._functorch import TransformType
from torch._ops import HigherOrderOperator

class CustomFunctionHigherOrderOperator(HigherOrderOperator):
    def __init__(self) -> None:
        ...
    
    def __call__(self, autograd_function, *args, **kwargs): # -> Any | None:
        ...
    


custom_function_call = ...
@custom_function_call.py_impl(TransformType.Grad)
@custom_function_call.py_impl(TransformType.Jvp)
def custom_function_call_grad(interpreter, autograd_function, *operands):
    ...

def generate_single_level_function(interpreter, autograd_function): # -> type[_]:
    ...

NO_OUT_DIMS = ...
def wrap_outputs_maintaining_identity(outputs, unwrapped_inputs, orig_inputs, wrap_fn, out_dims=...): # -> PyTree:
    ...

class VmapInfo(NamedTuple):
    batch_size: int
    randomness: str
    ...


def has_overridden_vmap_rule(autograd_function): # -> bool:
    ...

def validate_vmap_returns_tuple_of_two_elements(result): # -> None:
    ...

@custom_function_call.py_impl(TransformType.Vmap)
def custom_function_call_vmap(interpreter, autograd_function, *operands, **kwargs): # -> tuple[Any, ...] | Any | None:
    ...

def custom_function_call_vmap_helper(interpreter, vmap_function, op, *operands, **kwargs): # -> Any | None:
    ...

def unpack_outputs(outputs): # -> tuple[Any, tuple[Any, ...] | Any]:
    ...

def custom_function_call_vmap_generate_rule(interpreter, autograd_function, *operands): # -> tuple[Any, ...]:
    ...

@custom_function_call.py_impl(TransformType.Functionalize)
def custom_function_call_functionalize(interpreter, autograd_function, generate_vmap_rule, *operands):
    ...

def vmapify_autograd_function(autograd_function, in_dims, batch_size, randomness): # -> type[_]:
    ...

def get_tangents_in_dims(input_dims, tangents): # -> PyTree:
    ...

class WrappedCtx:
    _pt_reserved_attrs: tuple[str, ...] = ...
    def __init__(self, ctx) -> None:
        ...
    
    def __getattr__(self, name): # -> Any:
        ...
    
    def __setattr__(self, name, value): # -> None:
        ...
    


class CtxWithSavedTensors(WrappedCtx):
    _pt_reserved_attrs = ...
    def __init__(self, ctx, new_saved_tensors) -> None:
        ...
    
    @property
    def saved_tensors(self): # -> Any:
        ...
    


class CtxCustomSave(WrappedCtx):
    _pt_reserved_attrs = ...
    def __init__(self, ctx, current_level) -> None:
        ...
    
    def save_for_backward(self, *tensors): # -> None:
        ...
    
    def save_for_forward(self, *tensors): # -> None:
        ...
    


def reductify(grad_input, grad_input_bdim, input_bdim, batch_size, target_shape_without_bdim_to_reduce_to=...): # -> tuple[Any | None, ...]:
    ...

def reductify_leaf(grad_input, grad_input_bdim, input_bdim, batch_size, target_shape_without_bdim_to_reduce_to=...): # -> None:
    ...

def autograd_function_forward_rewritten(original_forward, original_setup_context): # -> Callable[..., Any]:
    ...

class AutogradFunctionApply(HigherOrderOperator):
    def __init__(self) -> None:
        ...
    
    def __call__(self, fwd, bwd, *fwd_args, **fwd_kwargs): # -> Any | None:
        class ApplyTemplate(torch.autograd.Function):
            ...
        
        
    


autograd_function_apply = ...
