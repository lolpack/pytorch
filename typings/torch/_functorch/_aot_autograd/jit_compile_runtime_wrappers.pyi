"""
This type stub file was generated by pyright.
"""

import dataclasses
import torch
from typing import Any, Callable, Optional, TYPE_CHECKING
from torch._subclasses import FakeTensor
from torch.fx.graph_module import GraphModule
from .schemas import AOTConfig, ViewAndMutationMeta

"""
Functions in this module do most of the "work" of AOTAutograd.
An aot_dispatch_* function:
- Takes in the input flat_fn, flat_args, and some metadata
- Runs a set of pre compile wrappers (e.g. argument deduping)
- Runs the actual compiler
- Wraps the returned callable in a set of post compile wrappers
- Returns the wrapped callable and metadata.
"""
if TYPE_CHECKING:
    ...
zip = ...
log = ...
aot_joint_log = ...
aot_graphs_log = ...
aten = ...
DispatchReturn = tuple[Callable, ViewAndMutationMeta]
def aot_dispatch_export(flat_fn: Callable, flat_args: list[Any], aot_config: AOTConfig, *, fw_metadata: ViewAndMutationMeta, needs_autograd: bool) -> DispatchReturn:
    ...

def sanitize_aot_config(input: AOTConfig) -> AOTConfig:
    ...

def aot_dispatch_base(flat_fn, flat_args: list[Any], aot_config: AOTConfig, *, fw_metadata: ViewAndMutationMeta) -> DispatchReturn:
    """
    Handles functions that don't need autograd. Runs wrappers and compiles with fw_compiler.
    """
    ...

def collect_fw_donated_buffer_idxs(fw_ins: list[Optional[FakeTensor]], user_fw_outs: list[Optional[FakeTensor]], bw_outs: list[Optional[FakeTensor]], saved_tensors: list[FakeTensor]) -> list[int]:
    """
    Checks if the saved tensors are donated buffers, which means a saved tensor is not
    an alias of any tensors in fw_ins, user_fw_outs, and bw_outs.
    """
    ...

def collect_bw_donated_buffer_idxs(fw_module: torch.fx.GraphModule, bw_module: torch.fx.GraphModule, fw_metadata: ViewAndMutationMeta) -> list[int]:
    """
    Collects backward donated buffer indexes from fw_module and bw_module.
    """
    ...

@dataclasses.dataclass
class InvokeSubgraphHopGraphs:
    """
    A data structure to hold all the information needed to partition the
    `joint_hop_gm` and joint graph and the restitch the `new_fw_hop_gm` and
    `new_bw_hop_gm` into the bigger `joint_gm`.
    """
    partitioning_done: bool = ...
    old_num_fw_outputs: Optional[int] = ...
    old_num_fw_inputs: Optional[int] = ...
    new_fw_hop_gm: Optional[torch.fx.GraphModule] = ...
    new_bw_hop_gm: Optional[torch.fx.GraphModule] = ...
    new_num_sym_nodes: Optional[int] = ...
    new_num_saved_nodes: Optional[int] = ...


def run_joint_graph_passes_on_hops(joint_gm: torch.fx.GraphModule, joint_inputs: Any, aot_config: AOTConfig) -> torch.fx.GraphModule:
    """
    This pass runs the joint graph passes on the HOP graph. In torch.compile, we
    typically have many passes which work on the joint graph and then end with a
    partitioner.


    The partitioner part is quite mechanical to handle. HOP have their own
    forward and backward graph. The process can be broken into following steps

    1) Get a `joint_hop_gm` from the `fw_hop_gm` and `bw_hop_gm`
    2) Run joint graph passes on the `joint_hop_gm` to get `new_fw_hop_gm` and `new_bw_hop_gm`
    3) Stitch the `new_fw_hop_gm` and `new_bw_hop_gm` back into the `joint_gm`.

    The terminology used in the code is
    `joint_graph/joint_gm` : Refers to the main graph. This may contain many HOPs which have their own `hop_graph`
    `fw_hop_graph/fw_hop_gm` : Refers to the forward graph associated with a HOP.
    `bw_hop_graph/bw_hop_gm` : Refers to the backward graph associated with a HOP.
    `joint_hop_graph/joint_hop_gm` : Refers to the subgraph associated with the HOP like invoke_subgraph.
    `new_fw_hop_graph/new_fw_hop_gm` : Refers to the forward graph after partitioning is applied to `joint_hop_gm`.
    `new_bw_hop_graph/new_bw_hop_gm` : Refers to the backward graph after partitioning is applied to `joint_hop_gm`.

    NB: This pass works for invoke_subgraph today because we took extra care in
    the Autograd.Dispatch key of invoke_subgraph to vastly simplify Step 1.
    """
    ...

def maybe_log_graph(gm, graph_name, aot_config, structured_log_prefix_fn, out_structured_logs: Optional[list[str]] = ...): # -> None:
    ...

def create_wrap_fn(fn, args): # -> tuple[_Wrapped[Callable[..., Any], Any, Callable[..., Any], PyTree], Any]:
    ...

def prepare_hook_gm(aot_config, fn, args): # -> GraphModule:
    ...

def maybe_inline_graph_saved_tensors_hooks(fw_module, bw_module, num_inner_fwd_outputs, inner_meta, aot_config, static_input_indices):
    ...

def aot_dispatch_autograd(flat_fn, flat_args: list[Any], aot_config: AOTConfig, *, fw_metadata: ViewAndMutationMeta) -> DispatchReturn:
    """
    Autograd logic. Generates a joint graph, partitions it, manipulates the input with various wrappers,
    and returns a wrapped torch.autograd.Function with a forward and backward.
    """
    ...

