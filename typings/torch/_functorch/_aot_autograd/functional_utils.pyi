"""
This type stub file was generated by pyright.
"""

import torch
from dataclasses import dataclass
from typing import Optional
from torch import Tensor
from torch.fx.experimental.symbolic_shapes import SymIntEqByExpr

"""
This file contains utilities related to functionalization in AOTAutograd:
1. converting to/from functional tensors
2. detecting Tensor mutations - both metadata and Tensor value
3. regenerating/replaying views from their base
4. checking if a graph is functional i.e. whether it contains any mutation ops
"""
aot_joint_log = ...
def to_fun(t): # -> Tensor | FunctionalTensor:
    ...

def sync_functional_tensor(t): # -> None:
    ...

def from_fun(t): # -> Tensor:
    ...

def is_fun(t): # -> bool:
    ...

def has_data_mutation(t): # -> bool:
    ...

def are_all_mutations_hidden_from_autograd(t): # -> bool:
    ...

def are_all_mutations_under_no_grad_or_inference_mode(t): # -> bool:
    ...

def was_inductor_storage_resized(t): # -> bool | None:
    ...

def has_metadata_mutation(f_arg, arg, *, check_only_storage_mutation: bool): # -> bool:
    ...

def gen_alias_from_base(aliased_base_tensor, target_meta_tensor, target_requires_grad, target_functional_tensor: Optional[FunctionalTensorMetadataEq] = ..., *, replay_views): # -> Tensor:
    ...

def has_same_metadata(t1, t2): # -> TypeGuard[Tensor] | Literal[False]:
    ...

@dataclass(frozen=True)
class MetadataKey:
    """
    This should be equal whenever has_same_metadata would return True
    """
    size: tuple[SymIntEqByExpr, ...]
    layout: torch.layout
    is_sparse: bool
    stride: Optional[tuple[SymIntEqByExpr, ...]]
    storage_offset: Optional[SymIntEqByExpr]
    is_conj: bool
    is_neg: bool
    @staticmethod
    def make(t): # -> MetadataKey:
        ...
    


class FunctionalTensorMetadataEq:
    def __init__(self, tensor: torch.Tensor) -> None:
        ...
    
    def __eq__(self, other: object) -> bool:
        ...
    


def was_tensor_updated(arg, new_arg): # -> bool:
    ...

def was_tensor_metadata_updated(arg, new_arg): # -> bool:
    ...

def assert_functional_graph(fx_g: torch.fx.Graph) -> int:
    ...

def propagate_input_mutation_stacktraces(fx_g: torch.fx.Graph) -> None:
    ...

