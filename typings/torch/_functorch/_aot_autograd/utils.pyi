"""
This type stub file was generated by pyright.
"""

import torch
import torch.utils._pytree as pytree
from typing import Any, Callable, Optional, Union

"""
Contains various utils for AOTAutograd, including those for handling collections.
"""
KNOWN_TYPES = ...
original_zip = zip
aot_graphs_effects_log = ...
def strict_zip(*iterables, strict=..., **kwargs): # -> zip[tuple[Any, ...]]:
    ...

def partial_flatten_asdict(obj: Any) -> Any:
    ...

def normalize_as_list(x): # -> list[Any]:
    ...

def make_boxed_func(f): # -> Callable[..., Any]:
    ...

def make_boxed_compiler(compiler): # -> _Wrapped[Callable[..., Any], Any, Callable[..., Any], Callable[..., Any]]:
    ...

def call_func_at_runtime_with_args(f, args: Union[tuple[Any], list[Any]], steal_args=..., disable_amp=...): # -> list[Any]:
    ...

class PytreeThunk:
    spec: Optional[pytree.TreeSpec] = ...
    is_simple: Optional[bool] = ...
    is_really_simple: Optional[bool] = ...
    def set(self, spec: pytree.TreeSpec) -> None:
        ...
    
    def unflatten(self, x: list[Any]) -> Any:
        ...
    


def create_tree_flattened_fn(fn, args, kwargs=...) -> tuple[Callable, PytreeThunk]:
    ...

def maybe_to_fresh_input(idx, t, meta): # -> Tensor:
    ...

def is_with_effects(node):
    ...

def is_with_effects_op(node, op):
    ...

def unlift_tokens(fw_module, fw_metadata, aot_config, bw_module=...): # -> None:
    ...

def root_module_when_exporting_non_strict(flat_fn): # -> None:
    ...

def copy_fwd_metadata_to_bw_nodes(fx_g): # -> None:
    """
    Input: `fx_g` which contains the joint fwd+bwd FX graph created by
    aot_autograd.

    This function walks the graph and copies over metadata from forward nodes
    to backward nodes, using the `seq_nr` field as a one-to-many mapping
    from forward node to backward node. This metadata is useful for performance
    profiling and debugging.
    """
    ...

def register_buffer_assignment_hook(mod, assigned_buffers): # -> RemovableHandle:
    """
    Register a hook that intercepts buffer assignments.
    This is used to detect when a buffer is assigned to, and then we can
    map that buffer to the corresponding proxy node in the graph.
    """
    ...

def contain_metadata_mutation_ops(module: torch.fx.GraphModule) -> bool:
    """
    Checks if the module contains any metadata mutation ops.
    """
    ...

def get_cuda_generator_meta_val(device_idx: int): # -> Generator:
    """
    Get a generator value to use as a meta val

    newly cloned generator will not contain tensors. it is only Generators that are
    registered to a CUDAGraph that contain tensors. since this does not contain Tensor
    it is fine to use in the meta.
    """
    ...

def top_saved_tensors_hooks():
    ...

def saved_tensors_hooks_are_inlineable(hooks) -> bool:
    ...

