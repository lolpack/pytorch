"""
This type stub file was generated by pyright.
"""

import torch
from collections.abc import Sequence
from enum import Enum
from functools import reduce
from typing import Callable, Optional, TypeVar, Union
from typing_extensions import ParamSpec
from torch import Tensor
from torch._prims_common import ELEMENTWISE_TYPE_PROMOTION_KIND
from torch._prims_common.wrappers import out_wrapper

_T = TypeVar("_T")
_P = ParamSpec("_P")
aten = ...
_meta_lib_dont_use_me_use_register_meta = ...
def register_meta(op) -> Callable[[Callable[_P, _T]], Callable[_P, _T]]:
    ...

def elementwise_meta(*args, type_promotion: ELEMENTWISE_TYPE_PROMOTION_KIND): # -> FakeTensor:
    ...

def toRealValueType(dtype):
    ...

def check_inplace_broadcast(self_shape, *args_shape): # -> None:
    ...

@register_meta([aten.linspace, aten.logspace])
@out_wrapper()
def meta_linspace_logspace(start, end, steps, base=..., dtype=..., device=..., layout=..., pin_memory=..., requires_grad=...): # -> Tensor:
    ...

@register_meta([aten.take.default, aten.take.out])
@out_wrapper()
def meta_take(self, index):
    ...

@register_meta([aten.linalg_cross.default, aten.linalg_cross.out])
@out_wrapper()
def linalg_cross(self, other, *, dim=...):
    ...

@register_meta(aten.linalg_matrix_exp)
@out_wrapper()
def linalg_matrix_exp(self): # -> Tensor:
    ...

@register_meta([aten.cummax.default, aten.cummax.out, aten.cummin.default, aten.cummin.out])
@out_wrapper("values", "indices")
def cummaxmin(self, dim): # -> tuple[Tensor, Tensor]:
    ...

@register_meta([aten.logcumsumexp.default, aten.logcumsumexp.out])
@out_wrapper()
def logcumsumexp(self, dim): # -> Tensor:
    ...

@register_meta([aten._fft_c2c.default, aten._fft_c2c.out])
@out_wrapper()
def meta_fft_c2c(self, dim, normalization, forward):
    ...

cufft_max_ndim = ...
def use_optimized_cufft_path(dim: list[int]): # -> bool:
    ...

@register_meta([aten._fft_r2c.default, aten._fft_r2c.out])
@out_wrapper()
def meta_fft_r2c(self, dim, normalization, onesided):
    ...

@register_meta(aten.randperm.generator_out)
def meta_randperm(n, *, generator=..., out): # -> TensorLikeType:
    ...

@register_meta(aten.randperm.default)
def meta_randperm_default(n, *, dtype=..., layout=..., device=..., pin_memory=...): # -> Tensor:
    ...

@register_meta([aten.randint.default, aten.randint.out])
@out_wrapper()
def meta_randint(high, size, *, dtype=..., layout=..., device=..., pin_memory=...): # -> Tensor:
    ...

@register_meta([aten.randint.low, aten.randint.low_out])
@out_wrapper()
def meta_randint_low(low, high, size, *, dtype=..., layout=..., device=..., pin_memory=...): # -> Tensor:
    ...

@register_meta([aten.rand.default, aten.rand.out])
@out_wrapper()
def meta_rand_default(size, *, dtype=..., layout=..., device=..., pin_memory=...): # -> Tensor:
    ...

@register_meta([aten._fft_c2r.default, aten._fft_c2r.out])
@out_wrapper()
def meta_fft_c2r(self: Tensor, dim: list[int], normalization: int, lastdim: int):
    ...

@register_meta(aten.copy_.default)
def meta_copy_(self, src, non_blocking=...):
    ...

def inferUnsqueezeGeometry(tensor, dim): # -> tuple[list[Any], list[Any]]:
    ...

@register_meta(aten.unsqueeze_.default)
def meta_unsqueeze_(self, dim):
    ...

@register_meta(aten._sparse_semi_structured_linear)
def meta_sparse_structured_linear(input: Tensor, weight: Tensor, _meta: Tensor, bias: Optional[Tensor] = ..., _activation_opt: Optional[str] = ..., out_dtype: Optional[torch.dtype] = ...):
    ...

@register_meta(aten._sparse_semi_structured_mm)
def meta_sparse_structured_mm(mat1: Tensor, mat1_meta: Tensor, mat2: Tensor, out_dtype: Optional[torch.dtype] = ...):
    ...

@register_meta(aten._sparse_semi_structured_addmm)
def meta_sparse_structured_addmm(input: Tensor, mat1: Tensor, mat1_meta: Tensor, mat2: Tensor, *, alpha=..., beta=..., out_dtype: Optional[torch.dtype] = ...):
    ...

@register_meta(aten._cslt_sparse_mm)
def meta__cslt_sparse_mm(compressed_A: torch.Tensor, dense_B: torch.Tensor, bias: Optional[Tensor] = ..., alpha: Optional[Tensor] = ..., out_dtype: Optional[torch.dtype] = ..., transpose_result: bool = ..., alg_id: int = ..., split_k: int = ..., split_k_mode: int = ...):
    ...

@register_meta(aten.index_reduce.default)
def meta_index_reduce(self: Tensor, dim: int, index: Tensor, source: torch.Tensor, reduce: str, *, include_self: bool = ...) -> Tensor:
    ...

@register_meta(aten.index_reduce_.default)
def meta_index_reduce_(self: Tensor, dim: int, index: Tensor, source: torch.Tensor, reduce: str, *, include_self: bool = ...) -> Tensor:
    ...

@out_wrapper()
@register_meta(aten.index_select.default)
def meta_index_select(self, dim, index):
    ...

@register_meta(aten.segment_reduce.default)
def meta_segment_reduce(data: Tensor, reduce: str, *, lengths: Optional[Tensor] = ..., indices: Optional[Tensor] = ..., offsets: Optional[Tensor] = ..., axis: int = ..., unsafe: bool = ..., initial=...) -> Tensor:
    ...

@register_meta([aten.max.default, aten.max.unary_out])
@out_wrapper()
def meta_max(self):
    ...

@register_meta(aten.max.dim)
def meta_max_dim(self, dim, keepdim=...): # -> tuple[Any, Any]:
    ...

@register_meta([aten.min.default, aten.min.unary_out])
@out_wrapper()
def meta_min(self):
    ...

@register_meta(aten.min.dim)
def meta_min_dim(self, dim, keepdim=...): # -> tuple[Any, Any]:
    ...

@register_meta(aten.angle.default)
def meta_angle(self): # -> Tensor:
    ...

@register_meta(aten.angle.out)
def meta_angle_out(self, out):
    ...

@register_meta(aten._assert_async.default)
def assert_async(val): # -> None:
    ...

@register_meta(aten._assert_async.msg)
def assert_async_meta(val, assert_msg): # -> None:
    ...

@register_meta(aten._print.default)
def print_meta(s): # -> None:
    ...

@register_meta(aten._make_dep_token.default)
def make_dep_token(*, dtype=..., layout=..., device=..., pin_memory=..., memory_format=...): # -> Tensor:
    ...

@register_meta(aten.sym_constrain_range.default)
def sym_constrain_range(size, min=..., max=...): # -> None:
    ...

@register_meta(aten._functional_sym_constrain_range.default)
def functional_sym_constrain_range(size, min=..., max=..., dep_token=...): # -> None:
    ...

@register_meta(aten.sym_constrain_range_for_size.default)
def sym_constrain_range_for_size(size, min=..., max=...): # -> None:
    ...

@register_meta(aten._functional_sym_constrain_range_for_size.default)
def functional_sym_constrain_range_for_size(size, min, max, dep_token):
    ...

@register_meta(aten._functional_assert_async.msg)
def functional_assert_async_meta(val, assert_msg, dep_token):
    ...

def squareCheckInputs(self: Tensor, f_name: str): # -> None:
    ...

def linearSolveCheckInputs(self: Tensor, A: Tensor, name: str): # -> None:
    ...

def checkFloatingOrComplex(t: Tensor, f_name: str, allow_low_precision_dtypes: bool = ...): # -> None:
    ...

def checkIsMatrix(A: Tensor, f_name: str, arg_name: str = ...): # -> None:
    ...

def checkInputsSolver(A: Tensor, B: Tensor, left: bool, f_name: str): # -> None:
    ...

def checkSameDevice(fn_name: str, result: Tensor, input: Tensor, result_name: str = ...): # -> None:
    ...

def checkUplo(UPLO: str): # -> None:
    ...

@register_meta([aten._linalg_eigh.default, aten._linalg_eigh.eigenvalues])
@out_wrapper("eigenvalues", "eigenvectors")
def meta__linalg_eigh(A: Tensor, UPLO: str = ..., compute_v: bool = ...): # -> tuple[Any, Any]:
    ...

@register_meta([aten._linalg_eigvals.default, aten.linalg_eigvals.out])
@out_wrapper()
def meta__linalg_eigvals(input: Tensor) -> Tensor:
    ...

@register_meta([aten.linalg_eig])
@out_wrapper("eigenvalues", "eigenvectors")
def meta_linalg_eig(input: Tensor): # -> tuple[Any, Any]:
    ...

def cloneBatchedColumnMajor(src: Tensor) -> Tensor:
    ...

@register_meta(aten.cholesky_solve)
@out_wrapper()
def cholesky_solve(self: Tensor, A: Tensor, upper: bool = ...) -> Tensor:
    ...

@register_meta(aten.cholesky)
@out_wrapper()
def cholesky(self: Tensor, upper: bool = ...) -> Tensor:
    ...

@register_meta(aten.cholesky_inverse)
@out_wrapper()
def cholesky_inverse(self: Tensor, upper: bool = ...) -> Tensor:
    ...

@register_meta(aten.linalg_cholesky_ex.default)
def linalg_cholesky_ex(A: Tensor, upper: bool = ..., check_errors: bool = ...): # -> tuple[Any, Any]:
    ...

@register_meta([aten.linalg_householder_product.default, aten.linalg_householder_product.out])
@out_wrapper()
def linalg_householder_product(input: Tensor, tau: Tensor) -> Tensor:
    ...

@register_meta(aten.linalg_inv_ex.default)
def linalg_inv_ex_meta(A: Tensor, check_errors: bool = ...): # -> tuple[Any, Any]:
    ...

@register_meta([aten.linalg_ldl_factor_ex.default, aten.linalg_ldl_factor_ex.out])
@out_wrapper("LD", "pivots", "info")
def linalg_ldl_factor_ex_meta(self: Tensor, *, hermitian: bool = ..., check_errors: bool = ...) -> tuple[Tensor, Tensor, Tensor]:
    ...

@register_meta([aten.linalg_ldl_solve.default, aten.linalg_ldl_solve.out])
@out_wrapper()
def linalg_ldl_solve_meta(LD: Tensor, pivots: Tensor, B: Tensor, *, hermitian: bool = ...) -> Tensor:
    ...

@register_meta([aten.linalg_lu.default, aten.linalg_lu.out])
@out_wrapper("P", "L", "U")
def linalg_lu_meta(A: Tensor, *, pivot: bool = ...) -> tuple[Tensor, Tensor, Tensor]:
    ...

@register_meta([aten.linalg_lu_factor_ex.default, aten.linalg_lu_factor_ex.out])
@out_wrapper("LU", "pivots", "info")
def linalg_lu_factor_ex_meta(A: Tensor, *, pivot: bool = ..., check_errors: bool = ...) -> tuple[Tensor, Tensor, Tensor]:
    ...

@register_meta([aten.linalg_lu_solve.default, aten.linalg_lu_solve.out])
@out_wrapper()
def linalg_lu_solve_meta(LU: Tensor, pivots: Tensor, B: Tensor, *, left: bool = ..., adjoint: bool = ...) -> Tensor:
    ...

@register_meta(aten.lu_unpack)
@out_wrapper("P", "L", "U")
def lu_unpack_meta(LU: Tensor, pivots: Tensor, unpack_data: bool = ..., unpack_pivots: bool = ...) -> tuple[Tensor, Tensor, Tensor]:
    ...

@register_meta([aten.linalg_qr.default, aten.linalg_qr.out])
@out_wrapper("Q", "R")
def linalg_qr_meta(A: Tensor, mode: str = ...) -> tuple[Tensor, Tensor]:
    ...

def linalg_solve_is_vector_rhs(input: Tensor, other: Tensor) -> bool:
    ...

@register_meta([aten.linalg_solve_triangular.default, aten.linalg_solve_triangular.out])
def linalg_solve_triangular_meta(A: Tensor, B: Tensor, *, upper: bool, left: bool = ..., unitriangular: bool = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

@register_meta(aten.triangular_solve)
@out_wrapper("X", "M", exact_dtype=True)
def triangular_solve_meta(self: Tensor, A: Tensor, upper: bool = ..., transpose: bool = ..., unitriangular: bool = ...) -> tuple[Tensor, Tensor]:
    ...

@register_meta(aten.ormqr)
@out_wrapper()
def ormqr(input: Tensor, tau: Tensor, other: Tensor, left: bool = ..., transpose: bool = ...) -> Tensor:
    ...

@register_meta(aten.reflection_pad1d)
@out_wrapper()
def meta_reflection_pad1d(input, padding):
    ...

@register_meta(aten.replication_pad1d)
@out_wrapper()
def meta_replication_pad1d(input, padding):
    ...

@register_meta(aten.reflection_pad1d_backward)
@out_wrapper("grad_input")
def meta_reflection_pad1d_backward(grad_output, input, padding):
    ...

@register_meta(aten.replication_pad1d_backward)
@out_wrapper("grad_input")
def meta_replication_pad1d_backward(grad_output, input, padding):
    ...

@register_meta(aten.reflection_pad2d)
@out_wrapper()
def meta_reflection_pad2d(input, padding):
    ...

@register_meta(aten.replication_pad2d)
@out_wrapper()
def meta_replication_pad2d(input, padding):
    ...

@register_meta([aten.reflection_pad2d_backward.default, aten.reflection_pad2d_backward.grad_input, aten.replication_pad2d_backward.default, aten.replication_pad2d_backward.grad_input])
@out_wrapper("grad_input")
def meta_pad2d_backward(grad_output, self, padding):
    ...

@register_meta(aten.reflection_pad3d)
@out_wrapper()
def meta_reflection_pad3d(input, padding):
    ...

@register_meta(aten.replication_pad3d)
@out_wrapper()
def meta_replication_pad3d(input, padding):
    ...

@register_meta([aten.reflection_pad3d_backward.default, aten.reflection_pad3d_backward.grad_input, aten.replication_pad3d_backward.default, aten.replication_pad3d_backward.grad_input])
@out_wrapper("grad_input")
def meta_pad3d_backward(grad_output, input, padding):
    ...

@register_meta(aten._pdist_forward)
@out_wrapper()
def meta__pdist_forward(self: Tensor, p: float = ...) -> Tensor:
    ...

@register_meta(aten._pdist_backward)
@out_wrapper()
def meta__pdist_backward(grad: Tensor, self: Tensor, p: float, pdist: Tensor) -> Tensor:
    ...

@register_meta([aten.baddbmm.default, aten.baddbmm.out])
@out_wrapper(exact_dtype=True)
def meta_baddbmm(self, batch1, batch2, *, beta=..., alpha=...):
    ...

@register_meta([aten.bernoulli.default, aten.bernoulli.out])
@out_wrapper()
def meta_bernoulli(self, *, generator=...): # -> Tensor:
    ...

@register_meta(aten.bernoulli_.float)
def meta_bernoulli_(self, p=..., generator=...):
    ...

@register_meta(aten.bernoulli.p)
def meta_bernoulli_p(self, p=..., generator=...): # -> Tensor:
    ...

@register_meta([aten.poisson.default, aten.poisson.out])
@out_wrapper()
def meta_poisson(self, generator=...): # -> Tensor:
    ...

@register_meta(aten._fused_moving_avg_obs_fq_helper.default)
def meta__fused_moving_avg_obs_fq_helper(self, observer_on, fake_quant_on, running_min, running_max, scale, zero_point, averaging_const, quant_min, quant_max, ch_axis, per_row_fake_quant=..., symmetric_quant=...): # -> tuple[Tensor, Tensor]:
    ...

@register_meta(aten.mm)
@out_wrapper(exact_dtype=True)
def meta_mm(a, b):
    ...

def device_hint(tensor) -> str:
    ...

def calc_conv_nd_return_shape(input_tensor: torch.Tensor, weight: torch.Tensor, stride: Union[list[int], int], padding: Union[list[int], int], dilation: Union[list[int], int], is_transposed: bool, groups: int, output_padding: Optional[Union[list[int], int]] = ...): # -> list[int]:
    ...

def is_channels_last(ten):
    ...

@register_meta(aten.miopen_batch_norm.default)
def meta_miopen_batch_norm(input_tensor: torch.Tensor, weight: torch.Tensor, bias: Optional[torch.Tensor], running_mean: Optional[torch.Tensor], running_var: Optional[torch.Tensor], training: bool, exponential_average_factor: float, epsilon: float): # -> tuple[Any, Any, Any]:
    ...

@register_meta(aten.convolution.default)
def meta_conv(input_tensor: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, stride: list[int], padding: list[int], dilation: list[int], is_transposed: bool, output_padding: list[int], groups: int):
    ...

if torch._C._has_mkldnn:
    _meta_lib_dont_use_me_use_register_meta_for_mkldnn = ...
    @register_meta(torch.ops.mkldnn._convolution_pointwise.default)
    def meta_mkldnn_convolution_default(input_tensor, weight, bias, padding, stride, dilation, groups, attr, scalars, algorithm):
        ...
    
    @register_meta(torch.ops.mkldnn._linear_pointwise.default)
    def meta_linear_pointwise_default(input_tensor, weight, bias, attr, scalars, algorithm):
        ...
    
    _meta_lib_dont_use_me_use_register_meta_for_onednn = ...
    @register_meta(torch.ops.onednn.qconv2d_pointwise.default)
    @register_meta(torch.ops.onednn.qconv_pointwise.default)
    def meta_qconv_pointwise(x, x_scale, x_zp, w, w_scale, w_zp, bias, stride, padding, dilation, groups, output_scale, output_zero_point, output_dtype, attr, scalars, algorithm):
        ...
    
    @register_meta(torch.ops.onednn.qconv2d_pointwise.binary)
    def meta_qconv2d_pointwise_binary(x, x_scale, x_zp, w, w_scale, w_zp, accum, bias, stride, padding, dilation, groups, output_scale, output_zero_point, output_dtype, accum_scale, accum_zero_point, binary_op_name, alpha, unary_op_name, unary_op_args, unary_op_algorithm):
        ...
    
    @register_meta(torch.ops.onednn.qlinear_pointwise.default)
    @register_meta(torch.ops.onednn.qlinear_pointwise.tensor)
    def meta_qlinear_pointwise(x, x_scale, x_zp, w, w_scale, w_zp, bias, output_scale, output_zero_point, output_dtype, post_op_name, post_op_args, post_op_algorithm):
        ...
    
    @register_meta(torch.ops.onednn.qlinear_pointwise.binary)
    @register_meta(torch.ops.onednn.qlinear_pointwise.binary_tensor)
    def meta_qlinear_pointwise_binary(x, x_scale, x_zp, w, w_scale, w_zp, x_2, bias, output_scale, output_zero_point, output_dtype, x2_scale, x2_zp, binary_op_name, alpha, unary_op_name, unary_op_args, unary_op_algorithm):
        ...
    
    @register_meta(torch.ops.onednn.linear_dynamic_fp16.default)
    @register_meta(torch.ops.onednn.linear_relu_dynamic_fp16.default)
    def meta_linear_dynamic_fp16(x, w, bias):
        ...
    
    _meta_lib_dont_use_me_use_register_meta_for_quantized = ...
    @register_meta(torch.ops.quantized.max_pool2d)
    def meta_quantized_max_pool2d(input, kernel_size, stride=..., padding=..., dilation=..., ceil_mode=...): # -> Tensor:
        ...
    
    @register_meta(torch.ops.quantized.int4mm_packed_weight_cpu)
    def meta_int4mm_packed_weight_cpu(x, w, q_group_size, q_scale_and_zeros):
        ...
    
def check_dim_size(tensor, dim, dim_size, size): # -> None:
    ...

@register_meta(aten.avg_pool2d.default)
def meta_avg_pool2d(input, kernel_size, stride=..., padding=..., ceil_mode=..., count_include_pad=..., divisor_override=...): # -> Tensor:
    ...

def avg_pool2d_backward_shape_check(input, gradOutput, nbatch, kH, kW, dH, dW, padH, padW, nInputPlane, inputHeight, inputWidth, outputHeight, outputWidth, mem_format): # -> None:
    ...

@register_meta(aten.avg_pool2d_backward.default)
def meta_avg_pool2d_backward(gradOutput_, input, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override): # -> Tensor:
    ...

@register_meta(aten.avg_pool3d)
@out_wrapper()
def meta_avg_pool3d(input, kernel_size, stride=..., padding=..., ceil_mode=..., count_include_pad=..., divisor_override=...):
    ...

@register_meta(aten.avg_pool3d_backward)
@out_wrapper("grad_input")
def meta_avg_pool3d_backward(grad_output, input, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override):
    ...

@register_meta(aten._adaptive_avg_pool2d.default)
def meta_adaptive_avg_pool2d(self, output_size): # -> Tensor:
    ...

@register_meta(aten._adaptive_avg_pool3d.default)
def meta_adaptive_avg_pool3d(self, output_size):
    ...

@register_meta(aten._adaptive_avg_pool2d_backward.default)
def meta__adaptive_avg_pool2d_backward(grad_out, self):
    ...

@register_meta(aten._adaptive_avg_pool3d_backward)
@out_wrapper("grad_input")
def meta__adaptive_avg_pool3d_backward(grad_output, self): # -> Tensor:
    ...

@register_meta(aten.adaptive_max_pool2d)
@out_wrapper("out", "indices")
def meta_adaptive_max_pool2d(input, output_size): # -> tuple[Any, Any]:
    ...

@register_meta(aten.adaptive_max_pool2d_backward)
@out_wrapper("grad_input")
def meta_adaptive_max_pool2d_backward(grad_output, input, indices):
    ...

@register_meta(aten.adaptive_max_pool3d)
@out_wrapper("out", "indices")
def meta_adaptive_max_pool3d(input, output_size): # -> tuple[Any, Any]:
    ...

@register_meta(aten.adaptive_max_pool3d_backward)
@out_wrapper("grad_input")
def meta_adaptive_max_pool3d_backward(grad_output, input, indices):
    ...

@register_meta(aten.repeat_interleave.Tensor)
def meta_repeat_interleave_Tensor(repeats, output_size=...):
    ...

@register_meta([aten.complex.default, aten.complex.out])
@out_wrapper()
def meta_complex(real, imag):
    ...

@register_meta([aten.nonzero_static.default, aten.nonzero_static.out])
@out_wrapper()
def nonzero_static(self, *, size, fill_value: int = ...):
    ...

@register_meta([torch.ops.aten.nonzero.default, torch.ops.aten.nonzero.out])
@out_wrapper()
def nonzero(self): # -> Tensor:
    ...

@register_meta([aten.index.Tensor, aten._unsafe_index.Tensor])
def meta_index_Tensor(self, indices):
    ...

@register_meta([aten.convolution_backward.default])
def meta_convolution_backward(grad_output_, input_, weight_, bias_sizes_opt, stride, padding, dilation, transposed, output_padding, groups, output_mask): # -> tuple[Any | None, Any | None, Any | None]:
    ...

@register_meta([aten.addbmm.default, aten.addbmm.out])
@out_wrapper(exact_dtype=True)
def meta_addbmm(self, batch1, batch2, *, beta=..., alpha=...):
    ...

@register_meta([aten.randint_like.Tensor])
def meta_randint_like(self, high, **kwargs):
    ...

@register_meta([aten._fused_adam_.default, aten._fused_adamw_.default])
def meta__fused_adam_(self, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, *, lr, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale=..., found_inf=...): # -> None:
    ...

@register_meta([aten._fused_adam.default])
def meta__fused_adam(self, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, *, lr, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale=..., found_inf=...): # -> tuple[list[Tensor], list[Tensor], list[Tensor], list[Tensor], list[Tensor]]:
    ...

@register_meta([aten._int_mm])
@out_wrapper()
def meta__int_mm(a, b):
    ...

@register_meta([aten._convert_weight_to_int4pack])
def meta__convert_weight_to_int4pack(w, inner_k_tiles):
    ...

@register_meta([aten._convert_weight_to_int4pack_for_cpu])
def meta__convert_weight_to_int4pack_for_cpu(w, inner_k_tiles):
    ...

@register_meta([aten._weight_int4pack_mm])
def meta__weight_int4pack_mm(x, w, q_group_size, q_scale_and_zeros):
    ...

@register_meta([aten._weight_int4pack_mm_for_cpu])
def meta__weight_int4pack_mm_for_cpu(x, w, q_group_size, q_scale_and_zeros):
    ...

def kai_roundup(a: int, b: int) -> int:
    ...

def get_kai_packed_weight_size(n_bits, N, K, groupsize): # -> None:
    ...

@register_meta([aten._dyn_quant_pack_4bit_weight])
def meta__dyn_quant_pack_4bit_weight(weights, scales_zeros, bias: Optional[Tensor], block_size, in_features, out_features):
    ...

@register_meta([aten._dyn_quant_matmul_4bit])
def meta__dyn_quant_matmul_4bit(inp, packed_weights, block_size, in_features, out_features):
    ...

@register_meta([aten._weight_int8pack_mm])
def meta__weight_int8pack_mm(x, w, q_scales):
    ...

@register_meta(aten._cdist_forward.default)
def meta_cdist_forward(x1, x2, p, compute_mode):
    ...

@register_meta(aten._cdist_backward)
@out_wrapper()
def meta_cdist_backward(grad, x1, x2, p, cdist): # -> Tensor:
    ...

@register_meta(aten._embedding_bag.default)
def meta_embedding_bag(weight, indices, offsets, scale_grad_by_freq=..., mode=..., sparse=..., per_sample_weights=..., include_last_offset=..., padding_idx=...): # -> tuple[Any, Any, Any, Any]:
    ...

@register_meta(aten._embedding_bag_forward_only.default)
def meta_embedding_bag_forward_only(weight, indices, offsets, *args): # -> tuple[Any, Any, Any, Any]:
    ...

@register_meta([aten.nansum.default, aten.nansum.out])
@out_wrapper()
def meta_nansum(input, dims=..., keepdim=..., *, dtype=...):
    ...

@register_meta([aten.median.default, aten.nanmedian.default])
def meta_median(input):
    ...

@register_meta([aten.median.dim, aten.median.dim_values, aten.nanmedian.dim, aten.nanmedian.dim_values, aten.mode.default, aten.mode.values])
@out_wrapper("values", "indices")
def meta_median_mode_dim(input, dim=..., keepdim=...): # -> tuple[Any, Any]:
    ...

@register_meta(aten.logical_not_.default)
def meta_logical_not_(self):
    ...

@register_meta(aten.repeat.default)
def meta_repeat(self, repeats):
    ...

@register_meta(aten.zero_.default)
def meta_zero_(self):
    ...

@register_meta([aten.mul_.Scalar, aten.div_.Scalar, aten.mul_.Tensor, aten.div_.Tensor, aten.logical_and_.default, aten.logical_or_.default, aten.logical_xor_.default])
def meta_binop_inplace(self, other):
    ...

@register_meta([aten.add_.Scalar, aten.sub_.Scalar, aten.add_.Tensor, aten.sub_.Tensor])
def meta_binop_inplace_alpha(self, other, alpha=...):
    """
    Some checks for inplace ops.
    Checks for promotion rules for some dtypes.
    int.add/sub_(float) and bool.add/sub_(others) are rejected.
    Promoting in these in-place operations would require reallocating
    and copying over elements, hence not allowed.
    Checks for alpha param.
    """
    ...

@register_meta([aten.round.default, aten.round.decimals])
def meta_round(self, **kwargs): # -> FakeTensor:
    ...

def shift_dtype_check(fn_name, self, val): # -> None:
    ...

@register_meta([aten.__rshift__.Tensor, aten.__rshift__.Scalar])
def meta_rshifts(self, other): # -> FakeTensor:
    ...

@register_meta([aten.__lshift__.Tensor, aten.__lshift__.Scalar])
def meta_lshifts(self, other): # -> FakeTensor:
    ...

@register_meta(aten.zero.default)
def meta_zero(self):
    ...

@register_meta([aten.fill_.Tensor, aten.fill_.Scalar])
def meta_fill_(self, val):
    ...

@register_meta([aten.fill.Tensor, aten.fill.Scalar])
def meta_fill(self, val): # -> Tensor:
    ...

@register_meta(aten.relu_.default)
def meta_relu_(self):
    ...

@register_meta(aten._add_relu.Tensor)
@out_wrapper()
def meta__add_relu(self, other, alpha=...) -> Tensor:
    ...

@register_meta([aten.rrelu_with_noise])
@out_wrapper()
def meta_rrelu_with_noise(self, noise, lower=..., upper=..., training=..., generator=...): # -> Tensor:
    ...

@register_meta([aten.rrelu_with_noise_functional])
def meta_rrelu_with_noise_functional(self, noise, lower=..., upper=..., training=..., generator=...): # -> tuple[Tensor, Tensor]:
    ...

@register_meta([aten.rrelu_with_noise_])
def meta_rrelu_with_noise_(self, lower=..., upper=..., training=..., generator=...):
    ...

@register_meta([aten.index_put.default, aten._unsafe_index_put.default])
def meta_index_put(self, indices, values, accumulate=...): # -> Tensor:
    ...

@register_meta(aten.masked_fill_.Scalar)
def meta_masked_fill_(self, mask, value):
    ...

@register_meta(aten._masked_scale.default)
def meta__masked_scale(self, mask, scale):
    ...

@register_meta(aten.masked_scatter_)
def meta_masked_scatter_(self, mask, source):
    ...

@register_meta(aten.masked_scatter)
@out_wrapper()
def meta_masked_scatter(self, mask, source):
    ...

@register_meta(aten.masked_scatter_backward)
def meta_masked_scatter_backward(self, mask, sizes):
    ...

@register_meta(aten.index_put_.default)
def meta_index_put_(self, indices, values, accumulate=...):
    ...

@register_meta(aten.alias.default)
def meta_alias(self):
    ...

def common_meta_baddbmm_bmm(batch1, batch2, is_bmm, self_baddbmm=..., out_dtype=...):
    ...

@register_meta(aten.bmm.default)
def meta_bmm(self, mat2):
    ...

@register_meta(aten.bmm.dtype)
def meta_bmm_dtype(self, mat2, out_dtype):
    ...

def div_rtn(x, y):
    ...

def pooling_output_shape_pad_lr(inputSize, kernelSize, pad_l, pad_r, stride, dilation, ceil_mode):
    ...

def pooling_output_shape(inputSize, kernelSize, pad, stride, dilation, ceil_mode):
    ...

def pool2d_shape_check(input, kH, kW, dH, dW, padH, padW, dilationH, dilationW, nInputPlane, inputHeight, inputWidth, outputHeight, outputWidth, memory_format): # -> None:
    ...

def pool3d_shape_check(input: Tensor, nslices: int, kT: int, kH: int, kW: int, dT: int, dH: int, dW: int, pT: int, pH: int, pW: int, dilationT: int, dilationH: int, dilationW: int, itime: int, iheight: int, iwidth: int, otime: int, oheight: int, owidth: int, fn_name: str, check_input_size: bool = ...): # -> None:
    ...

def max_pool3d_backward_shape_check(input, grad_output, indices, nslices, kT, kH, kW, dT, dH, dW, pT, pH, pW, dilationT, dilationH, dilationW, itime, iheight, iwidth, otime, oheight, owidth, fn_name): # -> None:
    ...

def avg_pool3d_backward_shape_check(input: Tensor, grad_output: Tensor, nslices: int, kT: int, kH: int, kW: int, dT: int, dH: int, dW: int, pT: int, pH: int, pW: int, itime: int, iheight: int, iwidth: int, otime: int, oheight: int, owidth: int, fn_name: str): # -> None:
    ...

def max_pool2d_checks_and_compute_shape(input, kernel_size, stride, padding, dilation, ceil_mode): # -> tuple[Any, Any, Any]:
    ...

@register_meta(aten.max_pool2d_with_indices_backward.default)
def meta_max_pool2d_with_indices_backward(grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices): # -> Tensor:
    ...

@register_meta(aten.max_pool2d_with_indices.default)
def meta_max_pool2d_with_indices(input, kernel_size, stride=..., padding=..., dilation=..., ceil_mode=...): # -> tuple[Tensor, Tensor]:
    ...

@register_meta(aten.fractional_max_pool2d.default)
def meta_fractional_max_pool2d(self, kernel_size, output_size, random_samples): # -> tuple[Tensor, Tensor]:
    ...

@register_meta(aten.max_pool3d_with_indices)
@out_wrapper("out", "indices")
def meta_max_pool3d_with_indices(input, kernel_size, stride=..., padding=..., dilation=..., ceil_mode=...): # -> tuple[Any, Any]:
    ...

@register_meta(aten.max_pool3d_with_indices_backward)
@out_wrapper("grad_input")
def meta_max_pool3d_with_indices_backward(grad_output, input, kernel_size, stride, padding, dilation, ceil_mode, indices):
    ...

def check_grid_sampler_common(input: Tensor, grid: Tensor): # -> None:
    ...

class GridSamplerInterpolation(Enum):
    BILINEAR = ...
    NEAREST = ...
    BICUBIC = ...


def check_grid_sampler_3d(input: Tensor, grid: Tensor, interpolation_mode: int): # -> None:
    ...

@register_meta(aten.grid_sampler_2d_backward.default)
def grid_sampler_2d_backward_meta(grad_output, input, grid, interpolation_mode, padding_mode, align_corners, output_mask): # -> tuple[Tensor | None, Tensor]:
    ...

@register_meta(aten.grid_sampler_3d)
@out_wrapper()
def grid_sampler_3d(input, grid, interpolation_mode, padding_mode, align_corners):
    ...

@register_meta(aten.grid_sampler_3d_backward)
@out_wrapper("grad_input", "grad_grid")
def grid_sampler_3d_backward(grad_output, input, grid, interpolation_mode, padding_mode, align_corners, output_mask): # -> tuple[Tensor | None, Tensor]:
    ...

@register_meta([aten.full.default])
def full(size, fill_value, *args, **kwargs): # -> Tensor:
    ...

@register_meta(aten.zeros_like.default)
def zeros_like(self, dtype=..., layout=..., device=..., pin_memory=..., memory_format=...): # -> Tensor | Any:
    ...

@register_meta([aten.ones.default, aten.ones.out])
@out_wrapper()
def meta_ones(size, *, dtype=..., layout=..., device=..., pin_memory=..., requires_grad=...): # -> Tensor:
    ...

@register_meta([aten.zeros.default, aten.zeros.out])
@out_wrapper()
def meta_zeros(size, *, dtype=..., layout=..., device=..., pin_memory=..., requires_grad=...): # -> Tensor:
    ...

@register_meta(aten.select.int)
def meta_select(self, dim, index):
    ...

@register_meta(aten.select_scatter.default)
def meta_select_scatter(self, src, dim, index): # -> Tensor:
    ...

@register_meta(aten.slice_scatter.default)
def meta_slice_scatter(self, src, dim=..., start=..., end=..., step=...): # -> Tensor:
    ...

def maybe_wrap_dim(dim: int, dim_post_expr: int, wrap_scalar: bool = ...): # -> int:
    ...

def ensure_nonempty_size(t, dim): # -> Literal[1]:
    ...

def gather_shape_check(self, dim, index): # -> None:
    ...

@register_meta(aten.gather.default)
def meta_gather(self, dim, index, sparse_grad=...):
    ...

def get_operator_enum(reduce_, use_new_options=...): # -> Literal['REDUCE_ADD', 'REDUCE_MULTIPLY', 'REDUCE_MEAN', 'REDUCE_MAXIMUM', 'REDUCE_MINIMUM'] | None:
    ...

def scatter_gather_dtype_check(method_name, self, index, src_opt=...): # -> None:
    ...

def ensure_nonempty_dim(dim): # -> int:
    ...

def scatter_shape_check(self, dim, index, src_opt=...): # -> None:
    ...

def scatter_meta_impl(self, dim, index, src=..., reduce_=..., use_new_options=...): # -> None:
    ...

@register_meta(aten.scatter_add.default)
def meta_scatter_add(self, dim, index, src):
    ...

@register_meta(aten.scatter_add_)
def meta_scatter_add_(self, dim, index, src):
    ...

@register_meta([aten.scatter.src, aten.scatter.value, aten.scatter.reduce, aten.scatter.value_reduce])
@out_wrapper()
def meta_scatter(self, dim, index, src_or_value, reduce=...):
    ...

@register_meta([aten.scatter_.src, aten.scatter_.value, aten.scatter_.reduce, aten.scatter_.value_reduce])
def meta_scatter_(self, dim, index, src_or_value, reduce=...):
    ...

@register_meta([aten._scaled_dot_product_flash_attention])
def meta__scaled_dot_product_flash_attention(query: Tensor, key: Tensor, value: Tensor, dropout_p: float = ..., is_causal: bool = ..., return_debug_mask: bool = ..., scale: Optional[float] = ...): # -> tuple[Any, Tensor, None, None, Any, Any, Tensor, Tensor, Tensor]:
    ...

def alloc_with_matching_layout(query: Tensor, res_shape: tuple[int, ...]):
    ...

@register_meta([aten._scaled_dot_product_cudnn_attention])
def meta__scaled_dot_product_cudnn_attention(query: Tensor, key: Tensor, value: Tensor, attn_bias: Optional[Tensor], compute_log_sumexp: bool, dropout_p: float = ..., is_causal: bool = ..., return_debug_mask: bool = ..., scale: Optional[float] = ...): # -> tuple[Any, Tensor, None, None, Any, Any, Tensor, Tensor, None]:
    ...

@register_meta([aten._scaled_dot_product_fused_attention_overrideable])
def meta__scaled_dot_product_fused_attention_overrideable(query: Tensor, key: Tensor, value: Tensor, attn_bias: Optional[Tensor] = ..., dropout_p: float = ..., is_causal: bool = ..., return_debug_mask: bool = ..., scale: Optional[float] = ...): # -> tuple[Any, Tensor, None, None, Any, Any, Tensor, Tensor, None]:
    ...

@register_meta([aten._scaled_dot_product_flash_attention_backward])
def meta__scaled_dot_product_flash_backward(grad_out: Tensor, query: Tensor, key: Tensor, value: Tensor, out: Tensor, logsumexp: Tensor, cum_seq_q: Tensor, cum_seq_k: Tensor, max_q: int, max_k: int, dropout_p: float, is_causal: bool, philox_seed: Tensor, philox_offset: Tensor, scale: Optional[float] = ...): # -> tuple[Any, Any, Any]:
    ...

@register_meta([aten._scaled_dot_product_flash_attention_for_cpu])
def meta__scaled_dot_product_flash_attention_for_cpu(query: Tensor, key: Tensor, value: Tensor, dropout_p: float = ..., is_causal: bool = ..., attn_mask: Optional[Tensor] = ..., scale: Optional[float] = ...): # -> tuple[Tensor, Any]:
    ...

@register_meta([aten._scaled_dot_product_flash_attention_for_cpu_backward])
def meta__scaled_dot_product_flash_attention_for_cpu_backward(grad_out: Tensor, query: Tensor, key: Tensor, value: Tensor, out: Tensor, logsumexp: Tensor, dropout_p: float, is_causal: bool, attn_mask: Optional[Tensor] = ..., scale: Optional[float] = ...): # -> tuple[Tensor, Tensor, Tensor]:
    ...

@register_meta([aten._scaled_dot_product_efficient_attention])
def meta__scaled_dot_product_efficient_attention(query: Tensor, key: Tensor, value: Tensor, attn_bias: Optional[Tensor], compute_log_sumexp: bool, dropout_p=..., is_causal: bool = ..., scale: Optional[float] = ...): # -> tuple[Any, Tensor, Tensor, Tensor]:
    ...

@register_meta([aten._scaled_dot_product_efficient_attention_backward])
def meta__scaled_dot_product_efficient_backward(grad_out: Tensor, query: Tensor, key: Tensor, value: Tensor, attn_bias: Optional[Tensor], out: Tensor, logsumexp: Tensor, philox_seed: Tensor, philox_offset: Tensor, dropout_p: float, grad_input_mask: list[bool], is_causal: bool = ..., scale: Optional[float] = ...): # -> tuple[Tensor, Tensor, Tensor, Any | None]:
    ...

@register_meta([aten._scaled_dot_product_cudnn_attention_backward])
def meta__scaled_dot_product_cudnn_backward(grad_out: Tensor, query: Tensor, key: Tensor, value: Tensor, out: Tensor, logsumexp: Tensor, philox_seed: Tensor, philox_offset: Tensor, attn_bias: Tensor, cum_seq_q: Tensor, cum_seq_k: Tensor, max_q: int, max_k: int, dropout_p: float, is_causal: bool, scale: Optional[float] = ...): # -> tuple[Tensor, Tensor, Tensor]:
    ...

@register_meta([aten._flash_attention_forward])
def meta__flash_attention_forward(query: Tensor, key: Tensor, value: Tensor, cum_seq_q: Optional[Tensor], cum_seq_k: Optional[Tensor], max_q: int, max_k: int, dropout_p: float, is_causal: bool, return_debug_mask: bool, scale: Optional[float] = ..., window_size_left: Optional[int] = ..., window_size_right: Optional[int] = ..., seqused_k: Optional[Tensor] = ..., alibi_slopes: Optional[Tensor] = ...): # -> tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:
    ...

@register_meta([aten._flash_attention_backward])
def meta__flash_attention_backward(grad_out: Tensor, query: Tensor, key: Tensor, value: Tensor, out: Tensor, logsumexp: Tensor, cum_seq_q: Tensor, cum_seq_k: Tensor, max_q: int, max_k: int, dropout_p: float, is_causal: bool, philox_seed: Tensor, philox_offset: Tensor, scale: Optional[float] = ..., window_size_left: Optional[int] = ..., window_size_right: Optional[int] = ...): # -> tuple[Tensor, Tensor, Tensor]:
    ...

@register_meta([aten._efficient_attention_forward])
def meta__efficient_attention_forward(query: Tensor, key: Tensor, value: Tensor, bias: Optional[Tensor], cu_seqlens_q: Optional[Tensor], cu_seqlens_k: Optional[Tensor], max_seqlen_q: Optional[int], max_seqlen_k: Optional[int], dropout_p: float, custom_mask_type: int, compute_log_sumexp: bool = ..., scale: Optional[float] = ..., causal_diagonal: Optional[Tensor] = ..., seqlen_k: Optional[Tensor] = ..., window_size: Optional[int] = ...): # -> tuple[Tensor, Tensor, Tensor, Tensor, int | Any, int | Any]:
    ...

@register_meta([aten._efficient_attention_backward])
def meta__efficient_attention_backward(grad_out: Tensor, query: Tensor, key: Tensor, value: Tensor, bias: Optional[Tensor], cu_seqlens_q: Optional[Tensor], cu_seqlens_k: Optional[Tensor], max_seqlen_q: torch.SymInt, max_seqlen_k: torch.SymInt, logsumexp: Tensor, dropout_p: float, philox_seed: Tensor, philox_offset: Tensor, custom_mask_type: int, bias_requires_grad: bool, scale: Optional[float] = ..., num_splits_key: Optional[int] = ..., shared_storage_dqdkdv: bool = ...): # -> tuple[Any | Tensor, Any | Tensor, Any | Tensor, Any | Tensor]:
    ...

@register_meta([aten._scaled_mm.default])
def meta_scaled_mm(self: torch.Tensor, mat2: torch.Tensor, scale_a: torch.Tensor, scale_b: torch.Tensor, bias: Optional[torch.Tensor] = ..., scale_result: Optional[torch.Tensor] = ..., out_dtype: Optional[torch.dtype] = ..., use_fast_accum: bool = ...): # -> Tensor:
    ...

@register_meta([aten.scatter_reduce.two, aten.scatter_reduce.two_out])
@out_wrapper()
def meta_scatter_reduce_two(self, dim, index, src, reduce, include_self=...):
    ...

@register_meta(aten.scatter_reduce_.two)
def meta_scatter_reduce__two(self, dim, index, src, reduce, include_self=...):
    ...

@register_meta([aten.multinomial.default, aten.multinomial.out])
@out_wrapper()
def meta_multinomial(input, num_samples, replacement=..., *, generator=...): # -> Tensor:
    ...

def multiply_integers(vs): # -> Literal[1]:
    ...

def upsample_common_check(input_size, output_size, num_spatial_dims): # -> tuple[Any, Any, *tuple[Any, ...]]:
    ...

@register_meta([aten.upsample_nearest1d.default, aten._upsample_nearest_exact1d.default])
def upsample_nearest1d(input, output_size, scales=...):
    ...

@register_meta([aten.upsample_nearest2d.default, aten._upsample_nearest_exact2d.default])
def upsample_nearest2d(input, output_size, scales_h=..., scales_w=...):
    ...

@register_meta([aten.upsample_nearest2d_backward.default, aten._upsample_nearest_exact2d_backward.default])
def upsample_nearest2d_backward(grad_output: Tensor, output_size: Sequence[Union[int, torch.SymInt]], input_size: Sequence[Union[int, torch.SymInt]], scales_h: Optional[float] = ..., scales_w: Optional[float] = ...):
    ...

@register_meta([aten.upsample_nearest3d.default, aten._upsample_nearest_exact3d.default])
def upsample_nearest3d(input, output_size, scales_d=..., scales_h=..., scales_w=...):
    ...

@register_meta([aten.sort.default, aten.sort.stable, aten.sort.values, aten.sort.values_stable])
def meta_sort(self, stable=..., dim=..., descending=..., values=..., indices=...): # -> tuple[Any | TensorLikeType, Any | TensorLikeType] | tuple[Tensor, Tensor]:
    ...

def rnn_cell_checkSizes(input_gates, hidden_gates, input_bias, hidden_bias, factor, prev_hidden): # -> None:
    ...

@register_meta(aten.mkldnn_rnn_layer.default)
def mkldnn_rnn_layer(input, w0, w1, w2, w3, hx_, cx_, reverse, batch_sizes, mode, hidden_size, num_layers, has_biases, bidirectional, batch_first, train): # -> tuple[Any, Tensor | Any, Tensor | Any, Tensor]:
    ...

def zero_numel_check_dims(self, dim, fn_name): # -> None:
    ...

def check_argmax_argmin(name, self, dim): # -> None:
    ...

@register_meta([aten.argmax.default, aten.argmin.default])
def argmax_argmin_meta(self, dim=..., keepdim=...):
    ...

@register_meta(aten.scalar_tensor.default)
def scalar_tensor(s, dtype=..., layout=..., device=..., pin_memory=...): # -> Tensor:
    ...

@register_meta(aten.topk.default)
def topk_meta(self, k, dim=..., largest=..., sorted=...): # -> tuple[Any, Any]:
    ...

@register_meta(aten._segment_reduce_backward)
@out_wrapper()
def meta__segment_reduce_backward(grad, output, data, reduce, lengths=..., offsets=..., axis=..., initial=...): # -> Tensor:
    ...

@register_meta([aten.kthvalue.default, aten.kthvalue.values])
@out_wrapper("values", "indices")
def kthvalue_meta(self, k, dim=..., keepdim=...): # -> tuple[Any, Any]:
    ...

legacy_contiguous_memory_format = ...
def checkLSTMBackwardSizes(grad_hy, grad_cy, cx, cy, workspace): # -> None:
    ...

@register_meta(aten.linear_backward.default)
def linear_backward(input_, grad_output_, weight_, output_mask): # -> tuple[Any | None, Any | None, Any | None]:
    ...

@register_meta(aten.pixel_shuffle.default)
def meta_pixel_shuffle(self, upscale_factor):
    ...

@register_meta(aten.mkldnn_rnn_layer_backward.default)
def mkldnn_rnn_layer_backward(input, weight0, weight1, weight2, weight3, hx_, cx_tmp, output, hy_, cy_, grad_output_r_opt, grad_hy_r_opt, grad_cy_r_opt, reverse, mode, hidden_size, num_layers, has_biases, train, bidirectional, batch_sizes, batch_first, workspace): # -> tuple[Any, Any, Any, Any, Any, Any, Any]:
    ...

@register_meta([aten.bucketize.Tensor, aten.bucketize.Tensor_out])
@out_wrapper()
def meta_bucketize(self, boundaries, *, out_int32=..., right=...): # -> Tensor:
    ...

@register_meta([aten.histc])
@out_wrapper()
def meta_histc(input, bins=..., min=..., max=...): # -> Tensor:
    ...

@register_meta([aten._upsample_bilinear2d_aa.default, aten._upsample_bicubic2d_aa.default])
def meta_upsample_bimode2d_aa(input, output_size, align_corners, scales_h=..., scales_w=...):
    ...

@register_meta([aten._upsample_bilinear2d_aa_backward.default])
def meta_upsample_bimode2d_aa_backward(grad_output, output_size, input_size, align_corners, scales_h=..., scales_w=...):
    ...

@register_meta([aten.nan_to_num.default, aten.nan_to_num.out])
@out_wrapper()
def nan_to_num(self, nan=..., posinf=..., neginf=...):
    ...

@register_meta(torch.ops.aten.transpose_)
def transpose_(self, dim0, dim1):
    ...

@register_meta(torch.ops.aten.t_)
def t_(self):
    ...

@register_meta(aten.searchsorted)
@out_wrapper()
def meta_searchsorted(sorted_sequence, self, *, out_int32=..., right=..., side=..., sorter=...): # -> Tensor:
    ...

@register_meta(aten.embedding_dense_backward)
def meta_embedding_dense_backward(grad_output, indices, num_weights, padding_idx, scale_grad_by_freq):
    ...

@register_meta(aten._embedding_bag_backward)
def meta_embedding_bag_backward(grad, indices, offsets, offset2bag, bag_size, maximum_indices, num_weights, scale_grad_by_freq, mode, sparse, per_sample_weights, padding_idx=...): # -> Any:
    ...

@register_meta(aten._embedding_bag_dense_backward)
def meta_embedding_bag_dense_backward(grad, indices, offset2bag, bag_size, maximum_indices, num_weights, scale_grad_by_freq, mode, per_sample_weights, padding_idx=...):
    ...

@register_meta(aten._embedding_bag_per_sample_weights_backward)
def meta_embedding_bag_per_sample_weights_backward(grad, weight, indices, offsets, offset2bag, mode, padding_idx=...):
    ...

@register_meta(aten.isin)
@out_wrapper()
def meta_isin(elements, test_elements, *, assume_unique=..., invert=...): # -> Tensor:
    ...

@register_meta(aten.polygamma)
@out_wrapper()
def meta_polygamma(n: int, self: Tensor) -> Tensor:
    ...

@register_meta(aten._local_scalar_dense)
def meta_local_scalar_dense(self: Tensor):
    ...

@register_meta(aten.silu)
@out_wrapper(exact_dtype=True)
def silu(self: Tensor) -> Tensor:
    ...

@register_meta(aten.sigmoid)
@out_wrapper()
def sigmoid(self: Tensor) -> Tensor:
    ...

@register_meta(aten._grouped_mm)
@out_wrapper()
def grouped_mm(mat_a: Tensor, mat_b: Tensor, offs: Optional[Tensor] = ..., bias: Optional[Tensor] = ..., out_dtype: Optional[torch.dtype] = ...) -> Tensor:
    ...

@register_meta([aten._scaled_grouped_mm.default])
def meta_scaled_grouped_mm(mat_a: torch.Tensor, mat_b: torch.Tensor, scale_a: torch.Tensor, scale_b: torch.Tensor, offs: Optional[torch.Tensor] = ..., bias: Optional[torch.Tensor] = ..., scale_result: Optional[torch.Tensor] = ..., out_dtype: Optional[torch.dtype] = ..., use_fast_accum: bool = ...): # -> Tensor:
    ...

@register_meta(aten._softmax)
@out_wrapper()
def softmax(x: Tensor, dim: int, half_to_float: bool) -> Tensor:
    ...

@register_meta(aten.embedding)
@out_wrapper()
def embedding(weight: Tensor, indices: Tensor, padding_idx: int = ..., scale_grad_by_freq: bool = ..., sparse: bool = ...) -> Tensor:
    ...

@register_meta(aten._jagged_to_padded_dense_forward.default)
def meta__jagged_to_padded_dense_forward(values: Tensor, offsets: list[Tensor], max_lengths: list[int], padding_value: float = ...):
    ...

@register_meta(aten.lerp)
@out_wrapper()
def lerp(start, end, weight): # -> FakeTensor:
    ...

@register_meta(aten.addcmul)
@out_wrapper()
def addcmul(input, tensor1, tensor2, *, value=...): # -> FakeTensor:
    ...

@register_meta(aten.addcdiv)
@out_wrapper()
def addcdiv(input, tensor1, tensor2, *, value=...): # -> FakeTensor:
    ...

lerp_ = ...
addcmul_ = ...
addcdiv_ = ...
def activate_meta(): # -> None:
    ...

