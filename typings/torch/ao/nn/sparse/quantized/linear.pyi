"""
This type stub file was generated by pyright.
"""

import torch
from typing import Optional

__all__ = ["LinearPackedParams", "Linear"]
class LinearPackedParams(torch.nn.Module):
    _version = ...
    def __init__(self, row_block_size=..., col_block_size=..., dtype=...) -> None:
        ...
    
    @torch.jit.export
    def set_weight_bias(self, weight: torch.Tensor, bias: Optional[torch.Tensor], row_block_size: Optional[int], col_block_size: Optional[int]) -> None:
        ...
    
    def forward(self, x):
        ...
    
    @torch.jit.export
    def __getstate__(self): # -> tuple[Any, bool, Any]:
        ...
    
    @torch.jit.export
    def __setstate__(self, state): # -> None:
        ...
    
    def __repr__(self): # -> str:
        ...
    


class Linear(torch.nn.Module):
    r"""
    A quantized sparse linear module with quantized tensor as inputs and outputs.
    """
    _version = ...
    _FLOAT_MODULE = torch.nn.Linear
    def __init__(self, in_features, out_features, row_block_size, col_block_size, bias=..., dtype=...) -> None:
        ...
    
    def extra_repr(self): # -> str:
        ...
    
    def __repr__(self): # -> str:
        ...
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        ...
    
    def weight(self): # -> Any:
        ...
    
    def bias(self): # -> Any:
        ...
    
    def set_weight_bias(self, w: torch.Tensor, b: Optional[torch.Tensor], row_block_size: Optional[int], col_block_size: Optional[int]) -> None:
        ...
    
    @classmethod
    def from_float(cls, mod, use_precomputed_fake_quant=...): # -> Self:
        r"""Create a quantized sparse module from a float module.

        We only care about the convert at this stage, no need for observers just yet.

        TODO(zaf): Need to add the sparse params to the qconfig
        """
        ...
    


