"""
This type stub file was generated by pyright.
"""

import torch
from typing import Optional
from torch import Tensor

"""
We will recreate all the RNN modules as we require the modules to be decomposed
into its building blocks to be able to observe.
"""
__all__ = ["LSTMCell", "LSTM"]
class LSTMCell(torch.nn.Module):
    r"""A quantizable long short-term memory (LSTM) cell.

    For the description and the argument types, please, refer to :class:`~torch.nn.LSTMCell`

    `split_gates`: specify True to compute the input/forget/cell/output gates separately
    to avoid an intermediate tensor which is subsequently chunk'd. This optimization can
    be beneficial for on-device inference latency. This flag is cascaded down from the
    parent classes.

    Examples::

        >>> import torch.ao.nn.quantizable as nnqa
        >>> rnn = nnqa.LSTMCell(10, 20)
        >>> input = torch.randn(6, 10)
        >>> hx = torch.randn(3, 20)
        >>> cx = torch.randn(3, 20)
        >>> output = []
        >>> for i in range(6):
        ...     hx, cx = rnn(input[i], (hx, cx))
        ...     output.append(hx)
    """
    _FLOAT_MODULE = torch.nn.LSTMCell
    __constants__ = ...
    def __init__(self, input_dim: int, hidden_dim: int, bias: bool = ..., device=..., dtype=..., *, split_gates=...) -> None:
        ...
    
    def forward(self, x: Tensor, hidden: Optional[tuple[Tensor, Tensor]] = ...) -> tuple[Tensor, Tensor]:
        ...
    
    def initialize_hidden(self, batch_size: int, is_quantized: bool = ...) -> tuple[Tensor, Tensor]:
        ...
    
    @classmethod
    def from_params(cls, wi, wh, bi=..., bh=..., split_gates=...): # -> Self:
        """Uses the weights and biases to create a new LSTM cell.

        Args:
            wi, wh: Weights for the input and hidden layers
            bi, bh: Biases for the input and hidden layers
        """
        ...
    
    @classmethod
    def from_float(cls, other, use_precomputed_fake_quant=..., split_gates=...): # -> Self:
        ...
    


class _LSTMSingleLayer(torch.nn.Module):
    r"""A single one-directional LSTM layer.

    The difference between a layer and a cell is that the layer can process a
    sequence, while the cell only expects an instantaneous value.
    """
    def __init__(self, input_dim: int, hidden_dim: int, bias: bool = ..., device=..., dtype=..., *, split_gates=...) -> None:
        ...
    
    def forward(self, x: Tensor, hidden: Optional[tuple[Tensor, Tensor]] = ...): # -> tuple[Tensor, tuple[Tensor, Tensor] | None]:
        ...
    
    @classmethod
    def from_params(cls, *args, **kwargs): # -> Self:
        ...
    


class _LSTMLayer(torch.nn.Module):
    r"""A single bi-directional LSTM layer."""
    def __init__(self, input_dim: int, hidden_dim: int, bias: bool = ..., batch_first: bool = ..., bidirectional: bool = ..., device=..., dtype=..., *, split_gates=...) -> None:
        ...
    
    def forward(self, x: Tensor, hidden: Optional[tuple[Tensor, Tensor]] = ...): # -> tuple[Tensor | Any, tuple[Any | Tensor | None, Any | Tensor | None]]:
        ...
    
    @classmethod
    def from_float(cls, other, layer_idx=..., qconfig=..., **kwargs): # -> Self:
        r"""
        There is no FP equivalent of this class. This function is here just to
        mimic the behavior of the `prepare` within the `torch.ao.quantization`
        flow.
        """
        ...
    


class LSTM(torch.nn.Module):
    r"""A quantizable long short-term memory (LSTM).

    For the description and the argument types, please, refer to :class:`~torch.nn.LSTM`

    Attributes:
        layers : instances of the `_LSTMLayer`

    .. note::
        To access the weights and biases, you need to access them per layer.
        See examples below.

    Examples::

        >>> import torch.ao.nn.quantizable as nnqa
        >>> rnn = nnqa.LSTM(10, 20, 2)
        >>> input = torch.randn(5, 3, 10)
        >>> h0 = torch.randn(2, 3, 20)
        >>> c0 = torch.randn(2, 3, 20)
        >>> output, (hn, cn) = rnn(input, (h0, c0))
        >>> # To get the weights:
        >>> # xdoctest: +SKIP
        >>> print(rnn.layers[0].weight_ih)
        tensor([[...]])
        >>> print(rnn.layers[0].weight_hh)
        AssertionError: There is no reverse path in the non-bidirectional layer
    """
    _FLOAT_MODULE = torch.nn.LSTM
    def __init__(self, input_size: int, hidden_size: int, num_layers: int = ..., bias: bool = ..., batch_first: bool = ..., dropout: float = ..., bidirectional: bool = ..., device=..., dtype=..., *, split_gates: bool = ...) -> None:
        ...
    
    def forward(self, x: Tensor, hidden: Optional[tuple[Tensor, Tensor]] = ...): # -> tuple[Any | Tensor, tuple[Any, Any]]:
        ...
    
    @classmethod
    def from_float(cls, other, qconfig=..., split_gates=...):
        ...
    
    @classmethod
    def from_observed(cls, other):
        ...
    


