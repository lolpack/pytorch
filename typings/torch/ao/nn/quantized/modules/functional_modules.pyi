"""
This type stub file was generated by pyright.
"""

import torch
from torch import Tensor

__all__ = ["FloatFunctional", "FXFloatFunctional", "QFunctional"]
class FloatFunctional(torch.nn.Module):
    r"""State collector class for float operations.

    The instance of this class can be used instead of the ``torch.`` prefix for
    some operations. See example usage below.

    .. note::

        This class does not provide a ``forward`` hook. Instead, you must use
        one of the underlying functions (e.g. ``add``).

    Examples::

        >>> f_add = FloatFunctional()
        >>> a = torch.tensor(3.0)
        >>> b = torch.tensor(4.0)
        >>> f_add.add(a, b)  # Equivalent to ``torch.add(a, b)``

    Valid operation names:
        - add
        - cat
        - mul
        - add_relu
        - add_scalar
        - mul_scalar
    """
    def __init__(self) -> None:
        ...
    
    def forward(self, x):
        ...
    
    def add(self, x: Tensor, y: Tensor) -> Tensor:
        ...
    
    def add_scalar(self, x: Tensor, y: float) -> Tensor:
        ...
    
    def mul(self, x: Tensor, y: Tensor) -> Tensor:
        ...
    
    def mul_scalar(self, x: Tensor, y: float) -> Tensor:
        ...
    
    def cat(self, x: list[Tensor], dim: int = ...) -> Tensor:
        ...
    
    def add_relu(self, x: Tensor, y: Tensor) -> Tensor:
        ...
    
    def matmul(self, x: Tensor, y: Tensor) -> Tensor:
        ...
    


class FXFloatFunctional(torch.nn.Module):
    r"""module to replace FloatFunctional module before FX graph mode quantization,
    since activation_post_process will be inserted in top level module directly

    Valid operation names:
        - add
        - cat
        - mul
        - add_relu
        - add_scalar
        - mul_scalar
    """
    def forward(self, x):
        ...
    
    def add(self, x: Tensor, y: Tensor) -> Tensor:
        ...
    
    def add_scalar(self, x: Tensor, y: float) -> Tensor:
        ...
    
    def mul(self, x: Tensor, y: Tensor) -> Tensor:
        ...
    
    def mul_scalar(self, x: Tensor, y: float) -> Tensor:
        ...
    
    def cat(self, x: list[Tensor], dim: int = ...) -> Tensor:
        ...
    
    def add_relu(self, x: Tensor, y: Tensor) -> Tensor:
        ...
    
    def matmul(self, x: Tensor, y: Tensor) -> Tensor:
        ...
    


class QFunctional(torch.nn.Module):
    r"""Wrapper class for quantized operations.

    The instance of this class can be used instead of the
    ``torch.ops.quantized`` prefix. See example usage below.

    .. note::

        This class does not provide a ``forward`` hook. Instead, you must use
        one of the underlying functions (e.g. ``add``).

    Examples::

        >>> q_add = QFunctional()
        >>> # xdoctest: +SKIP
        >>> a = torch.quantize_per_tensor(torch.tensor(3.0), 1.0, 0, torch.qint32)
        >>> b = torch.quantize_per_tensor(torch.tensor(4.0), 1.0, 0, torch.qint32)
        >>> q_add.add(a, b)  # Equivalent to ``torch.ops.quantized.add(a, b, 1.0, 0)``

    Valid operation names:
        - add
        - cat
        - mul
        - add_relu
        - add_scalar
        - mul_scalar
    """
    def __init__(self) -> None:
        ...
    
    def extra_repr(self): # -> str:
        ...
    
    def forward(self, x):
        ...
    
    def add(self, x: Tensor, y: Tensor) -> Tensor:
        ...
    
    def add_scalar(self, x: Tensor, y: float) -> Tensor:
        ...
    
    def mul(self, x: Tensor, y: Tensor) -> Tensor:
        ...
    
    def mul_scalar(self, x: Tensor, y: float) -> Tensor:
        ...
    
    def cat(self, x: list[Tensor], dim: int = ...) -> Tensor:
        ...
    
    def add_relu(self, x: Tensor, y: Tensor) -> Tensor:
        ...
    
    def matmul(self, x: Tensor, y: Tensor) -> Tensor:
        ...
    
    @classmethod
    def from_float(cls, mod, use_precomputed_fake_quant=...): # -> QFunctional:
        ...
    


