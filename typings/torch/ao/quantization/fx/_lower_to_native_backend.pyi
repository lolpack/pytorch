"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn
from typing import Any, Callable, Optional, Union
from torch.ao.nn.quantized.modules.utils import WeightedQuantizedModule
from torch.ao.quantization.qconfig import QConfigAny
from torch.fx import GraphModule, Node

QOP_TO_ARG_NAMES_TO_SKIP: dict[Callable[..., Any], list[str]] = ...
def is_fixed_qparams_node(node, modules): # -> tuple[Any | bool, Any | bool, Any | bool]:
    ...

def is_default_node(node, modules): # -> tuple[Any | bool, Any | bool, Any | bool]:
    ...

def is_copy_node(node, modules): # -> tuple[Any | bool, Any | bool, Any | bool]:
    ...

def is_general_tensor_shape_node(node, modules): # -> tuple[Any | bool, Any | bool, Any | bool]:
    ...

def is_other_node(node, modules): # -> tuple[Any | bool, Any | bool, Any | bool]:
    ...

def is_special_pattern_node(node, modules): # -> tuple[Any | bool, Any | bool, Any | bool]:
    ...

def is_dequantize_node(node): # -> bool:
    ...

def is_getattr_tensor_metadata_node(node): # -> bool:
    ...

def is_get_tensor_info_node(node): # -> bool:
    ...

def should_skip_lowering(op: torch.fx.node.Node, qconfig_map: dict[str, QConfigAny]): # -> bool:
    """
    Return True if the op is configured with a None qconfig, False otherwise.
    Note: maybe need to generalize this to also check for the dtype, and we
    only lower when dtype matches, but right now fbgemm/qnnpack only support
    a single dtype, so it is OK for now.
    """
    ...

STATIC_LOWER_MODULE_MAP: dict[type[nn.Module], type[WeightedQuantizedModule]] = ...
DYNAMIC_LOWER_MODULE_MAP: dict[type[nn.Module], type[nn.Module]] = ...
WEIGHT_ONLY_LOWER_MODULE_MAP: dict[type[nn.Module], type[nn.Module]] = ...
SPECIAL_PATTERN_LOWER_MODULE_MAP = ...
STATIC_LOWER_FUSED_MODULE_MAP: dict[type[nn.Module], tuple[type[nn.Module], type[WeightedQuantizedModule]]] = ...
STATIC_LOWER_FUSED_MODULE_TWO_INPUTS_MAP: dict[type[nn.Module], tuple[type[nn.Module], type[WeightedQuantizedModule]]] = ...
DYNAMIC_LOWER_FUSED_MODULE_MAP: dict[type[nn.Module], tuple[type[nn.Module], type[nn.Module]]] = ...
STATIC_LOWER_FUNCTIONAL_MAP: dict[Callable, tuple[Callable, Optional[Callable]]] = ...
WEIGHT_PREPACK_OPS: set[Callable] = ...
DYNAMIC_LOWER_FUNCTIONAL_MAP: dict[Callable, dict[tuple[torch.dtype, torch.dtype], tuple[Callable, Optional[Callable]]]] = ...
CONV_FUNCTIONAL_OPS: set[Callable] = ...
CONV_TRANSPOSE_FUNCTIONAL_OPS: set[Callable] = ...
QBIN_OP_MAPPING: dict[Union[Callable, str], Callable] = ...
QBIN_RELU_OP_MAPPING: dict[Union[Callable, str], Callable] = ...
ORIGINAL_WEIGHTS_LOOKUP = ...
def fold_weight(quantized_model: GraphModule, node_name_to_scope: dict[str, tuple[str, type]], keep_original_weights: bool = ...) -> GraphModule:
    """
    Trace back from the weight node util we hit getattr, reconstruct the
    graph module with the traced nodes and run the graph module to pack the
    weight. then replace the original chain of ops with the packed weight.
    """
    ...

def special_pattern_replacement(model: GraphModule): # -> GraphModule:
    ...

