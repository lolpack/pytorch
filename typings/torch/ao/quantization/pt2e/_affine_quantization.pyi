"""
This type stub file was generated by pyright.
"""

import torch
from typing import Any, Optional, Union
from torch.ao.quantization.observer import AffineQuantizedObserverBase, Granularity, MappingType, TorchAODType, ZeroPointDomain

ABC: Any = ...
logger = ...
FP8_TYPES = ...
_SUB_BYTE_UINT_BOUNDS = ...
_DTYPE_TO_QVALUE_BOUNDS: dict[Union[torch.dtype, TorchAODType], tuple[int, int]] = ...
quant_lib = ...
register_custom_op = ...
def choose_qparams_affine_with_min_max(min_val: torch.Tensor, max_val: torch.Tensor, mapping_type: MappingType, block_size: tuple[int, ...], target_dtype: torch.dtype, quant_min: Optional[int] = ..., quant_max: Optional[int] = ..., eps: Optional[float] = ..., scale_dtype: Optional[torch.dtype] = ..., zero_point_dtype: Optional[torch.dtype] = ..., preserve_zero: bool = ..., zero_point_domain: Optional[ZeroPointDomain] = ...) -> tuple[torch.Tensor, torch.Tensor]:
    """A variant of :func:`~torchao.quantization.quant_primitives.choose_qparams_affine`
    operator that pass in min_val and max_val directly instead of deriving these from a single input.
    This is used for observers in static quantization where min_val and max_val may be obtained through
    tracking all the data in calibration data set.

    Args:
      Mostly same as :func:`~torchao.quantization.quant_primitives.choose_qparams_affine`. with one
      difference: instead of passing in `input` Tensor and use that to calculate min_val/max_val
      and then scale/zero_point, we pass in min_val/max_val directly
    """
    ...

@torch.no_grad()
def quantize_affine(input: torch.Tensor, block_size: tuple[int, ...], scale: torch.Tensor, zero_point: Optional[torch.Tensor], output_dtype: torch.dtype, quant_min: Optional[Union[int, float]] = ..., quant_max: Optional[Union[int, float]] = ..., zero_point_domain: Optional[ZeroPointDomain] = ...) -> torch.Tensor:
    """
    Args:
      input (torch.Tensor): original float32, float16 or bfloat16 Tensor
      block_size: (Tuple[int, ...]): granularity of quantization,
           this means the size of the tensor elements that's sharing the same qparam
           e.g. when size is the same as the input tensor dimension, we are using per tensor quantization
      scale (float): quantization parameter for affine quantization
      zero_point (int): quantization parameter for affine quantization
      output_dtype (torch.dtype): requested dtype (e.g. torch.uint8) for output Tensor
      quant_min (Optional[int]): minimum quantized value for output Tensor, if not specified, it will be derived from dtype
      quant_max (Optional[int]): maximum quantized value for output Tensor, if not specified, it will be derived from dtype
      zero_point_domain (ZeroPointDomain): the domain that zero_point is in, should be either integer or float
        if zero_point is in integer domain, zero point is added to the quantized integer value during
        quantization
        if zero_point is in floating point domain, zero point is subtracted from the floating point (unquantized)
        value during quantization
        default is ZeroPointDomain.INT

    Note:
      How can block_size represent different granularities?
      let's say we have a Tensor of size: (3, 3, 10, 10), here is the table showing how block_size represents different
      granularities:

       granularity type       |     block_size
         per_tensor           |    (3, 3, 10, 10)
         per_axis (axis=0)    |    (1, 3, 10, 10)
         per_axis (axis=1)    |    (3, 1, 10, 10)
     per_group (groupsize=2)  |    (3, 3, 10, 2)
     per_group (groupsize=2) for axis = 3 | (3, 3, 2, 10)


    Output:
      quantized tensor with requested dtype
    """
    ...

def dequantize_affine(input: torch.Tensor, block_size: tuple[int, ...], scale: torch.Tensor, zero_point: Optional[torch.Tensor], input_dtype: torch.dtype, quant_min: Optional[Union[int, float]] = ..., quant_max: Optional[Union[int, float]] = ..., zero_point_domain: ZeroPointDomain = ..., *, output_dtype: torch.dtype = ...) -> torch.Tensor:
    """
    Args:
      input (torch.Tensor): quantized tensor, should match the dtype `dtype` argument
      block_size: (List[int]): granularity of quantization,
        this means the size of the tensor elements that's sharing the same qparam
        e.g. when size is the same as the input tensor dimension, we are using per tensor quantization
      scale (Tensor): quantization parameter for affine quantization
      zero_point (Tensor): quantization parameter for affine quantization
      input_dtype (torch.dtype): requested dtype (e.g. torch.uint8) for output Tensor
      quant_min (Optional[int]): minimum quantized value for input Tensor
      quant_max (Optional[int]): maximum quantized value for input Tensor
      output_dtype (torch.dtype): dtype for output Tensor, default is fp32
      zero_point_domain (ZeroPointDomain): the domain that zero_point is in, should be either integer or float
        if zero_point is in integer domain, zero point is added to the quantized integer value during
        quantization
        if zero_point is in floating point domain, zero point is subtracted from the floating point (unquantized)
        value during quantization
        default is ZeroPointDomain.INT

    Output:
      dequantized Tensor, with requested dtype or fp32
    """
    ...

class AffineQuantizedMinMaxObserver(AffineQuantizedObserverBase):
    def forward(self, input: torch.Tensor): # -> Tensor:
        ...
    
    def calculate_qparams(self) -> tuple[torch.Tensor, torch.Tensor]:
        ...
    


class AffineQuantizedMovingAverageMinMaxObserver(AffineQuantizedObserverBase):
    def __init__(self, mapping_type: MappingType, target_dtype: torch.dtype, granularity: Granularity, averaging_constant=..., quant_min: Optional[int] = ..., quant_max: Optional[int] = ..., eps: Optional[float] = ..., is_dynamic=..., scale_dtype: Optional[torch.dtype] = ..., zero_point_dtype: Optional[torch.dtype] = ..., preserve_zero: bool = ..., zero_point_domain: Optional[ZeroPointDomain] = ..., **kwargs) -> None:
        ...
    
    def forward(self, input: torch.Tensor): # -> Tensor:
        ...
    
    def calculate_qparams(self) -> tuple[torch.Tensor, torch.Tensor]:
        ...
    


class AffineQuantizedPlaceholderObserver(AffineQuantizedObserverBase):
    def __init__(self, mapping_type: MappingType, target_dtype: torch.dtype, granularity: Granularity, quant_min: Optional[int] = ..., quant_max: Optional[int] = ..., eps: Optional[float] = ..., is_dynamic=..., scale_dtype: Optional[torch.dtype] = ..., zero_point_dtype: Optional[torch.dtype] = ..., preserve_zero: bool = ..., zero_point_domain: Optional[ZeroPointDomain] = ..., **kwargs) -> None:
        ...
    
    def forward(self, input): # -> Tensor:
        ...
    
    def calculate_qparams(self):
        ...
    


