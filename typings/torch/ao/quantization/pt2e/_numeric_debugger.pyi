"""
This type stub file was generated by pyright.
"""

import torch
from collections.abc import Sequence
from dataclasses import dataclass
from typing import Callable, Optional
from torch.export import ExportedProgram
from torch.fx import GraphModule

NUMERIC_DEBUG_HANDLE_KEY = ...
CUSTOM_KEY = ...
log = ...
def generate_numeric_debug_handle(ep: ExportedProgram) -> None:
    """
    Attach numeric_debug_handle_id for all nodes in the graph module of the given
    ExportedProgram, like conv2d, squeeze, conv1d, etc, except for placeholder.
    Notice that nodes like getattr are out of scope since they are not in the graph.

    The graph nodes of input exported program are modified inplace.

    Here's an example of using debug handle quantize flow::

        ep = export_for_training(eager_model, example_inputs)
        generate_numeric_debug_handle(ep)

        m = ep.module()
        quantizer = XNNPACKQuantizer()
        m = prepare_pt2e(m, quantizer)
        m = convert_pt2e(m)
    """
    ...

class OutputLogger(torch.nn.Module):
    """
    Base class for capturing output values for nodes in a GraphModule, it only captures
    Tensor output currently, but we can extend it to work for other types of inputs later if needed
    """
    _is_impure = ...
    def __init__(self, debug_handle: int, node_name: Optional[str] = ..., nn_module_stack: Optional[object] = ...) -> None:
        ...
    
    def forward(self, x: object) -> object:
        ...
    
    def __extra_repr__(self) -> str:
        ...
    


def prepare_for_propagation_comparison(model: GraphModule) -> GraphModule:
    """Add output loggers to node that has numeric_debug_handle

    Args:
        model (GraphModule): original model
    Returns:
        a model with output loggers for all nodes that has numeric_debug_handle_id
    """
    ...

@dataclass(frozen=True)
class QuantizationComparisonResult:
    actual: torch.Tensor
    ref: torch.Tensor
    @property
    def mse_loss(self) -> object:
        ...
    
    @property
    def sqnr(self) -> object:
        ...
    
    def loss(self, loss_function: Callable[[torch.Tensor, torch.Tensor], torch.Tensor]) -> object:
        ...
    
    def __repr__(self) -> str:
        ...
    
    def __post_init__(self) -> None:
        ...
    


@dataclass(frozen=True)
class NodeAccuracySummary:
    handle: int
    actual_node_name: str
    actual_module_stack: str
    ref_node_name: str
    ref_module_stack: str
    results: Sequence[QuantizationComparisonResult]
    ...


def extract_results_from_loggers(model: GraphModule) -> dict[int, tuple[Optional[str], object, list[object]]]:
    """For a given model, extract the tensors stats and related information for each debug handle.
    The reason we have a list of object, instead of Tensor is because the output of node may not be
    a Tensor, it could be (nested) list, tuple or dict as well.

    Returns:
        A dict is keyed by the debug_handle id and the values are a list of object recorded
        in loggers

    """
    ...

def compare_results(ref_results: dict[int, tuple[Optional[str], object, list[torch.Tensor]]], actual_results: dict[int, tuple[Optional[str], object, list[torch.Tensor]]]) -> dict[int, NodeAccuracySummary]:
    """Given two dict mapping from `debug_handle_id` (int) to list of tensors
    return a map from `debug_handle_id` to `NodeAccuracySummary` that contains
    comparison information like SQNR, MSE etc.

    Args:
        ref_results (Dict[int, Tuple[str, object, List[torch.Tensor]]]): reference results for each debug_handle_id
        actual_results (Dict[int, Tuple[str, object, List[torch.Tensor]]]): actual results for each debug_handle_id

    Returns:
        Dict[int, NodeAccuracySummary]
    """
    ...

