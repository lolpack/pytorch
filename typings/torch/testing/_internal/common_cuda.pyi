"""
This type stub file was generated by pyright.
"""

import contextlib
from torch.testing._internal.common_utils import TEST_NUMBA, TEST_WITH_ROCM

r"""This file is allowed to initialize CUDA context when imported."""
CUDA_ALREADY_INITIALIZED_ON_IMPORT = ...
TEST_MULTIGPU = ...
CUDA_DEVICE = ...
if TEST_WITH_ROCM:
    TEST_CUDNN = ...
else:
    TEST_CUDNN = ...
TEST_CUDNN_VERSION = ...
SM53OrLater = ...
SM60OrLater = ...
SM70OrLater = ...
SM75OrLater = ...
SM80OrLater = ...
SM89OrLater = ...
SM90OrLater = ...
SM100OrLater = ...
SM120OrLater = ...
IS_THOR = ...
IS_JETSON = ...
IS_SM89 = ...
def evaluate_gfx_arch_within(arch_list): # -> bool:
    ...

def CDNA3OrLater(): # -> bool:
    ...

def CDNA2OrLater(): # -> bool:
    ...

def evaluate_platform_supports_flash_attention(): # -> bool | LazyVal:
    ...

def evaluate_platform_supports_efficient_attention(): # -> bool:
    ...

def evaluate_platform_supports_cudnn_attention(): # -> Literal[False]:
    ...

PLATFORM_SUPPORTS_FLASH_ATTENTION: bool = ...
PLATFORM_SUPPORTS_MEM_EFF_ATTENTION: bool = ...
PLATFORM_SUPPORTS_CUDNN_ATTENTION: bool = ...
PLATFORM_SUPPORTS_FUSED_ATTENTION: bool = ...
PLATFORM_SUPPORTS_FUSED_SDPA: bool = ...
PLATFORM_SUPPORTS_BF16: bool = ...
def evaluate_platform_supports_fp8(): # -> LazyVal | bool:
    ...

PLATFORM_SUPPORTS_FP8: bool = ...
PLATFORM_SUPPORTS_MX_GEMM: bool = ...
if TEST_NUMBA:
    TEST_NUMBA_CUDA = ...
else:
    TEST_NUMBA_CUDA = ...
__cuda_ctx_rng_initialized = ...
def initialize_cuda_context_rng(): # -> None:
    ...

@contextlib.contextmanager
def tf32_off(): # -> Generator[None, Any, None]:
    ...

@contextlib.contextmanager
def tf32_on(self, tf32_precision=...): # -> Generator[None, Any, None]:
    ...

@contextlib.contextmanager
def tf32_enabled(): # -> Generator[None, Any, None]:
    """
    Context manager to temporarily enable TF32 for CUDA operations.
    Restores the previous TF32 state after exiting the context.
    """
    ...

def tf32_on_and_off(tf32_precision=...): # -> Callable[..., _Wrapped[Callable[..., Any], Any, Callable[..., Any], None]]:
    ...

def with_tf32_off(f): # -> _Wrapped[Callable[..., Any], Any, Callable[..., Any], Any]:
    ...

TEST_CUSPARSE_GENERIC = ...
TEST_HIPSPARSE_GENERIC = ...
def xfailIfSM89(func):
    ...

def xfailIfSM100OrLater(func):
    ...

def xfailIfSM120OrLater(func):
    ...

def xfailIfDistributedNotSupported(func):
    ...

if not CUDA_ALREADY_INITIALIZED_ON_IMPORT:
    ...
