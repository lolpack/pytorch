"""
This type stub file was generated by pyright.
"""

import torch
from collections.abc import Sequence
from typing import Any, Callable, Optional, Union
from torch._library.custom_ops import CustomOpDef
from torch.overrides import TorchFunctionMode

def dontGenerateOpCheckTests(reason: str): # -> Callable[..., Any]:
    ...

def is_abstract(tensor: torch.Tensor) -> bool:
    ...

def safe_schema_check(op: torch._ops.OpOverload, args: tuple[Any, ...], kwargs: dict[str, Any], *, copy_inputs: bool = ..., rtol: Optional[float] = ..., atol: Optional[float] = ...) -> Any:
    ...

def safe_autograd_registration_check(op: torch._ops.OpOverload, args: tuple[Any, ...], kwargs: dict[str, Any], *, copy_inputs: bool = ..., rtol: Optional[float] = ..., atol: Optional[float] = ...) -> None:
    ...

def safe_fake_check(op: torch._ops.OpOverload, args: tuple[Any, ...], kwargs: dict[str, Any], *, copy_inputs: bool = ..., rtol: Optional[float] = ..., atol: Optional[float] = ...) -> None:
    ...

def safe_aot_autograd_check(op: torch._ops.OpOverload, args: tuple[Any, ...], kwargs: dict[str, Any], dynamic: bool, *, copy_inputs: bool = ..., rtol: Optional[float] = ..., atol: Optional[float] = ...) -> Any:
    ...

def deepcopy_tensors(inputs: Any) -> Any:
    ...

ALL_TEST_UTILS = ...
GDOC = ...
DEFAULT_TEST_UTILS = ...
DEPRECATED_DEFAULT_TEST_UTILS = ...
def generate_opcheck_tests(testcase: Any, namespaces: list[str], failures_dict_path: Optional[str] = ..., additional_decorators: Optional[dict[str, Callable]] = ..., test_utils: list[str] = ...) -> None:
    """Given an existing TestCase, use the existing tests to generate
    additional validation tests for custom operators.

    For {all existing tests in the TestCase} x {all test utils},
    we will generate one new test. The new test runs a TorchFunctionMode
    that intercepts ``op(*args, **kwargs)`` calls and invokes
    ``test_util(op, *args, **kwargs)``, where ``op`` is an operator.

    The test_util that we support are in ALL_TEST_UTILS. They are:
    - test_schema: This runs SchemaCheckMode.
    - test_autograd_registration: This runs autograd_registration_check.
    - test_faketensor: This runs CrossRefFakeMode.
    - test_aot_dispatch_static: This runs aot_autograd_check, which:
        checks that the outputs (and gradients, if they are computable)
        are the same under eager-mode PyTorch and using AOTAutograd.
    - test_aot_dispatch_dynamic: Same as aot_dispatch_static, but
        runs AOTAutograd using dynamic shapes instead of static shapes.

    The generated test will have name ``{test_util}__{original_name}``.
    For example, if there is a method named ``test_cumsum``, then
    we will generate a ``test_schema__test_cumsum``,
    ``test_faketensor__test_cumsum``, etc.

    For more details, see https://docs.google.com/document/d/1Pj5HRZvdOq3xpFpbEjUZp2hBovhy7Wnxw14m6lF2154/edit

    Args:
        testcase: The testcase we will modify and generate additional tests for.
        namespaces: We will only intercept calls to custom operators with these
                    namespaces.
        failures_dict_path: See ``validate_failures_dict_structure`` for more details
        test_utils: a list of test_utils to generate. Example: ["test_schema", "test_faketensor"]
    """
    ...

def generate_tag_tests(testcase, failures_dict, additional_decorators): # -> None:
    ...

TEST_OPTIONS = ...
def validate_failures_dict_formatting(failures_dict_path: str) -> None:
    ...

def validate_failures_dict_structure(failure_dict: FailuresDict, test_utils: list[str], testcase: Any) -> None:
    """Validates the failures dict.

    The failure dict looks something like the following.
    It maps operator name (qualname) to a list of autogenerated tests.
    Each autogenerated test may have a check for the operator (if the operator is
    called by the test); the dictionary specifies if we should skip the check,
    or if we expect some check to fail.

    {
        "fbgemm::split_lengths": {
            "test_schema__test_split_lengths": {
                "comment": "you can put whatever you want into the comment section",
                "status": "xfail",
            }
            "test_schema__test_split_lengths_empty": {
                "comment": "",
                "status": "skip",
            },
        },
        "fbgemm::gather_lengths": {
            "test_schema__test_gather_lengths": {
                "comment": "",
                "status": "skip",
            },
        },
    }

    """
    ...

def should_update_failures_dict() -> bool:
    ...

_is_inside_opcheck_mode = ...
def is_inside_opcheck_mode(): # -> Any:
    ...

class OpCheckMode(TorchFunctionMode):
    """
    For a given test, OpCheckMode intercepts calls to operators and runs
    test_util(op, args, kwargs) for each intercepted (op, args, kwargs).
    """
    def __init__(self, namespaces: list[str], test_util_name: str, test_util: Callable, failures_dict: FailuresDict, test_name: str, failures_dict_path: str) -> None:
        ...
    
    def maybe_raise_errors_on_exit(self) -> None:
        ...
    
    def __enter__(self, *args, **kwargs): # -> Self:
        ...
    
    def __exit__(self, *args, **kwargs): # -> None:
        ...
    
    def run_test_util(self, op, args, kwargs): # -> None:
        ...
    
    def __torch_function__(self, func, types, args=..., kwargs=...): # -> Any:
        ...
    


def should_print_better_repro() -> None:
    """If set, the tests generated by `generate_opcheck_tests` will print a
    repro command on failure.

    In order to print the repro command, we need to save some tensors to disk.
    These will be saved under the following directory:
    {tempfile.gettempdir()}/pytorch_opcheck_safe_to_delete/.

    Although this is a temp folder, it will usually not automatically get cleaned
    up, so you'll need to manually delete it.
    """
    ...

def opcheck(op: Union[torch._ops.OpOverload, torch._ops.OpOverloadPacket, CustomOpDef], args: tuple[Any, ...], kwargs: Optional[dict[str, Any]] = ..., *, test_utils: Union[str, Sequence[str]] = ..., raise_exception: bool = ..., rtol: Optional[float] = ..., atol: Optional[float] = ...) -> dict[str, str]:
    """See torch.library.opcheck for docstring"""
    ...

class OpCheckError(Exception):
    ...


def generate_repro(test: str, op: torch._ops.OpOverload, args: tuple[Any, ...], kwargs: dict[str, Any], *, save_data: bool, dry_run: bool = ...) -> str:
    ...

def resolve_unique_overload_or_throw(op: torch._ops.OpOverloadPacket) -> torch._ops.OpOverload:
    ...

DUMP_OPTIONS = ...
FailuresDictData = dict[str, dict[str, dict[str, str]]]
VERSION = ...
DESCRIPTION = ...
class FailuresDict:
    def __init__(self, path: str, data: FailuresDictData) -> None:
        ...
    
    @staticmethod
    def load(path, *, create_file=...) -> FailuresDict:
        ...
    
    def save(self) -> None:
        ...
    
    def get_status(self, qualname: str, test_name: str) -> str:
        ...
    
    def set_status(self, qualname: str, test_name: str, status: str, *, comment: Optional[str] = ...): # -> None:
        ...
    


