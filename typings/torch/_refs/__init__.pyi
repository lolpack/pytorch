"""
This type stub file was generated by pyright.
"""

import builtins
import collections
import inspect
import itertools
import math
import operator
import warnings
import torch
import torch._prims as prims
import torch._prims_common as utils
import torch.utils._pytree as pytree
import torch._refs._conversions
import torch._refs.fft
import torch._refs.linalg
import torch._refs.nn.functional
import torch._refs.special
from collections.abc import Iterable, Sequence
from enum import Enum
from functools import partial, reduce, singledispatch, wraps
from typing import Any, Callable, Optional, Union, cast, overload
from torch import sym_float, sym_int
from torch._prims_common import BoolLike, DeviceLikeType, Dim, DimsSequenceType, DimsType, ELEMENTWISE_TYPE_PROMOTION_KIND, FloatLike, FloatWithoutSymFloat, IntLike, Number, NumberType, REDUCTION_OUTPUT_TYPE_KIND, RealNumberType, ShapeType, StrideType, TensorLike, TensorLikeType, TensorOrNumberLikeType, TensorSequenceType, definitely_contiguous, definitely_contiguous_for_memory_format, dtype_to_type, is_weakly_lesser_type
from torch._prims_common.wrappers import _maybe_convert_to_dtype, _maybe_resize_out, _safe_copy_out, elementwise_type_promotion_wrapper, elementwise_unary_scalar_wrapper, out_wrapper
from torch._decomp import register_decomposition

__all__ = ["abs", "acos", "acosh", "asinh", "asin", "atan", "atanh", "bitwise_not", "ceil", "conj_physical", "cos", "cosh", "count_nonzero", "deg2rad", "digamma", "erf", "erfinv", "erfc", "exp", "expm1", "exponential", "exp2", "fill", "fill_", "floor", "frac", "geometric", "index_add", "index_copy", "index_copy_", "index_select", "index_fill", "index_fill_", "isfinite", "isinf", "isposinf", "isneginf", "isnan", "isreal", "i0", "lerp", "lgamma", "log", "log1p", "log2", "log10", "log_normal", "log_softmax", "mvlgamma", "norm", "normal", "nan_to_num", "neg", "positive", "rad2deg", "reciprocal", "round", "sigmoid", "sgn", "sign", "signbit", "sin", "sinc", "sinh", "softmax", "sqrt", "square", "tan", "tanh", "trace", "trunc", "add", "atan2", "bitwise_and", "bitwise_left_shift", "bitwise_or", "bitwise_right_shift", "bitwise_xor", "clamp_min", "clamp_max", "copysign", "div", "eq", "float_power", "floor_divide", "fmax", "fmin", "fmod", "gcd", "ge", "gt", "heaviside", "hypot", "igamma", "igammac", "imag", "isclose", "lcm", "le", "logaddexp", "logaddexp2", "logical_and", "logical_not", "logical_or", "logical_xor", "logsumexp", "lt", "maximum", "minimum", "mul", "ne", "nextafter", "pow", "real", "rpow", "remainder", "rsub", "rtruediv", "rfloordiv", "sub", "true_divide", "trunc_divide", "xlogy", "addcdiv", "addcmul", "clamp", "masked_fill", "masked_fill_", "where", "clone", "copy_to", "item", "to", "all", "amax", "amin", "any", "cumsum", "cumprod", "mean", "dot", "vdot", "std", "std_mean", "sum", "sum_to_size", "prod", "var", "var_mean", "addr", "alias", "alias_copy", "atleast_1d", "atleast_2d", "atleast_3d", "as_strided", "as_strided_copy", "as_strided_scatter", "block_diag", "broadcast_shapes", "broadcast_tensors", "broadcast_to", "cat", "chunk", "column_stack", "conj", "constant_pad_nd", "contiguous", "diag_embed", "diag", "diagonal", "diagonal_copy", "diagonal_scatter", "dsplit", "dstack", "expand", "expand_as", "expand_copy", "flatten", "flip", "fliplr", "flipud", "hsplit", "hstack", "meshgrid", "movedim", "narrow", "narrow_copy", "native_group_norm", "native_layer_norm", "permute", "permute_copy", "ravel", "repeat", "reshape", "reshape_as", "roll", "rot90", "rsqrt", "split_with_sizes", "stack", "swap_axes", "squeeze", "squeeze_copy", "t", "t_copy", "T", "take_along_dim", "tensor_split", "transpose", "transpose_copy", "unbind_copy", "unfold", "unfold_copy", "unsqueeze", "unsqueeze_copy", "view", "view_as", "view_copy", "vsplit", "vstack", "view_as_complex", "unflatten", "unbind", "triu", "tril", "triu_indices", "tril_indices", "arange", "cauchy", "empty", "empty_like", "empty_permuted", "empty_strided", "eye", "full", "full_like", "linspace", "logspace", "new_empty", "new_empty_strided", "new_full", "new_ones", "new_zeros", "ones", "ones_like", "randn", "scalar_tensor", "zero", "zeros", "zeros_like", "allclose", "equal", "bucketize", "is_complex", "renorm", "stft", "istft"]
Tensor = torch.Tensor
DispatchKey = torch._C.DispatchKey
aten = ...
def is_noncontiguous_supported(device): # -> Literal[True]:
    ...

def handle_noncontiguous_outputs(input_tlist, output):
    ...

infer_aten_op = ...
@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.COMPLEX_TO_FLOAT, exact_dtype=True)
def abs(a): # -> Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def acos(a): # -> Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def acosh(a): # -> Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def asin(a): # -> Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def asinh(a): # -> Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def atan(a): # -> Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def atanh(a): # -> Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def bitwise_not(a): # -> Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT, exact_dtype=True)
def ceil(a): # -> Any:
    ...

@register_decomposition(aten.is_complex)
def is_complex(input: TensorLikeType): # -> bool:
    ...

@register_decomposition(aten.conj_physical)
@out_wrapper()
def conj_physical(input: TensorLikeType): # -> TensorLikeType | Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def cos(a): # -> Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def cosh(a): # -> Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def digamma(a): # -> Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def erf(a): # -> Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def erfinv(a): # -> Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def erfc(a): # -> Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def exp(a): # -> Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def expm1(a): # -> Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def exp2(a): # -> Any:
    ...

@out_wrapper()
@elementwise_type_promotion_wrapper(type_promoting_args="a,", type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.NO_OPMATH)
def fill(a: TensorLikeType, value: NumberType) -> TensorLikeType:
    ...

def fill_(a: TensorLikeType, value: NumberType) -> TensorLikeType:
    ...

@register_decomposition(aten.zero)
@out_wrapper()
def zero(input: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT, exact_dtype=True)
def floor(a): # -> Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT, exact_dtype=True)
def frac(x: TensorLikeType) -> TensorLikeType:
    ...

def imag(a: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.ALWAYS_BOOL, aten_op=None)
def isfinite(a: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.ALWAYS_BOOL)
def isinf(a: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.ALWAYS_BOOL, exact_dtype=True)
def isposinf(a: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.ALWAYS_BOOL, exact_dtype=True)
def isneginf(a: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.ALWAYS_BOOL)
def isnan(a: TensorLikeType) -> TensorLikeType:
    ...

mvlgamma = ...
@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.ALWAYS_BOOL, aten_op=None)
def isreal(a: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT, aten_op=aten.i0)
def i0(a): # -> Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def lgamma(a): # -> Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def log(a): # -> Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def log1p(a): # -> Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def log2(a): # -> Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def log10(a): # -> Any:
    ...

@out_wrapper()
def log_softmax(a: TensorLikeType, dim: int, dtype: Optional[torch.dtype] = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.logsumexp)
@out_wrapper()
@elementwise_type_promotion_wrapper(type_promoting_args=("self", ), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def logsumexp(self: TensorLikeType, dim: DimsType, keepdim: bool = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.nan_to_num)
@out_wrapper()
def nan_to_num(a: TensorLikeType, nan: Optional[NumberType] = ..., posinf: Optional[NumberType] = ..., neginf: Optional[NumberType] = ...) -> TensorLikeType:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT, extra_meta=_neg_meta)
def neg(a): # -> Any:
    ...

def positive(a: TensorLikeType) -> TensorLikeType:
    ...

def real(a: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def reciprocal(a): # -> Any:
    ...

@register_decomposition(aten.round)
@out_wrapper()
@elementwise_type_promotion_wrapper(type_promoting_args=("a", ), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def round(a: TensorLikeType, *, decimals: int = ...) -> TensorLikeType:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def rsqrt(a): # -> Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def sigmoid(a: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT, exact_dtype=True)
def sgn(a): # -> Tensor:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT, exact_dtype=True)
def sign(a): # -> Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.ALWAYS_BOOL, exact_dtype=True)
def signbit(a): # -> Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def sin(a): # -> Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def sinc(a): # -> Tensor:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def sinh(a): # -> Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def sqrt(a): # -> Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.BOOL_TO_LONG, aten_op=None)
def square(a: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def tan(a): # -> Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def tanh(a): # -> Any:
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT, exact_dtype=True)
def trunc(a): # -> Any:
    ...

def view_as_complex(self: TensorLikeType) -> TensorLikeType:
    ...

@register_decomposition(aten.add)
@out_wrapper()
@elementwise_type_promotion_wrapper(type_promoting_args=("a", "b"), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def add(a: Union[TensorLikeType, NumberType], b: Union[TensorLikeType, NumberType], *, alpha: Optional[NumberType] = ...): # -> Any:
    """
    Reference implementation of torch.add
    """
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT, supports_lhs_python_scalar=False, supports_rhs_python_scalar=False)
def atan2(a, b): # -> Any:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def bitwise_and(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def bitwise_left_shift(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def bitwise_or(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def bitwise_right_shift(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def bitwise_xor(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT, supports_lhs_python_scalar=False)
def copysign(a: Union[TensorLikeType, NumberType], b: Union[TensorLikeType, NumberType]):
    ...

@register_decomposition(aten.div)
@out_wrapper()
def div(a: Union[TensorLikeType, NumberType], b: Union[TensorLikeType, NumberType], *, rounding_mode: Optional[str] = ...): # -> TensorLikeType | Any | None:
    """
    Reference implementation of torch.div
    """
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.ALWAYS_BOOL, supports_lhs_python_scalar=False)
def eq(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.BOOL_TO_LONG)
def pow(a: Union[TensorLikeType, NumberType], b: Union[TensorLikeType, NumberType]) -> TensorLikeType:
    ...

@out_wrapper()
def float_power(a: Union[TensorLikeType, NumberType], b: Union[TensorLikeType, NumberType]) -> Tensor:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=utils.ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT, supports_two_python_scalars=True, should_register_decomposition=False)
def floor_divide(a: Union[TensorLikeType, NumberType], b: Union[TensorLikeType, NumberType]): # -> Tensor | None:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT, supports_lhs_python_scalar=False, supports_rhs_python_scalar=False)
def fmax(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT, supports_lhs_python_scalar=False, supports_rhs_python_scalar=False)
def fmin(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT, supports_lhs_python_scalar=False, supports_rhs_python_scalar=True)
def fmod(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType:
    ...

@register_decomposition(aten.frexp)
@out_wrapper("mantissa", "exponent")
def frexp(self: TensorLikeType) -> tuple[TensorLikeType, TensorLikeType]:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT, supports_lhs_python_scalar=False, supports_rhs_python_scalar=False)
def gcd(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.ALWAYS_BOOL, supports_lhs_python_scalar=False)
def ge(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.ALWAYS_BOOL, supports_lhs_python_scalar=False)
def gt(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT, supports_lhs_python_scalar=False, supports_rhs_python_scalar=False)
def heaviside(input: TensorLikeType, values: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT, supports_lhs_python_scalar=False, supports_rhs_python_scalar=False)
def hypot(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT, supports_lhs_python_scalar=False, supports_rhs_python_scalar=False)
def igamma(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT, supports_lhs_python_scalar=False, supports_rhs_python_scalar=False)
def igammac(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType:
    ...

def isclose(a: TensorLikeType, b: TensorLikeType, rtol: float = ..., atol: float = ..., equal_nan: bool = ...) -> TensorLikeType:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT, supports_lhs_python_scalar=False, supports_rhs_python_scalar=False)
def lcm(a: TensorLikeType, b: TensorLikeType): # -> Tensor | Any:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.ALWAYS_BOOL, supports_lhs_python_scalar=False)
def le(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT, supports_lhs_python_scalar=False, supports_rhs_python_scalar=False)
def logaddexp(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT, supports_lhs_python_scalar=False, supports_rhs_python_scalar=False)
def logaddexp2(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.ALWAYS_BOOL)
def logical_and(a: TensorLikeType, b: TensorLikeType):
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.ALWAYS_BOOL)
def logical_not(a: TensorLikeType): # -> bool:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.ALWAYS_BOOL)
def logical_or(a: TensorLikeType, b: TensorLikeType): # -> TensorLikeType:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.ALWAYS_BOOL)
def logical_xor(a: TensorLikeType, b: TensorLikeType):
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.ALWAYS_BOOL, supports_lhs_python_scalar=False)
def lt(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def maximum(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def minimum(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT, supports_two_python_scalars=True)
def mul(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.ALWAYS_BOOL, supports_lhs_python_scalar=False)
def ne(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.NO_OPMATH, supports_lhs_python_scalar=False, supports_rhs_python_scalar=False)
def nextafter(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def remainder(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType:
    ...

@register_decomposition(aten.rsub)
@out_wrapper()
def rsub(a: Union[TensorLikeType, NumberType], b: Union[TensorLikeType, NumberType], alpha: NumberType = ...): # -> Tensor:
    ...

@register_decomposition(aten.sub)
@out_wrapper()
@elementwise_type_promotion_wrapper(type_promoting_args=("a", "b"), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def sub(a: Union[TensorLikeType, NumberType], b: Union[TensorLikeType, NumberType], *, alpha: NumberType = ...): # -> Any:
    """
    Reference implementation of torch.sub
    """
    ...

@_make_elementwise_binary_reference(type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT, name="true_divide", aten_op=None, supports_two_python_scalars=True)
def true_divide(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType:
    ...

@register_decomposition(aten.xlogy)
@out_wrapper()
@elementwise_type_promotion_wrapper(type_promoting_args=("a", "b"), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def xlogy(a: Union[TensorLikeType, NumberType], b: Union[TensorLikeType, NumberType]): # -> Tensor:
    ...

@_make_elementwise_binary_reference(type_promotion_kind=utils.ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT, aten_op=None, supports_two_python_scalars=True)
def trunc_divide(a: Union[TensorLikeType, NumberType], b: Union[TensorLikeType, NumberType]): # -> Any:
    ...

@register_decomposition(aten.addcdiv)
@out_wrapper()
@elementwise_type_promotion_wrapper(type_promoting_args=("self", "tensor1", "tensor2"), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def addcdiv(self: TensorLikeType, tensor1: TensorLikeType, tensor2: TensorLikeType, *, value: NumberType = ...) -> TensorLikeType:
    """
    Reference implementation of torch.addcdiv
    """
    ...

@register_decomposition(aten.addcmul)
@out_wrapper()
@elementwise_type_promotion_wrapper(type_promoting_args=("self", "tensor1", "tensor2"), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def addcmul(self: TensorLikeType, tensor1: TensorLikeType, tensor2: TensorLikeType, *, value: NumberType = ...) -> TensorLikeType:
    """
    Reference implementation of torch.addcmul
    """
    ...

@register_decomposition(aten.clamp)
@out_wrapper()
@elementwise_type_promotion_wrapper(type_promoting_args=("a", "min", "max"), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def clamp(a: TensorLikeType, min: Optional[TensorOrNumberLikeType] = ..., max: Optional[TensorOrNumberLikeType] = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.clamp_min)
@out_wrapper()
def clamp_min(self: TensorLikeType, min: Optional[TensorOrNumberLikeType] = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.clamp_max)
@out_wrapper()
def clamp_max(self: TensorLikeType, max: Optional[TensorOrNumberLikeType] = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.where)
@out_wrapper()
@elementwise_type_promotion_wrapper(type_promoting_args=("a", "b"), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.NO_OPMATH)
def where(pred: Tensor, a: Optional[TensorOrNumberLikeType] = ..., b: Optional[TensorOrNumberLikeType] = ...): # -> Any:
    """ """
    ...

@register_decomposition(aten.clone)
@out_wrapper()
def clone(a: TensorLikeType, *, memory_format: torch.memory_format = ...) -> TensorLikeType:
    ...

def copy_to(a: Tensor, b: Tensor, *, allow_cross_device=...): # -> Any:
    ...

@register_decomposition(aten.item)
def item(a: TensorLikeType) -> NumberType:
    ...

def to(a: TensorLikeType, *args, **kwargs) -> TensorLikeType:
    ...

@register_decomposition(aten.all)
@out_wrapper()
def all(a: TensorLikeType, dim: Optional[DimsType] = ..., keepdim: bool = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.any)
@out_wrapper()
def any(a: TensorLikeType, dim: Optional[DimsType] = ..., keepdim: bool = ...) -> TensorLikeType:
    ...

@register_decomposition([aten.sum.dim_IntList, aten.sum.IntList_out])
def sum(a: TensorLikeType, dim: Union[Optional[int], Optional[list[int]]] = ..., keepdim: bool = ..., *, dtype: Optional[torch.dtype] = ..., out: Optional[Tensor] = ...) -> TensorLikeType:
    ...

def sum_to_size(a: Tensor, *shape) -> Tensor:
    ...

@register_decomposition(aten.prod)
def prod(a: TensorLikeType, dim: Union[Optional[int], Optional[list[int]]] = ..., keepdim: bool = ..., *, dtype=..., out: Optional[Tensor] = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.amin)
def amin(a: TensorLikeType, dim: Optional[DimsType] = ..., keepdim: bool = ..., *, out: Optional[Tensor] = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.amax)
def amax(a: TensorLikeType, dim: Optional[DimsType] = ..., keepdim: bool = ..., *, out: Optional[Tensor] = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.var)
@out_wrapper()
def var(a: TensorLikeType, dim: Optional[DimsType] = ..., unbiased: Optional[bool] = ..., keepdim: bool = ..., *, correction: Optional[NumberType] = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.std)
@out_wrapper()
def std(a: TensorLikeType, dim: Union[Optional[int], Optional[list[int]]] = ..., unbiased: Optional[bool] = ..., keepdim: bool = ..., *, correction: Optional[NumberType] = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.mean)
def mean(a: TensorLikeType, dim: Optional[DimsType] = ..., keepdim: bool = ..., *, dtype=..., out=...) -> TensorLikeType:
    ...

@register_decomposition(aten.std_mean)
@out_wrapper("out0", "out1")
def std_mean(a: TensorLikeType, dim: Optional[DimsType] = ..., *, unbiased: Optional[bool] = ..., keepdim: bool = ..., correction: Optional[NumberType] = ...): # -> tuple[TensorLikeType, TensorLikeType]:
    ...

@register_decomposition(aten.var_mean)
@out_wrapper("out0", "out1")
def var_mean(a: TensorLikeType, dim: Optional[DimsType] = ..., unbiased: Optional[bool] = ..., keepdim: bool = ..., *, correction: Optional[NumberType] = ...): # -> tuple[TensorLikeType, TensorLikeType]:
    ...

@register_decomposition(aten.addr)
@out_wrapper()
@elementwise_type_promotion_wrapper(type_promoting_args=("self", "vec1", "vec2"), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def addr(self: TensorLikeType, vec1: TensorLikeType, vec2: TensorLikeType, *, beta: NumberType = ..., alpha: NumberType = ...) -> TensorLikeType:
    ...

def atleast_1d(arg: Union[TensorLikeType, Sequence[TensorLikeType]], *args: TensorLikeType) -> Union[TensorLikeType, tuple[TensorLikeType, ...]]:
    """Reference implementation of :func:`torch.atleast_1d`."""
    ...

def atleast_2d(arg: Union[TensorLikeType, Sequence[TensorLikeType]], *args: TensorLikeType) -> Union[TensorLikeType, tuple[TensorLikeType, ...]]:
    """Reference implementation of :func:`torch.atleast_2d`."""
    ...

def atleast_3d(arg: Union[TensorLikeType, Sequence[TensorLikeType]], *args: TensorLikeType) -> Union[TensorLikeType, tuple[TensorLikeType, ...]]:
    """Reference implementation of :func:`torch.atleast_3d`."""
    ...

def as_strided(a: TensorLikeType, size: ShapeType, stride: StrideType, storage_offset: Optional[int] = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.as_strided_scatter)
@out_wrapper()
def as_strided_scatter(input: TensorLikeType, src: TensorLikeType, size: ShapeType, stride: StrideType, storage_offset: Optional[int] = ...) -> TensorLikeType:
    ...

def broadcast_shapes(*shapes) -> ShapeType:
    ...

@aten.broadcast_tensors.default.py_impl(DispatchKey.CompositeImplicitAutograd)
@aten.broadcast_tensors.default.py_impl(DispatchKey.Meta)
def broadcast_tensors(*tensors) -> list[TensorLikeType]:
    ...

def broadcast_to(a: TensorLikeType, size: ShapeType) -> TensorLikeType:
    ...

@register_decomposition(aten.cat)
@out_wrapper()
@elementwise_type_promotion_wrapper(type_promoting_args=("tensors", ), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.NO_OPMATH)
def cat(tensors: TensorSequenceType, dim: int = ...) -> TensorLikeType:
    ...

@out_wrapper()
def column_stack(tensors: TensorSequenceType) -> TensorLikeType:
    ...

def conj(input: TensorLikeType) -> TensorLikeType:
    ...

@register_decomposition(aten.constant_pad_nd)
@out_wrapper()
def constant_pad_nd(input: TensorLikeType, pad: list[int], value: NumberType = ...) -> TensorLikeType:
    ...

def contiguous(a: Tensor, *, memory_format: torch.memory_format = ...) -> Tensor:
    ...

@out_wrapper()
def dstack(tensors: TensorSequenceType) -> TensorLikeType:
    ...

@register_decomposition(aten.expand)
def expand(a: Tensor, *shape) -> Tensor:
    ...

def expand_as(a: Tensor, b: Tensor) -> Tensor:
    ...

def chunk(a: TensorLikeType, chunks: int, dim: int = ...) -> tuple[TensorLikeType, ...]:
    ...

def flatten(a: TensorLikeType, start_dim: int = ..., end_dim: int = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.flip)
@out_wrapper()
def flip(a: TensorLikeType, dims: DimsSequenceType) -> TensorLikeType:
    ...

def fliplr(a: TensorLikeType) -> TensorLikeType:
    ...

def flipud(a: TensorLikeType) -> TensorLikeType:
    ...

def narrow(a: TensorLikeType, dim: int, start: Union[int, TensorLikeType], length: int) -> TensorLikeType:
    ...

@register_decomposition(aten.native_group_norm.default)
def native_group_norm(input: Tensor, weight: Optional[Tensor], bias: Optional[Tensor], batch_size: int, num_channels: int, flattened_inner_size: int, num_groups: int, eps: float) -> tuple[Tensor, Tensor, Tensor]:
    ...

@register_decomposition(aten.native_layer_norm)
@out_wrapper("out0", "out1", "out2")
def native_layer_norm(input: Tensor, normalized_shape: ShapeType, weight: Optional[Tensor], bias: Optional[Tensor], eps: float) -> tuple[Tensor, Tensor, Tensor]:
    ...

@torch._subclasses.fake_impls.register_op_impl(aten.native_layer_norm.default)
def native_layer_norm_fake(fake_mode, func, *args, **kwargs): # -> tuple[Tensor, Tensor, Tensor]:
    ...

@register_decomposition(aten.permute)
def permute(a: TensorLikeType, *dims) -> TensorLikeType:
    ...

@register_decomposition(aten.renorm)
@out_wrapper()
def renorm(input: TensorLikeType, p: RealNumberType, dim: int, maxnorm: RealNumberType) -> TensorLikeType:
    ...

@aten.stft.center.py_impl(DispatchKey.CompositeImplicitAutograd)
def stft(input: Tensor, n_fft: int, hop_length: Optional[int] = ..., win_length: Optional[int] = ..., window: Optional[Tensor] = ..., center: bool = ..., pad_mode: str = ..., normalized: bool = ..., onesided: Optional[bool] = ..., return_complex: Optional[bool] = ..., align_to_window: Optional[bool] = ...) -> Tensor:
    ...

@aten.istft.default.py_impl(DispatchKey.CompositeImplicitAutograd)
def istft(input: Tensor, n_fft: int, hop_length: Optional[int] = ..., win_length: Optional[int] = ..., window: Optional[Tensor] = ..., center: bool = ..., normalized: bool = ..., onesided: Optional[bool] = ..., length: Optional[int] = ..., return_complex=...) -> Tensor:
    ...

@register_decomposition(aten.repeat)
@out_wrapper()
def repeat(a: Tensor, *repeat_shape) -> Tensor:
    ...

def reshape(a: TensorLikeType, *shape: ShapeType) -> TensorLikeType:
    ...

def reshape_as(self: TensorLikeType, other: TensorLikeType) -> TensorLikeType:
    ...

@register_decomposition(aten.roll)
@out_wrapper()
def roll(a: TensorLikeType, shifts: DimsType, dims: DimsType = ...) -> TensorLikeType:
    """Reference implementation of :func:`torch.roll`."""
    ...

@register_decomposition(aten.rot90)
@out_wrapper()
def rot90(a: TensorLikeType, k: int = ..., dims: DimsSequenceType = ...) -> TensorLikeType:
    """Reference implementation of :func:`torch.rot90`."""
    ...

@register_decomposition(aten.stack)
@out_wrapper()
def stack(tensors: TensorSequenceType, dim: int = ...) -> TensorLikeType:
    ...

@out_wrapper()
def softmax(a: TensorLikeType, dim: int, dtype: Optional[torch.dtype] = ...) -> TensorLikeType:
    ...

@out_wrapper()
def hstack(tensors: TensorSequenceType) -> TensorLikeType:
    ...

@out_wrapper()
def vstack(tensors: TensorSequenceType) -> TensorLikeType:
    ...

def unflatten(a: TensorLikeType, dim: int, sizes: ShapeType) -> TensorLikeType:
    ...

@register_decomposition(aten.unbind)
def unbind(t: TensorLikeType, dim: int = ...) -> TensorSequenceType:
    ...

@out_wrapper()
def index_copy(x: TensorLike, dim: int, index: TensorLike, tensor: TensorLike):
    ...

def index_copy_(x: TensorLike, dim: int, index: TensorLike, tensor: TensorLike): # -> TensorLike:
    ...

@register_decomposition(aten.index_fill)
@out_wrapper()
def index_fill(x: TensorLike, dim: int, index: TensorLike, value: Union[NumberType, TensorLike]): # -> TensorLike:
    ...

@register_decomposition(aten.index_fill_)
def index_fill_(x: TensorLike, dim: int, index: TensorLike, value: Union[NumberType, TensorLike]): # -> TensorLike:
    ...

@out_wrapper()
def index_add(x: TensorLike, dim: int, index: TensorLike, tensor: TensorLike, *, alpha: NumberType = ...):
    ...

@register_decomposition(aten.index_select)
@out_wrapper()
def index_select(x: TensorLike, dim: int, index: TensorLike):
    ...

@register_decomposition(aten.squeeze.dims)
def squeeze(a: TensorLikeType, dim: Optional[DimsType] = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.split_with_sizes)
def split_with_sizes(self: Tensor, split_sizes: list[int], dim: int = ...) -> list[Tensor]:
    ...

def tensor_split(a: TensorLikeType, indices_or_sections: Union[Tensor, DimsType], dim: int = ...) -> tuple[TensorLikeType, ...]:
    ...

def hsplit(a: TensorLikeType, indices_or_sections: DimsType) -> tuple[TensorLikeType, ...]:
    ...

def vsplit(a: TensorLikeType, indices_or_sections: DimsType) -> tuple[TensorLikeType, ...]:
    ...

@register_decomposition(aten.diag.out)
@out_wrapper()
def diag(self: TensorLikeType, offset: int = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.diagonal_scatter)
@out_wrapper()
def diagonal_scatter(input: TensorLikeType, src: TensorLikeType, offset: int = ..., dim1: int = ..., dim2: int = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.diagonal)
def diagonal(self: TensorLikeType, offset: int = ..., dim1: int = ..., dim2: int = ...) -> TensorLikeType:
    """
    Reference implementation of torch.diagonal
    """
    ...

@register_decomposition(aten.diag_embed)
@out_wrapper()
def diag_embed(t: TensorLikeType, offset: int = ..., dim1: int = ..., dim2: int = ...) -> TensorLikeType:
    """
    Reference implementation of torch.diag_embed
    """
    ...

def block_diag(*tensors: list[TensorLikeType]) -> TensorLikeType:
    """
    This is used as an input to PythonRefInfo. `torch.block_diag`
    expects arguments splatted, but `aten.block_diag` expects only
    one argument that is a list of Tensors.
    """
    ...

def dsplit(a: TensorLikeType, sections: DimsType) -> TensorSequenceType:
    ...

@register_decomposition(aten.t.default)
def t(a: TensorLikeType): # -> Tensor:
    ...

def T(a: TensorLikeType) -> TensorLikeType:
    ...

@register_decomposition(aten.alias)
def alias(a: TensorLikeType) -> TensorLikeType:
    ...

@register_decomposition(aten.transpose)
def transpose(a: TensorLikeType, dim0: int, dim1: int) -> TensorLikeType:
    ...

swap_axes = ...
@register_decomposition(aten.unfold)
def unfold(self: TensorLikeType, dimension: int, size: int, step: int) -> TensorLikeType:
    ...

@register_decomposition(aten.unfold_copy)
@out_wrapper()
def unfold_copy(self: TensorLikeType, dimension: int, size: int, step: int):
    ...

@register_decomposition(aten.cumsum)
def cumsum(a: TensorLikeType, dim: int, *, dtype: Optional[torch.dtype] = ..., out: Optional[Tensor] = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.cumprod)
def cumprod(a: TensorLikeType, dim: int, *, dtype: Optional[torch.dtype] = ..., out: Optional[Tensor] = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.unsqueeze)
def unsqueeze(a: TensorLikeType, dim: int) -> TensorLikeType:
    ...

@register_decomposition(aten.view.default)
def view(a: TensorLikeType, *shape: ShapeType) -> TensorLikeType:
    ...

def view_as(self: TensorLikeType, other: TensorLikeType) -> TensorLikeType:
    ...

def ravel(a: TensorLikeType) -> TensorLikeType:
    ...

@out_wrapper()
def take_along_dim(a: torch.Tensor, indices: torch.Tensor, dim: Optional[int] = ...) -> torch.Tensor:
    ...

@out_wrapper()
def empty(*shape, dtype: Optional[torch.dtype] = ..., layout: torch.layout = ..., device: Optional[DeviceLikeType] = ..., requires_grad: bool = ..., pin_memory: bool = ..., memory_format: torch.memory_format = ...) -> TensorLikeType:
    ...

@out_wrapper()
def empty_permuted(shape, physical_layout, dtype: Optional[torch.dtype] = ..., layout: torch.layout = ..., device: Optional[DeviceLikeType] = ..., requires_grad: bool = ..., pin_memory: bool = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.new_empty)
@out_wrapper()
def new_empty(a: TensorLikeType, size: ShapeType, *, dtype: Optional[torch.dtype] = ..., layout: Optional[torch.layout] = ..., device: Optional[DeviceLikeType] = ..., pin_memory: bool = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.new_empty_strided)
@out_wrapper()
def new_empty_strided(a: TensorLikeType, size: ShapeType, stride: StrideType, *, dtype: Optional[torch.dtype] = ..., layout: Optional[torch.layout] = ..., device: Optional[DeviceLikeType] = ..., pin_memory: bool = ...) -> TensorLikeType:
    """
    Reference implementation of torch.Tensor.new_empty_strided
    """
    ...

@register_decomposition(aten.zeros.default)
@out_wrapper()
def zeros(*size, dtype: Optional[torch.dtype] = ..., layout: torch.layout = ..., device: Optional[DeviceLikeType] = ..., pin_memory: bool = ..., requires_grad: bool = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.new_zeros)
@out_wrapper()
def new_zeros(a: TensorLikeType, size: ShapeType, *, dtype: Optional[torch.dtype] = ..., layout: Optional[torch.layout] = ..., device: Optional[DeviceLikeType] = ..., pin_memory: bool = ..., requires_grad: bool = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.ones.default)
@out_wrapper()
def ones(*size, dtype: Optional[torch.dtype] = ..., layout: torch.layout = ..., device: Optional[DeviceLikeType] = ..., pin_memory: bool = ..., requires_grad: bool = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.new_ones)
@out_wrapper()
def new_ones(a: TensorLikeType, size: ShapeType, *, dtype: Optional[torch.dtype] = ..., layout: Optional[torch.layout] = ..., device: Optional[DeviceLikeType] = ..., pin_memory: bool = ..., requires_grad: bool = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.new_full)
@out_wrapper()
def new_full(a: TensorLikeType, size: ShapeType, fill_value: NumberType, *, dtype: Optional[torch.dtype] = ..., layout: Optional[torch.layout] = ..., device: Optional[DeviceLikeType] = ..., pin_memory: bool = ...) -> TensorLikeType:
    ...

@aten.empty.out.py_impl(DispatchKey.CompositeImplicitAutograd)
def empty_out(size: TensorLikeType, out: TensorLikeType, memory_format: Optional[torch.memory_format] = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.empty_like)
@out_wrapper()
def empty_like(a: TensorLikeType, *, dtype: Optional[torch.dtype] = ..., device: Optional[DeviceLikeType] = ..., layout: Optional[torch.layout] = ..., pin_memory: bool = ..., requires_grad: bool = ..., memory_format: torch.memory_format = ...) -> TensorLikeType:
    ...

@register_decomposition([aten.arange.start_step, aten.arange.start_out])
@out_wrapper()
def arange(start: NumberType = ..., end: Optional[NumberType] = ..., step: NumberType = ..., *, dtype: Optional[torch.dtype] = ..., layout: torch.layout = ..., device: Optional[DeviceLikeType] = ..., pin_memory: bool = ..., requires_grad: bool = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.lerp)
@out_wrapper()
@elementwise_type_promotion_wrapper(type_promoting_args=("start", "end", "weight"), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def lerp(start: Tensor, end: Tensor, weight: Union[Tensor, NumberType]): # -> Any:
    ...

@register_decomposition(aten.linspace)
@out_wrapper()
def linspace(start: Union[NumberType, TensorLikeType], end: Union[NumberType, TensorLikeType], steps: NumberType, *, dtype: Optional[torch.dtype] = ..., device: Optional[DeviceLikeType] = ..., layout: torch.layout = ..., pin_memory: bool = ..., requires_grad: bool = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.logspace)
@out_wrapper()
def logspace(start: Union[NumberType, TensorLikeType], end: Union[NumberType, TensorLikeType], steps: NumberType, base: NumberType = ..., *, dtype: Optional[torch.dtype] = ..., device: Optional[DeviceLikeType] = ..., layout: torch.layout = ..., pin_memory: bool = ..., requires_grad: bool = ...) -> TensorLikeType:
    ...

@overload
def meshgrid(tensors: Sequence[TensorLikeType], indexing: str):
    ...

@overload
def meshgrid(*tensors: TensorLikeType, indexing: str):
    ...

@register_decomposition(aten.meshgrid)
def meshgrid(*tensors: Union[TensorLikeType, list[TensorLikeType], tuple[TensorLikeType]], indexing: str) -> list[TensorLikeType]:
    ...

def movedim(input: TensorLikeType, source: Union[int, DimsSequenceType], destination: Union[int, DimsSequenceType]) -> TensorLikeType:
    """
    Reference implementation of torch.movedim
    """
    ...

@register_decomposition(aten.empty_strided)
@out_wrapper()
def empty_strided(shape: Union[ShapeType, tuple[ShapeType]], strides: StrideType, *, dtype: Optional[torch.dtype] = ..., device: Optional[DeviceLikeType] = ..., layout: torch.layout = ..., requires_grad: bool = ..., pin_memory: bool = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.eye)
@out_wrapper()
def eye(n: int, m: Optional[int] = ..., *, dtype: Optional[torch.dtype] = ..., layout: torch.layout = ..., device: Optional[DeviceLikeType] = ..., pin_memory: bool = ..., requires_grad: bool = ...) -> TensorLikeType:
    """
    Reference implementation of torch.eye
    """
    ...

@register_decomposition([aten.full.default, aten.full.out])
@out_wrapper()
def full(shape: ShapeType, fill_value: NumberType, *, dtype: Optional[torch.dtype] = ..., layout: torch.layout = ..., device: Optional[DeviceLikeType] = ..., pin_memory: bool = ..., requires_grad: bool = ...) -> TensorLikeType:
    ...

def full_like(a: TensorLikeType, fill_value: NumberType, *, dtype: Optional[torch.dtype] = ..., layout: Optional[torch.layout] = ..., device: Optional[DeviceLikeType] = ..., pin_memory: bool = ..., requires_grad: bool = ..., memory_format: torch.memory_format = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.zeros_like)
@out_wrapper()
def zeros_like(a: TensorLikeType, *, dtype: Optional[torch.dtype] = ..., layout: Optional[torch.layout] = ..., device: Optional[DeviceLikeType] = ..., pin_memory: bool = ..., requires_grad: bool = ..., memory_format: torch.memory_format = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.ones_like)
@out_wrapper()
def ones_like(a: TensorLikeType, *, dtype: Optional[torch.dtype] = ..., layout: Optional[torch.layout] = ..., device: Optional[DeviceLikeType] = ..., pin_memory: bool = ..., requires_grad: bool = ..., memory_format: torch.memory_format = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.randn.default)
@out_wrapper()
def randn(*shape, dtype: Optional[torch.dtype] = ..., device: Optional[DeviceLikeType] = ..., layout: Optional[torch.layout] = ..., requires_grad: bool = ..., pin_memory: bool = ...) -> TensorLikeType:
    ...

def scalar_tensor(a: NumberType, *, dtype: Optional[torch.dtype] = ..., layout: torch.layout = ..., device: Optional[DeviceLikeType] = ..., pin_memory: bool = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.masked_fill)
@out_wrapper()
def masked_fill(a: TensorLikeType, mask: TensorLikeType, value: TensorOrNumberLikeType):
    ...

@register_decomposition(aten.masked_fill_)
def masked_fill_(a: TensorLikeType, mask: TensorLikeType, value: TensorOrNumberLikeType) -> TensorLikeType:
    ...

def allclose(a: TensorLikeType, b: TensorLikeType, rtol: float = ..., atol: float = ..., equal_nan: bool = ...) -> bool:
    """
    Reference implementation of torch.allclose
    """
    ...

def equal(a: TensorLikeType, b: TensorLikeType) -> bool:
    ...

@register_decomposition(aten.norm)
@out_wrapper(exact_dtype=True)
def norm(input: TensorLikeType, p: Optional[Union[float, str]] = ..., dim: Optional[DimsType] = ..., keepdim: bool = ..., *, dtype: Optional[torch.dtype] = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.trace)
@out_wrapper()
def trace(self: TensorLikeType) -> TensorLikeType:
    ...

rtruediv = ...
rfloordiv = ...
rpow = ...
@register_decomposition(aten.triu)
@out_wrapper()
def triu(a: TensorLikeType, diagonal: int = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.tril)
@out_wrapper()
def tril(a: TensorLikeType, diagonal: int = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.tril_indices)
@out_wrapper()
def tril_indices(row: int, col: int, offset: int = ..., *, dtype: torch.dtype = ..., layout: torch.layout = ..., device: DeviceLikeType = ..., pin_memory: bool = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.triu_indices)
@out_wrapper()
def triu_indices(row: int, col: int, offset: int = ..., *, dtype: torch.dtype = ..., layout: torch.layout = ..., device: DeviceLikeType = ..., pin_memory: bool = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.bucketize)
@out_wrapper(exact_dtype=True)
def bucketize(a: TensorOrNumberLikeType, boundaries: TensorLikeType, *, out_int32: bool = ..., right: bool = ...): # -> Tensor:
    ...

@register_decomposition(aten.cauchy)
@out_wrapper()
@elementwise_type_promotion_wrapper(type_promoting_args=("self", ), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def cauchy(self, median=..., sigma=..., generator=...):
    ...

@register_decomposition(aten.exponential)
@out_wrapper()
@elementwise_type_promotion_wrapper(type_promoting_args=("self", ), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def exponential(self, rate=..., generator=...):
    ...

@register_decomposition(aten.geometric)
@out_wrapper()
@elementwise_type_promotion_wrapper(type_promoting_args=("self", ), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def geometric(self, p, generator=...):
    ...

@register_decomposition(aten.log_normal)
@out_wrapper()
@elementwise_type_promotion_wrapper(type_promoting_args=("self", ), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def log_normal(self, mean=..., std=..., generator=...): # -> Tensor:
    ...

@register_decomposition(aten.normal)
@out_wrapper()
@elementwise_type_promotion_wrapper(type_promoting_args=("mean", "std"), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def normal(mean=..., std=..., size=..., *, generator=..., dtype=..., layout=..., device=..., pin_memory=...): # -> Any:
    ...

@register_decomposition(aten.normal_)
def normal_(self, mean=..., std=..., *, generator=...):
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def rad2deg(self: TensorLikeType):
    ...

@_make_elementwise_unary_reference(ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def deg2rad(self: TensorLikeType):
    ...

@register_decomposition(aten.count_nonzero)
@out_wrapper()
def count_nonzero(self, dim: Optional[DimsType] = ...):
    ...

@register_decomposition(aten.dot)
@out_wrapper(exact_dtype=True)
@_dot_check_wrapper
@elementwise_type_promotion_wrapper(type_promoting_args=("self", "other"), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def dot(self, other): # -> Tensor:
    ...

@register_decomposition(aten.vdot)
@out_wrapper(exact_dtype=True)
@_dot_check_wrapper
@elementwise_type_promotion_wrapper(type_promoting_args=("self", "other"), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def vdot(self, other): # -> Tensor:
    ...

@register_decomposition(aten.select_scatter)
@out_wrapper()
def select_scatter(x: TensorLikeType, src: TensorLikeType, dim: int, index: int): # -> Tensor:
    ...

abs_ = ...
acos_ = ...
acosh_ = ...
add_ = ...
addcmul_ = ...
addcdiv_ = ...
asin_ = ...
asinh_ = ...
atan_ = ...
atanh_ = ...
atan2_ = ...
bitwise_and_ = ...
bitwise_left_shift_ = ...
bitwise_not_ = ...
bitwise_or_ = ...
bitwise_right_shift_ = ...
bitwise_xor_ = ...
ceil_ = ...
clamp_ = ...
clamp_min_ = ...
clamp_max_ = ...
conj_physical_ = ...
copysign_ = ...
cos_ = ...
cosh_ = ...
cumsum_ = ...
cumprod_ = ...
deg2rad_ = ...
digamma_ = ...
div_ = ...
eq_ = ...
erf_ = ...
erfc_ = ...
erfinv_ = ...
exp_ = ...
exp2_ = ...
expm1_ = ...
float_power_ = ...
floor_ = ...
floor_divide_ = ...
fmod_ = ...
frac_ = ...
gcd_ = ...
ge_ = ...
gt_ = ...
heaviside_ = ...
hypot_ = ...
igamma_ = ...
igammac_ = ...
i0_ = ...
lcm_ = ...
le_ = ...
lerp_ = ...
lgamma_ = ...
log10_ = ...
log1p_ = ...
log2_ = ...
log_ = ...
logical_and_ = ...
logical_not_ = ...
logical_or_ = ...
logical_xor_ = ...
lt_ = ...
mul_ = ...
mvlgamma_ = ...
nan_to_num_ = ...
ne_ = ...
neg_ = ...
nextafter_ = ...
pow_ = ...
rad2deg_ = ...
reciprocal_ = ...
remainder_ = ...
rsqrt_ = ...
sgn_ = ...
sigmoid_ = ...
sign_ = ...
sin_ = ...
sinc_ = ...
sinh_ = ...
sqrt_ = ...
square_ = ...
sub_ = ...
tan_ = ...
tanh_ = ...
tril_ = ...
triu_ = ...
true_divide_ = ...
trunc_ = ...
xlogy_ = ...
cauchy_ = ...
exponential_ = ...
geometric_ = ...
log_normal_ = ...
zero_ = ...
alias_copy = ...
as_strided_copy = ...
diagonal_copy = ...
expand_copy = ...
narrow_copy = ...
squeeze_copy = ...
permute_copy = ...
t_copy = ...
transpose_copy = ...
unbind_copy = ...
unsqueeze_copy = ...
view_copy = ...
def tensor(data, *, dtype=..., device=..., pin_memory=..., requires_grad=...): # -> _NotImplementedType:
    ...

