"""
This type stub file was generated by pyright.
"""

import math
import torch
import torch._prims as prims
import torch._prims_common as utils
import torch._refs as refs
from functools import wraps
from typing import Callable, Optional, TypeVar, Union
from typing_extensions import Concatenate, ParamSpec
from torch._decomp import register_decomposition
from torch._prims_common import ELEMENTWISE_TYPE_PROMOTION_KIND, NumberType, ShapeType, TensorLike, TensorLikeType
from torch._prims_common.wrappers import elementwise_type_promotion_wrapper, elementwise_unary_scalar_wrapper, out_wrapper
from torch._refs import _make_inplace

__all__ = ["alpha_dropout", "celu", "celu_", "channel_shuffle", "dropout", "elu", "elu_", "gelu", "glu", "group_norm", "hardshrink", "hardtanh", "hinge_embedding_loss", "huber_loss", "l1_loss", "layer_norm", "leaky_relu", "log_softmax", "margin_ranking_loss", "mish", "mish_", "mse_loss", "nll_loss", "pairwise_distance", "pdist", "poisson_nll_loss", "prelu", "relu", "relu6", "selu", "selu_", "smooth_l1_loss", "softmax", "softmin", "softplus", "softshrink", "tanhshrink", "threshold", "threshold_", "triplet_margin_loss"]
_T = TypeVar("_T")
_P = ParamSpec("_P")
Tensor = torch.Tensor
aten = ...
DispatchKey = torch._C.DispatchKey
@register_decomposition(aten.alpha_dropout)
def alpha_dropout(self: TensorLikeType, p: float = ..., training: bool = ..., inplace: bool = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.celu)
@_inplace_wrapper
@out_wrapper()
@elementwise_type_promotion_wrapper(type_promoting_args=("a", ), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def celu(a: TensorLikeType, alpha: Optional[NumberType] = ..., inplace: bool = ...) -> TensorLikeType:
    """
    Reference implementation of torch.nn.functional.celu
    """
    ...

@_inplace_wrapper
@out_wrapper()
def dropout(a: TensorLikeType, p: float = ..., training: bool = ..., inplace: bool = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.elu)
@_inplace_wrapper
@out_wrapper()
@elementwise_type_promotion_wrapper(type_promoting_args=("a", ), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def elu(a: TensorLikeType, alpha: NumberType = ..., scale: NumberType = ..., input_scale: NumberType = ..., inplace: bool = ...) -> TensorLikeType:
    """
    Reference implementation of torch.nn.functional.elu
    """
    ...

@register_decomposition(aten.relu)
@_inplace_wrapper
@out_wrapper()
@elementwise_type_promotion_wrapper(type_promoting_args=("a", ), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def relu(a: TensorLikeType, inplace: bool = ...) -> TensorLikeType:
    """
    Reference implementation of torch.nn.functional.relu
    """
    ...

@register_decomposition(aten.channel_shuffle)
@out_wrapper()
def channel_shuffle(input: TensorLikeType, groups: int) -> TensorLikeType:
    """
    Reference implementation of :func:`torch.nn.functional.channel_shuffle`.
    """
    ...

def group_norm(input: Tensor, num_groups: int, weight: Optional[Tensor] = ..., bias: Optional[Tensor] = ..., eps: float = ...) -> Tensor:
    """
    Reference implementation of :func:`torch.nn.functional.group_norm`.
    """
    ...

def layer_norm(input: Tensor, normalized_shape: ShapeType, weight: Optional[Tensor] = ..., bias: Optional[Tensor] = ..., eps: float = ...) -> Tensor:
    """
    Reference implementation of :func:`torch.nn.functional.layer_norm`.
    """
    ...

@register_decomposition(aten.leaky_relu)
@_inplace_wrapper
@out_wrapper()
@elementwise_type_promotion_wrapper(type_promoting_args=("a", ), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def leaky_relu(a: TensorLikeType, negative_slope: float = ..., inplace: bool = ...) -> TensorLikeType:
    """
    Reference implementation of torch.nn.functional.leaky_relu
    """
    ...

@register_decomposition(aten.mish)
@_inplace_wrapper
@out_wrapper()
@elementwise_type_promotion_wrapper(type_promoting_args=("a", ), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def mish(a: TensorLikeType, inplace: bool = ...) -> TensorLikeType:
    """
    Reference implementation of torch.nn.functional.mish
    """
    ...

@register_decomposition(aten.selu)
@_inplace_wrapper
@out_wrapper()
@elementwise_type_promotion_wrapper(type_promoting_args=("a", ), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def selu(a: TensorLikeType, inplace: bool = ...) -> TensorLikeType:
    """
    Reference implementation of torch.nn.functional.selu
    """
    ...

def softmax(a: TensorLikeType, dim: Optional[int] = ..., _stacklevel: int = ..., dtype: Optional[torch.dtype] = ...) -> TensorLikeType:
    ...

def softmin(a: TensorLikeType, dim: Optional[int] = ..., _stacklevel: int = ..., dtype: Optional[torch.dtype] = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.softplus)
@_inplace_wrapper
@out_wrapper()
@elementwise_type_promotion_wrapper(type_promoting_args=("a", ), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def softplus(a: TensorLikeType, beta: Optional[NumberType] = ..., threshold: NumberType = ..., inplace: bool = ...) -> TensorLikeType:
    """
    Reference implementation of torch.nn.functional.softplus
    """
    ...

@aten.hardshrink.default.py_impl(DispatchKey.Autograd)
@register_decomposition(aten.hardshrink)
@out_wrapper()
def hardshrink(a: TensorLikeType, lambd: float = ...): # -> Tensor:
    ...

@aten.softshrink.default.py_impl(DispatchKey.Autograd)
@register_decomposition(aten.softshrink)
@out_wrapper()
def softshrink(a: TensorLikeType, lambd: float = ...): # -> Tensor:
    ...

@elementwise_type_promotion_wrapper(type_promoting_args=("input", "target"), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.COMPLEX_TO_FLOAT)
def l1_loss(input: TensorLikeType, target: TensorLikeType, size_average: Optional[bool] = ..., reduce: Optional[bool] = ..., reduction: str = ...) -> TensorLikeType:
    """
    Reference implementation of torch.nn.functional.l1_loss
    """
    ...

@elementwise_type_promotion_wrapper(type_promoting_args=("input", "target"), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.COMPLEX_TO_FLOAT)
def smooth_l1_loss(input: TensorLikeType, target: TensorLikeType, size_average: Optional[bool] = ..., reduce: Optional[bool] = ..., reduction: str = ..., beta: float = ...) -> TensorLikeType:
    """
    Reference implementation of torch.nn.functional.smooth_l1_loss
    """
    ...

def log_softmax(a: TensorLikeType, dim: Optional[int] = ..., _stacklevel: int = ..., dtype: Optional[torch.dtype] = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.margin_ranking_loss)
def margin_ranking_loss(input1: TensorLikeType, input2: TensorLikeType, target: TensorLikeType, margin: float = ..., reduction: str = ...) -> TensorLikeType:
    ...

@elementwise_type_promotion_wrapper(type_promoting_args=("input", "target"), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.COMPLEX_TO_FLOAT)
def mse_loss(input: TensorLikeType, target: TensorLikeType, size_average: Optional[bool] = ..., reduce: Optional[bool] = ..., reduction: str = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.hinge_embedding_loss)
def hinge_embedding_loss(input: TensorLikeType, target: TensorLikeType, margin: float = ..., reduction: str = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.nll_loss)
@out_wrapper()
@elementwise_type_promotion_wrapper(type_promoting_args=("input", ), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def nll_loss(input: TensorLikeType, target: TensorLikeType, weight: Optional[TensorLikeType] = ..., size_average: Optional[bool] = ..., ignore_index: int = ..., reduce: Optional[bool] = ..., reduction: str = ...) -> TensorLikeType:
    """
    Reference implementation of torch.nn.functional.nll_loss
    """
    ...

@register_decomposition(aten.huber_loss)
@out_wrapper()
@elementwise_type_promotion_wrapper(type_promoting_args=("input", "target"), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def huber_loss(input: TensorLikeType, target: TensorLikeType, reduction: Union[str, int] = ..., delta: float = ...) -> TensorLikeType:
    """
    Reference implementation of torch.nn.functional.huber_loss
    """
    ...

@elementwise_unary_scalar_wrapper
@elementwise_type_promotion_wrapper(type_promoting_args=("a", ), type_promotion_kind=utils.ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def tanhshrink(a: TensorLikeType) -> TensorLikeType:
    """
    Reference implementation of torch.nn.functional.tanhshrink
    """
    ...

@register_decomposition(aten.threshold)
@_inplace_wrapper
@out_wrapper()
@elementwise_type_promotion_wrapper(type_promoting_args=("a", ), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def threshold(a: TensorLikeType, threshold: NumberType, value: Union[bool, int, float], inplace: bool = ...) -> TensorLikeType:
    """
    Reference implementation of torch.nn.functional.threshold
    """
    ...

def triplet_margin_loss(anchor: TensorLikeType, positive: TensorLikeType, negative: TensorLikeType, margin: float = ..., p: float = ..., eps: float = ..., swap: bool = ..., size_average: Optional[bool] = ..., reduce: Optional[bool] = ..., reduction: str = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.hardtanh)
@_inplace_wrapper
@out_wrapper()
@elementwise_unary_scalar_wrapper
@elementwise_type_promotion_wrapper(type_promoting_args="a", type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def hardtanh(a: TensorLikeType, min_val: NumberType = ..., max_val: NumberType = ..., inplace: bool = ...) -> TensorLikeType:
    """
    Reference implementation of torch.nn.functional.hardtanh
    """
    ...

@register_decomposition(aten.gelu)
@out_wrapper()
@elementwise_unary_scalar_wrapper
@elementwise_type_promotion_wrapper(type_promoting_args=("a", ), type_promotion_kind=utils.ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def gelu(a: TensorLikeType, approximate: str = ...) -> TensorLikeType:
    """
    Reference implementation of torch.nn.functional.gelu
    """
    ...

@elementwise_type_promotion_wrapper(type_promoting_args=("input", "target"), type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT)
def poisson_nll_loss(input: TensorLikeType, target: TensorLikeType, log_input: bool = ..., full: bool = ..., size_average: Optional[bool] = ..., eps: float = ..., reduce: Optional[bool] = ..., reduction: str = ...) -> TensorLikeType:
    """
    Reference implementation of torch.nn.functional.poisson_nll_loss
    """
    ...

@register_decomposition(aten.prelu)
@elementwise_type_promotion_wrapper(type_promoting_args=("a", "weight"), type_promotion_kind=utils.ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def prelu(a: TensorLikeType, weight: TensorLikeType) -> TensorLikeType:
    """
    Reference implementation of torch.nn.functional.prelu
    """
    ...

@register_decomposition(aten.relu6)
@_inplace_wrapper
@out_wrapper()
def relu6(a: TensorLikeType, inplace: bool = ...) -> TensorLikeType:
    """
    Reference implementation of torch.nn.functional.relu6
    """
    ...

@register_decomposition(aten.glu)
@out_wrapper()
@elementwise_type_promotion_wrapper(type_promoting_args=("a", ), type_promotion_kind=utils.ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def glu(a: TensorLikeType, dim: int = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.pairwise_distance)
@out_wrapper()
def pairwise_distance(x1: TensorLikeType, x2: TensorLikeType, p: NumberType = ..., eps: NumberType = ..., keepdim=...) -> TensorLikeType:
    ...

@register_decomposition(aten.pdist)
@out_wrapper()
@elementwise_type_promotion_wrapper(type_promoting_args=("a", ), type_promotion_kind=utils.ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT)
def pdist(a: TensorLikeType, p: float = ...) -> TensorLikeType:
    ...

@register_decomposition(aten.pixel_shuffle)
@out_wrapper()
def pixel_shuffle(self: Tensor, upscale_factor: int):
    ...

@register_decomposition(aten.pixel_unshuffle)
@out_wrapper()
def pixel_unshuffle(self: Tensor, downscale_factor: int):
    ...

celu_ = ...
elu_ = ...
mish_ = ...
selu_ = ...
threshold_ = ...
