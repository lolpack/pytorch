"""
This type stub file was generated by pyright.
"""

import torch
from typing import Callable
from torch._ops import OpOverload

aten = ...
rng_decompositions: dict[str, dict[OpOverload, Callable]] = ...
def register_rng_decomposition(aten_op): # -> Callable[[Callable[_P, _T]], Callable[_P, _T]]:
    ...

def throw_on_non_cuda(device):
    ...

@register_rng_decomposition(aten.rand)
def rand(shape, dtype=..., layout=..., device=..., pin_memory=...): # -> Any:
    ...

@register_rng_decomposition(aten.rand_like)
def rand_like(x: torch.Tensor, dtype=..., layout=..., device=..., pin_memory=..., memory_format=...): # -> Any:
    ...

class PhiloxState:
    """
    Represents a PhiloxRngState - (seed, offset) where offset = base_offset +
    relative_offset. seed and base_offset basically point to the rng state just
    before tracing starts. relative offset tracks the totally consumed offset at
    trace time.
    """
    def __init__(self) -> None:
        ...
    
    def reset(self): # -> None:
        ...
    
    def validate_state(self): # -> None:
        ...
    
    def advance_offset(self, consumed_offset): # -> None:
        ...
    
    def set_state(self, seed, base_offset, relative_offset=...): # -> None:
        ...
    
    def get_state_as_tuple(self): # -> tuple[Tensor | Any, Any]:
        ...
    
    def get_state_as_tensor(self): # -> Tensor:
        ...
    
    def set_state_from_tensor(self, state): # -> None:
        ...
    


class PhiloxStateTracker:
    """
    Singleton class to track the philox rng state during AOT Autograd tracing.
    For each aot tracing instance, AOT Autograd resets this tracker and keeps
    track of both forward and backward offsets. At runtime, we only care about
    the total consumed forward and backward offsets. For dynamic shapes, these
    offsets are a function of input shapes. Therefore, the AOT generated graphs
    have additional outputs that compute total consumed forward and backward
    offsets.
    """
    running_state: PhiloxState
    fwd_state: PhiloxState
    bwd_state: PhiloxState
    def __enter__(self): # -> Self:
        ...
    
    def __exit__(self, exc_type, exc_cal, exc_tb): # -> None:
        ...
    
    @classmethod
    def reset(cls): # -> None:
        ...
    
    @classmethod
    def mark_beginning_of_forward(cls): # -> None:
        ...
    
    @classmethod
    def mark_beginning_of_backward(cls): # -> None:
        ...
    
    @classmethod
    def record_state(cls, seed, offset, mode): # -> None:
        ...
    
    @classmethod
    def get_state_as_tensor(cls): # -> Tensor:
        ...
    
    @classmethod
    def get_state_as_tuple(cls): # -> tuple[Tensor | Any, Any]:
        ...
    
    @classmethod
    def set_state_from_tensor(cls, x): # -> None:
        ...
    
    @classmethod
    def advance_offset(cls, consumed_offset): # -> None:
        ...
    
    @classmethod
    def get_current_relative_offset(cls): # -> int:
        ...
    
    @staticmethod
    def multiple_of_4(offset):
        ...
    
    @classmethod
    def get_updated_fwd_offset(cls): # -> Tensor:
        ...
    
    @classmethod
    def get_updated_bwd_offset(cls): # -> Tensor:
        ...
    


extra_random_decomps = ...
register_extra_random_decomp = ...
@register_extra_random_decomp([aten.bernoulli_])
def bernoulli_(self, p=...): # -> _NotImplementedType:
    ...

@register_extra_random_decomp([aten.bernoulli.p])
def bernoulli_p(self, p=..., *, generator=...): # -> _NotImplementedType:
    ...

