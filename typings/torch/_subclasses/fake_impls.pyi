"""
This type stub file was generated by pyright.
"""

import functools
import torch
from typing import Callable, Union
from torch._ops import OpOverload

pytree = ...
__all__ = ["op_implementations_checks", "get_fast_op_impls", "stride_incorrect_op", "has_meta"]
op_implementations_dict = ...
op_implementations_checks = ...
aten = ...
def ordered_set(*items): # -> dict[Any, bool]:
    ...

def is_noncontiguous_supported(device):
    ...

_like_tensor_constructors = ...
_device_not_kwarg_ops = ...
_non_kwarg_device_constructors = ...
def contains_tensor_types(type): # -> bool:
    ...

def register_op_impl(run_impl_check: Union[Callable[[OpOverload], bool], OpOverload]): # -> Callable[..., Any]:
    ...

@register_op_impl(op_implementations_dict.__contains__)
def dispatch_to_op_implementations_dict(fake_mode, func, *args, **kwargs):
    ...

@register_op_impl(_is_tensor_constructor)
@register_op_impl([*_like_tensor_constructors])
def constructors(fake_mode, func, *args, **kwargs): # -> FakeTensor:
    ...

@register_op_impl(aten.is_pinned.default)
def non_kwarg_is_pinned(fake_mode, func, *args, **kwargs):
    ...

@register_op_impl(aten.to.prim_Device)
@register_op_impl(aten.to.device)
def non_kwarg_to(fake_mode, func, *args, **kwargs):
    ...

def stride_incorrect_op(op): # -> Literal[False]:
    ...

@register_op_impl(stride_incorrect_op)
def wordaround_stride_incorrect_op(fake_mode, func, *args, **kwargs): # -> FakeTensor:
    ...

@register_op_impl(aten.resize_as_.default)
def resize_as_(fake_mode, func, *args, **kwargs):
    ...

@register_op_impl(lambda func: torch.Tag.dynamic_output_shape in func.tags and func not in [aten.index.Tensor, aten.nonzero.default, aten.repeat_interleave.Tensor])
def dyn_shape(fake_mode, func, *args, **kwargs):
    ...

@register_op_impl(aten._unique2.default)
def unique2(fake_mode, func, arg, sorted=..., return_inverse=..., return_counts=...): # -> tuple[Any, ...]:
    ...

@register_op_impl(aten.unique_dim.default)
def unique_dim(fake_mode, func, arg, dim, sorted=..., return_inverse=..., return_counts=...): # -> tuple[Any, ...]:
    ...

@register_op_impl(aten.unique_consecutive.default)
def _(fake_mode, func, arg, return_inverse=..., return_counts=..., dim=...): # -> tuple[Any, ...]:
    ...

@register_op_impl(aten.repeat_interleave.Tensor)
def repeat_interleave_tensor(fake_mode, func, repeats, output_size=...):
    ...

@register_op_impl(torch.ops.aten.item.default)
@register_op_impl(torch.ops.aten._local_scalar_dense.default)
def local_scalar_dense(fake_mode, func, arg):
    ...

@register_op_impl(torch.ops.aten.nonzero_numpy.default)
def nonzero_numpy(fake_mode, func, arg): # -> Any:
    ...

@register_op_impl(torch.ops.aten.nonzero.default)
def nonzero(fake_mode, func, arg):
    ...

@register_op_impl(torch.ops.aten.masked_select.default)
def masked_select(fake_mode, func, self, mask):
    ...

@register_op_impl(torch.ops.aten._assert_tensor_metadata.default)
def assert_tensor_metadata(fake_mode, func, t, sizes=..., strides=..., dtype=..., *, device=..., layout=...) -> None:
    ...

@register_op_impl(lambda func: torch.Tag.data_dependent_output in func.tags)
def data_dep(fake_mode, func, *args, **kwargs):
    ...

def check_no_bool_index_tensors(func, self, indices): # -> None:
    ...

def run_and_return_new_tensor_of_input_device(fake_mode, func, args, kwargs): # -> FakeTensor:
    ...

_is_builtin_namespaces = ...
def is_builtin(op): # -> bool:
    ...

def has_meta(func): # -> bool:
    ...

@register_op_impl(lambda func: is_builtin(func) and func.name().startswith("aten::_foreach_") and has_meta(func))
def foreach_run_and_map_input_device(fake_mode, func, *args, **kwargs): # -> _NotImplementedType | list[Any]:
    ...

@register_op_impl(aten.index.Tensor)
def index_tensor(fake_mode, func, *args, **kwargs):
    ...

@register_op_impl(aten._embedding_bag.default)
def embedding_bag(fake_mode, func, *args, **kwargs): # -> tuple[Any, Any, Any, Any]:
    ...

@register_op_impl(aten._unsafe_index_put.default)
@register_op_impl(aten.copy.default)
@register_op_impl(aten.copy_.default)
@register_op_impl(aten.slice_scatter.default)
def multi_device_op_default(fake_mode, func, *args, **kwargs): # -> FakeTensor:
    ...

@register_op_impl(aten.copy.out)
@register_op_impl(aten.slice_scatter.out)
def multi_device_op_out(fake_mode, func, *args, **kwargs): # -> Any:
    ...

@register_op_impl(aten.index_put.default)
@register_op_impl(aten.index_put_.default)
def index_put_impl(fake_mode, func, *args, **kwargs): # -> Any | FakeTensor:
    ...

@register_op_impl(aten._nested_tensor_from_tensor_list.default)
@register_op_impl(aten._nested_tensor_from_tensor_list.out)
@register_op_impl(aten._nested_view_from_buffer.default)
@register_op_impl(aten._nested_view_from_buffer_copy.default)
def nested_tensors_unsupported(fake_mode, func, *args, **kwargs):
    ...

@register_op_impl([x for x in _device_not_kwarg_ops if x not in (aten.is_pinned.default, aten.to.device, aten.to.prim_Device, aten._nested_tensor_from_tensor_list.default, aten._nested_tensor_from_tensor_list.out)])
def nyi(fake_mode, func, *args, **kwargs): # -> None:
    ...

@register_op_impl([aten.convolution.default, aten.convolution_backward.default])
def conv(fake_mode, func, *args, **kwargs): # -> FakeTensor | tuple[Any | FakeTensor, Any | FakeTensor, Any | FakeTensor]:
    ...

@register_op_impl(torch.ops.aten.bincount.default)
def bincount(fake_mode, func, inputs, weights=..., minlength=...):
    ...

FAST_OP_IMPLEMENTATIONS = ...
def register_fast_op_impl(func: OpOverload): # -> Callable[..., Any]:
    ...

def infer_size(a, b): # -> tuple[int, ...]:
    ...

def make_fast_binary_impl(slow_ref, type_promotion_kind=...): # -> Callable[..., Any | FakeTensor]:
    ...

def fast_detach(fake_mode, x, include_real=...): # -> FakeTensor:
    ...

@functools.cache
def get_fast_op_impls(): # -> dict[Any, Any]:
    ...

