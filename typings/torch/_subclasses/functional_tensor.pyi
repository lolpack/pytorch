"""
This type stub file was generated by pyright.
"""

import contextlib
import torch
from abc import ABC, abstractmethod
from contextlib import AbstractContextManager
from typing import Any, Callable, Optional, Union
from torch.utils._python_dispatch import TorchDispatchMode

not_implemented_log = ...
class FunctionalTensor(torch.Tensor):
    """
    Functional tensors represent tensors that will remove mutations
    from a program. If you perform a mutable operation on a functional tensor,
    it will re-dispatch to the functional variant of that operation.

    Historically, functionalization is implemented in C++ in the dispatcher.
    This class is a lightweight python shim around the C++ functionalization logic.

    FunctionalTensor is required to be used with a corresponding
    FunctionalTensormode active, because it relies
    on using the mode for dispatch (which can properly handle factory functions).
    """
    elem: torch.Tensor
    _mode_key = ...
    _extra_dispatch_keys = ...
    metadata_fns = ...
    _inference_mode_base: Optional[FunctionalTensor] = ...
    def __new__(cls, elem, mode):
        ...
    
    def __torch_dispatch__(self, func, types, args=..., kwargs=...): # -> _NotImplementedType:
        ...
    
    def __repr__(self) -> str:
        ...
    
    @staticmethod
    def to_functional(x): # -> FunctionalTensor:
        ...
    
    def from_functional(self): # -> Tensor:
        ...
    
    def is_base_tensor(self) -> bool:
        ...
    
    def replace_(self, output) -> None:
        ...
    
    def commit_update(self) -> None:
        ...
    
    def sync(self) -> None:
        ...
    
    def mark_mutation_hidden_from_autograd(self) -> None:
        ...
    
    def tolist(self) -> Any:
        ...
    
    def to(self, *args, **kwargs):
        ...
    
    def cuda(self, device=..., *args, **kwargs):
        ...
    
    char = ...
    cpu = ...
    bfloat16 = ...
    byte = ...
    double = ...
    float = ...
    bool = ...
    half = ...
    int = ...
    long = ...
    def to_dense(self):
        ...
    
    @property
    def layout(self): # -> layout:
        ...
    
    def __bool__(self): # -> bool:
        ...
    


class FunctionalTensorMode(TorchDispatchMode):
    def __init__(self, pre_dispatch=..., export=..., _allow_token_discovery=...) -> None:
        ...
    
    def __enter__(self): # -> Self:
        ...
    
    def __exit__(self, a, b, c): # -> None:
        ...
    
    def __torch_dispatch__(self, func, types, args=..., kwargs=...): # -> _NotImplementedType | Any | tuple[Any, ...]:
        ...
    
    @classmethod
    def is_infra_mode(cls) -> bool:
        ...
    


@contextlib.contextmanager
def disable_functional_mode(): # -> Generator[TorchDispatchMode | None, Any, None]:
    ...

def dispatch_functionalize(func, mode: FunctionalTensorMode = ...): # -> Callable[..., PyTree]:
    ...

class BaseFunctionalizeAPI(ABC):
    @abstractmethod
    def wrap_tensors(self, args: tuple[Any]) -> tuple[Any]:
        ...
    
    @abstractmethod
    def unwrap_tensors(self, args: Union[torch.Tensor, tuple[torch.Tensor, ...]]) -> Any:
        ...
    
    @abstractmethod
    def functionalize(self, inner_f: Callable) -> Callable:
        ...
    
    @abstractmethod
    def redispatch_to_next(self) -> AbstractContextManager:
        ...
    
    @abstractmethod
    def replace(self, input_tensor, output_tensor) -> None:
        ...
    
    @abstractmethod
    def commit_update(self, tensor) -> None:
        ...
    
    @abstractmethod
    def sync(self, tensor) -> None:
        ...
    
    @abstractmethod
    def mark_mutation_hidden_from_autograd(self, tensor) -> None:
        ...
    


class PythonFunctionalizeAPI(BaseFunctionalizeAPI):
    def __init__(self, mode: Optional[FunctionalTensorMode] = ..., pre_dispatch: bool = ...) -> None:
        ...
    
    def wrap_tensors(self, args: tuple[Any]) -> tuple[Any]:
        ...
    
    def unwrap_tensors(self, args: Union[torch.Tensor, tuple[torch.Tensor, ...], list[torch.Tensor]]) -> Any:
        ...
    
    def functionalize(self, inner_f: Callable) -> Callable:
        ...
    
    def redispatch_to_next(self) -> AbstractContextManager:
        ...
    
    def replace(self, input_tensor, output_tensor) -> None:
        ...
    
    def commit_update(self, tensor) -> None:
        ...
    
    def sync(self, tensor) -> None:
        ...
    
    def mark_mutation_hidden_from_autograd(self, tensor) -> None:
        ...
    


class CppFunctionalizeAPI(BaseFunctionalizeAPI):
    def wrap_tensors(self, args: tuple[Any]) -> tuple[Any]:
        ...
    
    def unwrap_tensors(self, args: Union[torch.Tensor, tuple[torch.Tensor, ...]]) -> Union[torch.Tensor, tuple[torch.Tensor, ...]]:
        ...
    
    def functionalize(self, inner_f: Callable) -> Callable:
        ...
    
    def redispatch_to_next(self) -> AbstractContextManager:
        ...
    
    def replace(self, input_tensor, output_tensor) -> None:
        ...
    
    def commit_update(self, tensor) -> None:
        ...
    
    def sync(self, tensor) -> None:
        ...
    
    def mark_mutation_hidden_from_autograd(self, tensor) -> None:
        ...
    


class FunctorchFunctionalizeAPI(BaseFunctionalizeAPI):
    def __init__(self, interpreter) -> None:
        ...
    
    def wrap_tensors(self, args: tuple[Any]) -> tuple[Any]:
        ...
    
    def unwrap_tensors(self, args: Union[torch.Tensor, tuple[torch.Tensor, ...]]) -> Union[torch.Tensor, tuple[torch.Tensor, ...]]:
        ...
    
    def functionalize(self, inner_f: Callable) -> Callable:
        ...
    
    def redispatch_to_next(self) -> AbstractContextManager:
        ...
    
    def replace(self, input_tensor, output_tensor) -> None:
        ...
    
    def commit_update(self, tensor) -> None:
        ...
    
    def sync(self, tensor) -> None:
        ...
    
    def mark_mutation_hidden_from_autograd(self, tensor) -> None:
        ...
    


def mb_unwrap_functional_tensor(tensor: torch.Tensor): # -> Tensor:
    ...

