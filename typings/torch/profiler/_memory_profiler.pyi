"""
This type stub file was generated by pyright.
"""

import collections
import dataclasses
import enum
import torch
from collections.abc import Iterator
from typing import Optional
from torch._C import FunctionSchema
from torch._C._autograd import _ProfilerResult
from torch._C._profiler import RecordScope, _ExtraFields_Allocation, _ExtraFields_TorchOp, _ProfilerEvent, _TensorMetadata

KeyAndID = tuple["Key", int]
TensorAndID = tuple["TensorKey", int]
log = ...
class Category(enum.Enum):
    INPUT = ...
    TEMPORARY = ...
    ACTIVATION = ...
    GRADIENT = ...
    AUTOGRAD_DETAIL = ...
    PARAMETER = ...
    OPTIMIZER_STATE = ...


_CATEGORY_TO_COLORS = ...
_CATEGORY_TO_INDEX = ...
class Action(enum.Enum):
    PREEXISTING = ...
    CREATE = ...
    INCREMENT_VERSION = ...
    DESTROY = ...


_ACTION_TO_INDEX = ...
@dataclasses.dataclass(eq=True, unsafe_hash=False, frozen=True)
class Key:
    device: torch.device
    ...


@dataclasses.dataclass
class _Storage:
    """Bundle storage pointer and id.

    All profiling logic should use `allocation_id`, however it is useful to
    print storage pointers for debugging and unit tests sometimes look up
    values using the storage data pointer of a live Tensor."""
    ptr: int
    allocation_id: int
    def __repr__(self) -> str:
        ...
    
    def __eq__(self, other: object) -> bool:
        ...
    
    def __hash__(self) -> int:
        ...
    


@dataclasses.dataclass(eq=True, unsafe_hash=True, frozen=True)
class TensorKey(Key):
    """Hashable identifier for a storage which has been assigned an ID.

    A detailed description of Tensor IDs and why they are needed is given in
    `torch/csrc/profiler/collection.h` when `TensorID` is declared. To
    summarize, multiple Storage buffers can map to the same logical Tensor.
    This dataclass is used to refer to a concrete in-memory StorageImpl of
    a Tensor.
    """
    id: int
    storage: _Storage
    def __repr__(self) -> str:
        ...
    
    def __lt__(self, other: TensorKey) -> bool:
        ...
    
    @classmethod
    def from_allocation(cls, alloc: _ExtraFields_Allocation) -> Optional[TensorKey]:
        ...
    
    @classmethod
    def from_tensor(cls, t: Optional[_TensorMetadata]) -> Optional[TensorKey]:
        ...
    


def extract_parameters(node: _ProfilerEvent) -> Iterator[TensorKey]:
    ...

def extract_gradients(node: _ProfilerEvent) -> Iterator[tuple[Optional[TensorKey], TensorKey]]:
    ...

def get_scopes(event: Optional[_ProfilerEvent]) -> tuple[RecordScope, ...]:
    ...

class SchemaMatcher:
    """Lookup operator schema based on profiled name.

    When profiling we record the operator's name but not the schema. However
    some analysis requires that information. Fortunately we can look up
    registered schema from the recorded name. We do not, however, record the
    overload and so we must compare the profiled arguments with all overloads
    to determine viable matches.

    Note: Once https://github.com/pytorch/pytorch/issues/78871 is completed
    this code will be obsolete.
    """
    @classmethod
    def inputs_are_mutable(cls, t: _ExtraFields_TorchOp) -> tuple[Optional[bool], ...]:
        """Determine which inputs may have mutated based on function schema.

        Note that we don't need to resolve down to a single schema to perform
        this analysis. An input is mutable if it is mutable in any overload. In
        practice, however, it is overwhelmingly common to match a single
        overload. If we cannot find any valid schema then we must be
        conservative and assume all inputs are mutable.
        """
        ...
    
    @classmethod
    def match_schemas(cls, t: _ExtraFields_TorchOp) -> tuple[FunctionSchema, ...]:
        ...
    
    @staticmethod
    def lookup_schemas(name: str) -> Optional[tuple[FunctionSchema, ...]]:
        ...
    


class OpTree:
    def __init__(self, result: _ProfilerResult) -> None:
        ...
    
    def dfs(self, *args, **kwargs) -> Iterator[_ProfilerEvent]:
        ...
    
    @property
    def sorted_nodes(self) -> tuple[_ProfilerEvent, ...]:
        ...
    


class SizeMap:
    def __init__(self, op_tree: OpTree) -> None:
        ...
    
    def __getitem__(self, key: TensorKey): # -> int:
        ...
    


@dataclasses.dataclass()
class DataFlowEdge:
    input_version: Optional[int] = ...
    mutated: Optional[bool] = ...
    @property
    def is_allocation(self) -> bool:
        ...
    
    @property
    def is_deletion(self) -> bool:
        ...
    


class DataFlowNode:
    def __init__(self, event: _ProfilerEvent, graph: DataFlowGraph) -> None:
        ...
    
    @property
    def inputs(self) -> dict[TensorKey, tuple[bool, int]]:
        ...
    
    @property
    def outputs(self) -> dict[TensorKey, int]:
        ...
    
    @property
    def intermediates(self) -> tuple[TensorKey, ...]:
        ...
    
    @property
    def start_time(self) -> int:
        ...
    


class DataFlowGraph:
    def __init__(self, op_tree: OpTree) -> None:
        ...
    
    @property
    def flow_nodes(self) -> tuple[DataFlowNode, ...]:
        ...
    
    def validate(self): # -> None:
        ...
    
    @property
    def leaf_events(self) -> tuple[_ProfilerEvent, ...]:
        ...
    
    def lookup(self, key: TensorKey) -> int:
        ...
    
    def bump(self, key: TensorKey) -> None:
        ...
    
    def delete(self, key: TensorKey) -> None:
        ...
    


@dataclasses.dataclass
class CategoryElement:
    by_id: Optional[Category] = ...
    by_key: dict[TensorKey, Category] = ...
    by_version: dict[TensorAndID, Category] = ...
    _by_id_keyset: set[TensorKey] = ...


@dataclasses.dataclass
class CategoryDict:
    _values: collections.defaultdict[int, CategoryElement] = ...
    def set_by_id(self, key: TensorKey, category: Category) -> None:
        ...
    
    def set_by_key(self, key: TensorKey, category: Category) -> None:
        ...
    
    def set_by_version(self, key: TensorKey, version: int, category: Category) -> None:
        ...
    
    def setdefault_by_version(self, key: TensorKey, version: int, category: Category) -> None:
        ...
    
    def get(self, key: Key, version: int) -> Optional[Category]:
        ...
    


class MemoryProfile:
    def __init__(self, result: _ProfilerResult) -> None:
        ...
    
    @property
    def timeline(self) -> tuple[tuple[int, Action, KeyAndID, int], ...]:
        ...
    


class MemoryProfileTimeline:
    def __init__(self, memory_profile) -> None:
        """The minimum representation of the memory profile timeline
        includes the memory timeline and categories. The timeline
        consists of [timestamp, action, (TensorKey, version), numbytes]
        elements, to denote any actions (pre-existing, create, destroy,
        or increment_version) that occurred to a specific Tensor for a
        chunk of memory. The categories help map each (TensorKey,
        version) pair into a category."""
        ...
    
    def export_memory_timeline(self, path, device_str) -> None:
        """Saves the memory timeline as [times, sizes by category]
        as a JSON formatted file to the given path for the given
        device."""
        ...
    
    def export_memory_timeline_raw(self, path, device_str) -> None:
        """Saves the memory timeline as raw memory event tuples in the
        form of (timestamp, action, numbytes, category)
        as a JSON formatted file to the given path for the given
        device."""
        ...
    
    def export_memory_timeline_html(self, path, device_str, figsize=..., title=...) -> None:
        """Exports the memory timeline as an HTML file which contains
        the memory timeline plot embedded as a PNG file."""
        ...
    


