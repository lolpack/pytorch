"""
This type stub file was generated by pyright.
"""

import abc
import torch
import torch.utils._pytree as pytree
from dataclasses import dataclass
from enum import Enum
from typing import Any, Optional, Union
from torch.export.exported_program import ExportedProgram, ModuleCallSignature

log = ...
__all__ = ["FlatArgsAdapter", "InterpreterModule", "InterpreterModuleDispatcher", "UnflattenedModule", "unflatten"]
class _AttrKind(Enum):
    PARAMETER = ...
    BUFFER = ...
    CONSTANT = ...
    MODULE = ...


RUN_WITH_INTERPRETER = ...
class _SubmoduleBase:
    _ty: Optional[str]
    def type_name(self) -> Optional[str]:
        ...
    


class InterpreterModule(_SubmoduleBase, torch.nn.Module):
    """A module that uses torch.fx.Interpreter to execute instead of the usual
    codegen that GraphModule uses. This provides better stack trace information
    and makes it easier to debug execution.
    """
    graph_module: Optional[torch.fx.GraphModule]
    def __init__(self, graph: torch.fx.Graph, ty: Optional[str] = ...) -> None:
        ...
    
    def forward(self, *args, **kwargs): # -> Any:
        ...
    
    def finalize(self): # -> None:
        ...
    
    def print_readable(self, print_output=..., include_stride=..., include_device=..., colored=...): # -> str:
        ...
    


class InterpreterModuleDispatcher(_SubmoduleBase, torch.nn.Module):
    """
    A module that carries a sequence of InterpreterModules corresponding to
    a sequence of calls of that module. Each call to the module dispatches
    to the next InterpreterModule, and wraps back around after the last.
    """
    def __init__(self, attrs: set[str], call_modules: list[InterpreterModule]) -> None:
        ...
    
    def forward(self, *args, **kwargs): # -> Any:
        ...
    
    def call_modules(self): # -> list[InterpreterModule]:
        ...
    
    def print_readable(self, print_output=..., include_stride=..., include_device=..., colored=...): # -> str:
        ...
    


class FlatArgsAdapter(abc.ABC):
    """
    Adapts input arguments with ``input_spec`` to align ``target_spec``.
    """
    @abc.abstractmethod
    def adapt(self, target_spec: pytree.TreeSpec, input_spec: pytree.TreeSpec, input_args: list[Any], metadata: Optional[dict[str, Any]] = ..., obj: Optional[Any] = ...) -> list[Any]:
        """NOTE: This adapter may mutate given ``input_args_with_path``."""
        ...
    


class UnflattenedModule(torch.nn.Module):
    def __init__(self, export_module: ExportedProgram, flat_args_adapter: Optional[FlatArgsAdapter] = ...) -> None:
        ...
    
    def process_forward_inputs(self, *args, **kwargs): # -> list[Any]:
        ...
    
    def forward(self, *args, **kwargs): # -> Any | tuple[Any, ...]:
        ...
    
    def finalize(self): # -> None:
        ...
    
    def print_readable(self, print_output=..., include_stride=..., include_device=..., colored=...): # -> str:
        ...
    


def unflatten(module: ExportedProgram, flat_args_adapter: Optional[FlatArgsAdapter] = ...) -> UnflattenedModule:
    """Unflatten an ExportedProgram, producing a module with the same module
    hierarchy as the original eager module. This can be useful if you are trying
    to use :mod:`torch.export` with another system that expects a module
    hierachy instead of the flat graph that :mod:`torch.export` usually produces.

    .. note:: The args/kwargs of unflattened modules will not necessarily match
        the eager module, so doing a module swap (e.g. :code:`self.submod =
        new_mod`) will not necessarily work. If you need to swap a module out, you
        need to set the :code:`preserve_module_call_signature` parameter of
        :func:`torch.export.export`.

    Args:
        module (ExportedProgram): The ExportedProgram to unflatten.
        flat_args_adapter (Optional[FlatArgsAdapter]): Adapt flat args if input TreeSpec does not match with exported module's.

    Returns:
        An instance of :class:`UnflattenedModule`, which has the same module
        hierarchy as the original eager module pre-export.
    """
    ...

class _ModuleFrame:
    def __init__(self, flat_graph: torch.fx.Graph, nodes: tuple[torch.fx.Node, ...], seen_nodes, seen_modules, seen_attrs, created_modules, parent, module_stack: list[tuple[str, Optional[str], int]], module_id, module_call_graph: dict[str, ModuleCallSignature], module: Optional[Union[torch.fx.GraphModule, UnflattenedModule]] = ...) -> None:
        ...
    
    def add_placeholder(self, x): # -> None:
        ...
    
    def copy_sym_call_function(self, x): # -> Node:
        ...
    
    def remap_input(self, x): # -> Node | Any:
        ...
    
    def finalize_outputs(self): # -> None:
        ...
    
    def copy_node(self, node): # -> None:
        ...
    
    def run_outer(self): # -> None:
        ...
    
    def print(self, *args, **kwargs): # -> None:
        ...
    
    def run_from(self, node_idx): # -> None:
        ...
    


@dataclass
class _SubmoduleEntry:
    parent_fqn: str
    parent_module: torch.nn.Module
    parent_call_module: torch.fx.Node
    fqn: str
    call_idx: int
    module: torch.nn.Module
    ...


class _IVals:
    """
    Collect the intermediate values of mutations in a graph.

    Example: in the following graph, suppose that buf_in and buf_out
    are the input and output values of a buffer.

        buf_in = placeholder()
        ...
        ival1 = f0(buf_in, ...)  # inside self.n0(...)
        ...
        ival2 = f1(ival1, ...)  # inside self.n1(...)
        ...
        buf_out = f2(ival2, ...)  # inside self.n2(...)
        return buf_out, ...

    Here ival1 and ival2 are intermediate values created inside
    calls to n0 and n1 respectively, and used inside calls to
    n1 and n2 respectively.
    """
    def __init__(self) -> None:
        ...
    
    def read(self, mf, node):
        """
        Read state corresponding to a given intermediate value.
        """
        ...
    
    def update(self, partitions): # -> None:
        """
        Update states corresponding to intermediate values that were read.
        """
        ...
    


