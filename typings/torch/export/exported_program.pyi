"""
This type stub file was generated by pyright.
"""

import dataclasses
import sympy
import torch
import torch.utils._pytree as pytree
from collections.abc import Iterator
from typing import Any, Callable, Optional, TYPE_CHECKING, Union
from torch.fx._symbolic_trace import _ConstantAttributeType
from torch.utils._sympy.value_ranges import ValueRanges
from torch._export.verifier import Verifier
from torch.export.decomp_utils import CustomDecompTable
from torch.fx._compatibility import compatibility
from torch.fx.passes.infra.pass_base import PassResult
from .graph_signature import ArgumentSpec, ExportGraphSignature

if TYPE_CHECKING:
    ...
__all__ = ["ExportedProgram", "ModuleCallEntry", "ModuleCallSignature", "default_decompositions"]
PassType = Callable[[torch.fx.GraphModule], Optional[PassResult]]
@dataclasses.dataclass
class ModuleCallSignature:
    inputs: list[ArgumentSpec]
    outputs: list[ArgumentSpec]
    in_spec: pytree.TreeSpec
    out_spec: pytree.TreeSpec
    forward_arg_names: Optional[list[str]] = ...
    def replace_all_uses_with(self, original_node, new_node): # -> None:
        ...
    


@dataclasses.dataclass
class ModuleCallEntry:
    fqn: str
    signature: Optional[ModuleCallSignature] = ...


_AUTOGRAD_ALIAS_BACKEND_KEYS_TO_OVERRIDE = ...
_BACKEND_KEYS_TO_OVERRIDE = ...
def default_decompositions() -> CustomDecompTable:
    """
    This is the default decomposition table which contains decomposition of
    all ATEN operators to core aten opset. Use this API together with
    :func:`run_decompositions()`
    """
    ...

class ExportedProgram:
    """
    Package of a program from :func:`export`. It contains
    an :class:`torch.fx.Graph` that represents Tensor computation, a state_dict containing
    tensor values of all lifted parameters and buffers, and various metadata.

    You can call an ExportedProgram like the original callable traced by
    :func:`export` with the same calling convention.

    To perform transformations on the graph, use ``.module`` property to access
    an :class:`torch.fx.GraphModule`. You can then use
    `FX transformation <https://pytorch.org/docs/stable/fx.html#writing-transformations>`_
    to rewrite the graph. Afterwards, you can simply use :func:`export`
    again to construct a correct ExportedProgram.
    """
    _graph_module: torch.fx.GraphModule
    _graph_signature: ExportGraphSignature
    _state_dict: dict[str, Any]
    _range_constraints: dict[sympy.Symbol, ValueRanges]
    _module_call_graph: list[ModuleCallEntry]
    _example_inputs: Optional[tuple[tuple[Any, ...], dict[str, Any]]]
    _constants: dict[str, _ConstantAttributeType]
    _verifiers: list[type[Verifier]]
    def __init__(self, root: Union[torch.nn.Module, dict[str, Any]], graph: torch.fx.Graph, graph_signature: ExportGraphSignature, state_dict: dict[str, Union[torch.Tensor, torch.nn.Parameter]], range_constraints: dict[sympy.Symbol, Any], module_call_graph: list[ModuleCallEntry], example_inputs: Optional[tuple[tuple[Any, ...], dict[str, Any]]] = ..., constants: Optional[dict[str, _ConstantAttributeType]] = ..., *, verifiers: Optional[list[type[Verifier]]] = ...) -> None:
        ...
    
    @property
    @compatibility(is_backward_compatible=False)
    def graph_module(self): # -> GraphModule:
        ...
    
    @graph_module.setter
    @compatibility(is_backward_compatible=False)
    def graph_module(self, value):
        ...
    
    @property
    @compatibility(is_backward_compatible=False)
    def graph(self): # -> Graph:
        ...
    
    @graph.setter
    @compatibility(is_backward_compatible=False)
    def graph(self, value):
        ...
    
    @property
    @compatibility(is_backward_compatible=False)
    def graph_signature(self): # -> ExportGraphSignature:
        ...
    
    @graph_signature.setter
    @compatibility(is_backward_compatible=False)
    def graph_signature(self, value):
        ...
    
    @property
    @compatibility(is_backward_compatible=False)
    def state_dict(self): # -> dict[str, Any]:
        ...
    
    @state_dict.setter
    @compatibility(is_backward_compatible=False)
    def state_dict(self, value):
        ...
    
    @compatibility(is_backward_compatible=False)
    def parameters(self) -> Iterator[torch.nn.Parameter]:
        """
        Returns an iterator over original module's parameters.
        """
        ...
    
    @compatibility(is_backward_compatible=False)
    def named_parameters(self) -> Iterator[tuple[str, torch.nn.Parameter]]:
        """
        Returns an iterator over original module parameters, yielding
        both the name of the parameter as well as the parameter itself.
        """
        ...
    
    @compatibility(is_backward_compatible=False)
    def buffers(self) -> Iterator[torch.Tensor]:
        """
        Returns an iterator over original module buffers.
        """
        ...
    
    @compatibility(is_backward_compatible=False)
    def named_buffers(self) -> Iterator[tuple[str, torch.Tensor]]:
        """
        Returns an iterator over original module buffers, yielding
        both the name of the buffer as well as the buffer itself.
        """
        ...
    
    @property
    @compatibility(is_backward_compatible=False)
    def range_constraints(self): # -> dict[Symbol, ValueRanges[Any]]:
        ...
    
    @range_constraints.setter
    @compatibility(is_backward_compatible=False)
    def range_constraints(self, value):
        ...
    
    @property
    @compatibility(is_backward_compatible=False)
    def module_call_graph(self): # -> list[ModuleCallEntry]:
        ...
    
    @module_call_graph.setter
    @compatibility(is_backward_compatible=False)
    def module_call_graph(self, value):
        ...
    
    @property
    @compatibility(is_backward_compatible=False)
    def example_inputs(self): # -> tuple[tuple[Any, ...], dict[str, Any]] | None:
        ...
    
    @example_inputs.setter
    @compatibility(is_backward_compatible=False)
    def example_inputs(self, value): # -> None:
        ...
    
    @property
    @compatibility(is_backward_compatible=False)
    def call_spec(self): # -> CallSpec:
        ...
    
    @call_spec.setter
    @compatibility(is_backward_compatible=False)
    def call_spec(self, value):
        ...
    
    @property
    @compatibility(is_backward_compatible=False)
    def verifier(self) -> Any:
        ...
    
    @verifier.setter
    @compatibility(is_backward_compatible=False)
    def verifier(self, value):
        ...
    
    @property
    @compatibility(is_backward_compatible=False)
    def dialect(self) -> str:
        ...
    
    @dialect.setter
    @compatibility(is_backward_compatible=False)
    def dialect(self, value):
        ...
    
    @property
    @compatibility(is_backward_compatible=False)
    def verifiers(self): # -> list[type[Verifier]]:
        ...
    
    @verifiers.setter
    @compatibility(is_backward_compatible=False)
    def verifiers(self, value):
        ...
    
    @property
    @compatibility(is_backward_compatible=False)
    def tensor_constants(self): # -> dict[str, _ConstantAttributeType]:
        ...
    
    @tensor_constants.setter
    @compatibility(is_backward_compatible=False)
    def tensor_constants(self, value):
        ...
    
    @property
    @compatibility(is_backward_compatible=False)
    def constants(self): # -> dict[str, _ConstantAttributeType]:
        ...
    
    @constants.setter
    @compatibility(is_backward_compatible=False)
    def constants(self, value):
        ...
    
    def __call__(self, *args: Any, **kwargs: Any) -> Any:
        ...
    
    def __str__(self) -> str:
        ...
    
    def module(self) -> torch.nn.Module:
        """
        Returns a self contained GraphModule with all the parameters/buffers inlined.
        """
        ...
    
    @_disable_prexisiting_fake_mode
    def run_decompositions(self, decomp_table: Optional[dict[torch._ops.OperatorBase, Callable]] = ..., decompose_custom_triton_ops: bool = ...) -> ExportedProgram:
        """
        Run a set of decompositions on the exported program and returns a new
        exported program. By default we will run the Core ATen decompositions to
        get operators in the
        `Core ATen Operator Set <https://pytorch.org/docs/stable/torch.compiler_ir.html>`_.

        For now, we do not decompose joint graphs.

        Args:
            decomp_table:
             An optional argument that specifies decomp behaviour for Aten ops
             (1) If None, we decompose to core aten decompositions
             (2) If empty, we don't decompose any operator


        Some examples:

        If you don't want to decompose anything

        .. code-block:: python

            ep = torch.export.export(model, ...)
            ep = ep.run_decompositions(decomp_table={})

        If you want to get a core aten operator set except for certain operator, you can do following:

        .. code-block:: python

            ep = torch.export.export(model, ...)
            decomp_table = torch.export.default_decompositions()
            decomp_table[your_op] = your_custom_decomp
            ep = ep.run_decompositions(decomp_table=decomp_table)
        """
        ...
    
    @compatibility(is_backward_compatible=False)
    def validate(self): # -> None:
        ...
    


