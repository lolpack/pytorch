"""
This type stub file was generated by pyright.
"""

from typing import Optional, Union
from torch import Tensor
from .optimizer import Optimizer, ParamsT, _disable_dynamo_if_unsupported, _use_grad_for_differentiable

r"""Implementation for the RAdam algorithm."""
__all__ = ["RAdam", "radam"]
class RAdam(Optimizer):
    def __init__(self, params: ParamsT, lr: Union[float, Tensor] = ..., betas: tuple[float, float] = ..., eps: float = ..., weight_decay: float = ..., decoupled_weight_decay: bool = ..., *, foreach: Optional[bool] = ..., maximize: bool = ..., capturable: bool = ..., differentiable: bool = ...) -> None:
        ...
    
    def __setstate__(self, state): # -> None:
        ...
    
    @_use_grad_for_differentiable
    def step(self, closure=...): # -> None:
        """Perform a single optimization step.

        Args:
            closure (Callable, optional): A closure that reevaluates the model
                and returns the loss.
        """
        ...
    


@_disable_dynamo_if_unsupported(single_tensor_fn=_single_tensor_radam)
def radam(params: list[Tensor], grads: list[Tensor], exp_avgs: list[Tensor], exp_avg_sqs: list[Tensor], state_steps: list[Tensor], decoupled_weight_decay: bool = ..., foreach: Optional[bool] = ..., differentiable: bool = ..., capturable: bool = ..., has_complex: bool = ..., maximize: bool = ..., *, beta1: float, beta2: float, lr: float, weight_decay: float, eps: float): # -> None:
    r"""Functional API that performs RAdam algorithm computation.

    See :class:`~torch.optim.RAdam` for details.
    """
    ...

