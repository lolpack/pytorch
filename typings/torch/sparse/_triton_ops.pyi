"""
This type stub file was generated by pyright.
"""

import torch
from typing import Optional
from torch.utils._triton import has_triton

TORCH_SPARSE_BSR_SCATTER_MM_LRU_CACHE_SIZE = ...
def check(cond, msg): # -> None:
    ...

def check_bsr_layout(f_name, t): # -> None:
    ...

def check_device(f_name, t, device): # -> None:
    ...

def check_mm_compatible_shapes(f_name, lhs, rhs): # -> None:
    ...

def check_dtype(f_name, t, dtype, *additional_dtypes): # -> None:
    ...

def check_blocksize(f_name, blocksize): # -> None:
    ...

def make_triton_contiguous(t):
    """Return input as a triton-contiguous tensor.

    A triton-contiguous tensor is defined as a tensor that has strides
    with minimal value smaller than or equal to 1.

    While triton kernels support triton-non-contiguous tensors (all
    strides being greater than 1) arguments, a considerable slow-down
    occurs because tensor data is copied element-wise rather than
    chunk-wise. Zero strides is assumed to not have this defect.
    """
    ...

def broadcast_batch_dims(f_name, *tensors): # -> None:
    ...

def slicer(dim, slice_range, *tensors): # -> Generator[Any, Any, None]:
    ...

def multidim_slicer(dims, slices, *tensors): # -> Generator[Any, Any, None]:
    ...

def ptr_stride_extractor(*tensors): # -> Generator[Any, Any, None]:
    ...

def grid_partitioner(full_grid, grid_blocks, tensor_dims_map): # -> Generator[tuple[list[Any], *tuple[Any, ...]], Any, None]:
    ...

def launch_kernel(kernel, tensor_dims_map, full_grid, grid_blocks=...): # -> None:
    ...

def prepare_inputs(bsr, *dense_tensors): # -> tuple[Any, Any, Any, *tuple[Any, ...]]:
    ...

def broadcast_batch_dims_bsr(f_name, bsr, *tensors): # -> Tensor:
    ...

def tile_to_blocksize(t, blocksize):
    ...

def as1Dbatch(tensor):
    """Return tensor as 3D tensor by either prepending new dimensions to
    the tensor shape (when ``tensor.ndim < 3``), or by collapsing
    starting dimensions into the first dimension (when ``tensor.ndim >
    3``).
    """
    ...

def scatter_mm(blocks, others, indices_data, *, accumulators=...): # -> Tensor:
    """Scattered matrix multiplication of tensors.

    A scattered matrix multiplication is defined as a series of matrix
    multiplications applied to input tensors according to the input
    and output mappings specified by indices data.

    The following indices data formats are supported for defining a
    scattered matrix multiplication operation (:attr:`indices_data[0]`
    holds the name of the indices data format as specified below):

    - ``"scatter_mm"`` - matrix multiplications scattered in batches
      of tensors.

      If :attr:`blocks` is a :math:`(* \times M \times K) tensor,
      :attr:`others` is a :math:`(* \times K \times N)` tensor,
      :attr:`accumulators` is a :math:`(* \times M \times N)` tensor,
      and :attr:`indices = indices_data['indices']` is a :math:`(*
      \times 3)` tensor, then the operation is equivalent to the
      following code::

        c_offsets, pq = indices_data[1:]
        for r in range(len(c_offsets) - 1):
            for g in range(c_offsets[r], c_offsets[r + 1]):
                p, q = pq[g]
                accumulators[r] += blocks[p] @ others[q]

    - ``"bsr_strided_mm"`` - matrix multiplications scattered in
      batches of tensors and a tensor.

      If :attr:`blocks` is a :math:`(Ms \times Ks) tensor,
      :attr:`others` is a :math:`(* \times K \times N)` tensor,
      :attr:`accumulators` is a :math:`(* \times M \times N)` tensor, then
      the operation is equivalent to the following code::

        c_indices, r_offsets, p_offsets, q_offsets, meta = indices_data[1:]
        for b in range(nbatches):
            for i, r in enumerate(r_offsets):
                r0, r1 = divmod(r, N)
                acc = accumulators[b, r0:r0 + Ms, r1:r1 + Ns]
                for g in range(c_indices[i], c_indices[i+1]):
                    p = p_offsets[g]
                    q0, q1 = divmod(q_offsets[g], N)
                    acc += blocks[p] @ others[b, q0:q0 + Ks, q1:q1 + Ns]

      where ``Ns = N // meta['SPLIT_N']``, and ``M`` and ``K`` are
      integer multiples of ``Ms`` and ``Ks``, respectively.

    - ``"bsr_strided_mm_compressed"`` - matrix multiplications
      scattered in batches of tensors and a tensor. A memory and
      processor efficient version of ``"bsr_strided_mm"`` format.  If
      :attr:`blocks` is a :math:`(Ms \times Ks) tensor, :attr:`others`
      is a :math:`(* \times K \times N)` tensor, :attr:`accumulators`
      is a :math:`(* \times M \times N)` tensor, then the operation is
      equivalent to the following code::

        c_indices, r_offsets, q_offsets, meta = indices_data[1:]
        for b in range(nbatches):
            for r in r_offsets:
                m = (r // N) // Ms
                n = (r % N) // Ns
                r0, r1 = divmod(r, N)
                c0, c1 = c_indices[m], c_indices[m + 1]
                acc = accumulators[b, r0:r0 + Ms, r1:r1 + Ns]
                for i, p in enumerate(range(c0, c1)):
                    q = q_offsets[n * c1 + (SPLIT_N - n) * c0 + i]
                    q0, q1 = divmod(q, N)
                    acc += blocks[p] @ others[b, q0:q0 + Ks, q1:q1 + Ns]

      where ``Ns = N // meta['SPLIT_N']``, and ``M`` and ``K`` are
      integer multiples of ``Ms`` and ``Ks``, respectively.

      Notice that the order of ``r_offsets`` items can be arbitrary;
      this property enables defining swizzle operators via
      rearrangements of ``r_offsets`` items..

    Auxiliary functions are provided for pre-computing
    :attr:`indices_data`. For example,
    :func:`bsr_scatter_mm_indices_data` is used to define indices data
    for matrix multiplication of BSR and strided tensors.

    Parameters
    ----------
    blocks (Tensor): a 3-D tensor of first matrices to be multiplied

    others (Tensor): a tensor of second matrices to be multiplied. If
      ``indices_data[0]=="scatter_mm"``, the tensor is a 1-D batch
      tensor of second input matrices to be multiplied. Otherwise, the
      second input matrices are slices of the :attr:`others` tensor.
    indices_data (tuple): a format data that defines the inputs and
      outputs of scattered matrix multiplications.

    Keyword arguments
    -----------------

    accumulators (Tensor, optional): a tensor of matrix product
      accumulators. If ``indices_data[0]=="scatter_mm"``, the tensor
      is a 1-D batch tensor of output matrices. Otherwise, output
      matrices are slices of the :attr:`accumulators` tensor.
    """
    ...

def scatter_mm_meta(M, K, N, Ms, Ks, GROUP_SIZE=..., TILE_M=..., TILE_N=..., SPLIT_N=..., num_warps=..., num_stages=..., **extra):
    ...

def bsr_dense_addmm_meta(M, K, N, Ms, Ks, beta, alpha, SPLIT_N=..., GROUP_SIZE_ROW=..., num_warps=..., num_stages=..., sparsity=..., dtype=..., out_dtype=..., _version=..., **extra): # -> dict[Any, Any] | dict[Literal['GROUP_SIZE', 'SPLIT_N', 'TILE_M', 'TILE_N', 'num_stages', 'num_warps'], Any] | dict[Literal['GROUP_SIZE_ROW', 'SPLIT_N', 'num_stages', 'num_warps'], Any] | dict[str, Any | int]:
    ...

class TensorAsKey:
    """A light-weight wrapper of a tensor that enables storing tensors as
    keys with efficient memory reference based comparison as an
    approximation to data equality based keys.

    Motivation: the hash value of a torch tensor is tensor instance
    based that does not use data equality and makes the usage of
    tensors as keys less useful. For instance, the result of
    ``len({a.crow_indices(), a.crow_indices()})`` is `2`, although,
    the tensor results from `crow_indices` method call are equal, in
    fact, these share the same data storage.
    On the other hand, for efficient caching of tensors we want to
    avoid calling torch.equal that compares tensors item-wise.

    TensorAsKey offers a compromise in that it guarantees key equality
    of tensors that references data in the same storage in the same
    manner and without accessing underlying data. However, this
    approach does not always guarantee correctness. For instance, for
    a complex tensor ``x``, we have ``TensorAsKey(x) ==
    TensorAsKey(x.conj())`` while ``torch.equal(x, x.conj())`` would
    return False.
    """
    def __init__(self, obj) -> None:
        ...
    
    def __hash__(self) -> int:
        ...
    
    def __eq__(self, other) -> bool:
        ...
    
    @property
    def obj(self): # -> Any | None:
        """Return object if alive, otherwise None."""
        ...
    


def bsr_scatter_mm_indices_data(bsr, other, indices_format=..., **meta_input): # -> tuple[Any | Tensor | Literal['bsr_strided_mm_compressed'], ...] | tuple[Tensor | Any | Literal['bsr_strided_mm'], ...] | tuple[Tensor | Literal['scatter_mm'], ...] | tuple[Literal['bsr_strided_mm_compressed'], Any, Any, Tensor] | tuple[Literal['bsr_strided_mm'], Tensor, Any, Tensor, Tensor] | tuple[Literal['scatter_mm'], Tensor, Tensor]:
    """Computes indices data for :func:`scatter_mm` used in BSR and
    strided tensor matrix multiplication.
    """
    ...

def bsr_scatter_mm(bsr, other, indices_data=..., out=...):
    """BSR @ strided -> strided"""
    ...

def bsr_dense_addmm(input: torch.Tensor, bsr: torch.Tensor, dense: torch.Tensor, *, beta=..., alpha=..., left_alpha: Optional[torch.Tensor] = ..., right_alpha: Optional[torch.Tensor] = ..., out: Optional[torch.Tensor] = ..., skip_checks: bool = ..., max_grid: Optional[tuple[Optional[int], Optional[int], Optional[int]]] = ..., meta: Optional[dict] = ...): # -> Tensor | None:
    """Compute

      out = beta * input + left_alpha.reshape(-1, 1) * (alpha * (bsr @ dense)) * right_alpha.reshape(1, -1)

    where left_alpha, right_alpha are (* + 1)-D tensors when
    specified, otherwise, these are treated as tensors filled with
    ones.
    """
    ...

if has_triton():
    def sampled_addmm(input: torch.Tensor, mat1: torch.Tensor, mat2: torch.Tensor, *, beta=..., alpha=..., out: Optional[torch.Tensor] = ..., skip_checks: bool = ..., max_grid: Optional[tuple[Optional[int], Optional[int], Optional[int]]] = ...): # -> Tensor | None:
        ...
    
    def bsr_dense_mm(bsr: torch.Tensor, dense: torch.Tensor, *, out: Optional[torch.Tensor] = ..., skip_checks: bool = ..., max_grid: Optional[tuple[Optional[int], Optional[int], Optional[int]]] = ..., meta: Optional[dict] = ...): # -> Tensor | None:
        ...
    
    def bsr_softmax(input, max_row_nnz=...): # -> Tensor:
        ...
    
else:
    bsr_softmax = ...
    bsr_dense_mm = ...
    sampled_addmm = ...
    _scaled_dot_product_attention = ...
    _scatter_mm2 = ...
    _scatter_mm6 = ...
    _bsr_strided_addmm_kernel = ...
