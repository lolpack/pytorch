"""
This type stub file was generated by pyright.
"""

import torch
from dataclasses import dataclass
from typing import Optional, Union
from torch._C import DispatchKey
from torch._higher_order_ops.utils import FunctionalizeCtxWrapper, register_fake
from torch._ops import HigherOrderOperator
from torch.fx.experimental.proxy_tensor import ProxyTorchDispatchMode
from torch.fx.graph_module import GraphModule

invoke_subgraph_counter = ...
@dataclass
class OutputMetadata:
    num_fw_outs: Optional[int] = ...
    indexes_with_none: set[int] = ...
    indexes_with_no_grad: set[int] = ...


class InvokeSubgraphHOP(HigherOrderOperator):
    def __init__(self) -> None:
        ...
    
    def __call__(self, subgraph: Union[GraphModule, FunctionalizeCtxWrapper], identifier: Optional[str], *operands): # -> Any | None:
        ...
    
    def gen_schema(self, subgraph, identifier, *operands): # -> FunctionSchema:
        ...
    


invoke_subgraph = ...
def invoke_subgraph_placeholder(func, *args, **kwargs):
    ...

def mark_compile_region(fn=...): # -> Callable[..., Any] | Callable[..., Callable[..., Any]]:
    """
    This wrapper instructs torch.compile to compile the wrapped region once and
    reuse the compiled artifact, instead of the usual way of aggressively
    inlining the function.

    Under the hood, it tells TorchDynamo to use InvokeSubgraph HOP for the
    region. For PyTorch eager, this is a no-op.
    """
    ...

def get_invoke_subgraph_cache(): # -> None:
    ...

def trace_joint_graph(fn, fw_inputs, fw_outputs): # -> GraphModule:
    """
    Naively trace out a joint graph. This simplifies the reconstruction of joint
    graph in the min-cut partitioner later on.
    """
    ...

def create_fw_bw_graph(subgraph, operands, grad_outputs=...): # -> tuple[GraphModule, GraphModule, OutputMetadata]:
    ...

def get_output_metadata(subgraph, *operands): # -> OutputMetadata:
    ...

def trace_joint_graph_as_bwd(subgraph, num_primals, joint_operands, include_key_set, exclude_key_set): # -> GraphModule:
    """
    Naively trace out a joint graph. This simplifies the reconstruction of joint
    graph in the min-cut partitioner later on.
    """
    ...

class InvokeSubgraphAutogradOp(torch.autograd.Function):
    """
    Saves the subgraph, i.e. original callable, in the forward method. And then
    traces out a joint graph in the backward. This delaying of tracing in
    backward, also called as lazy backward, ensures that the assumptions about
    the grad_out strides and tensor-subclass-ness are already accounted for.
    """
    @staticmethod
    def forward(ctx, subgraph, identifier, output_metadata, *operands): # -> Any | None:
        ...
    
    @staticmethod
    def backward(ctx, *grad_outs): # -> tuple[None, None, None, *tuple[Any, ...]]:
        ...
    


@invoke_subgraph.py_autograd_impl
def _(subgraph, identifier, *operands): # -> Any | None:
    ...

@invoke_subgraph.py_impl(DispatchKey.CompositeExplicitAutograd)
def _(subgraph, identifier, *operands):
    ...

@invoke_subgraph.py_functionalize_impl
def _(ctx, subgraph, identifier, *operands): # -> Any:
    ...

@register_fake(invoke_subgraph)
def _(subgraph, identifier, *operands):
    ...

@invoke_subgraph.py_impl(ProxyTorchDispatchMode)
def _(proxy_mode: ProxyTorchDispatchMode, subgraph, identifier, *operands): # -> Any | None:
    ...

