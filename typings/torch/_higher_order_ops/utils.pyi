"""
This type stub file was generated by pyright.
"""

import torch
from dataclasses import dataclass
from typing import Any, Callable, Optional, TypeVar, Union, overload
from torch._higher_order_ops.schema import HopSchema
from torch._ops import HigherOrderOperator, OpOverload, OperatorBase
from torch._subclasses.fake_tensor import FakeTensor
from torch.fx.passes.shape_prop import TensorMetadata

@dataclass
class UnsupportedAliasMutationException(RuntimeError):
    reason: str
    ...


def autograd_not_implemented_inner(operator: OperatorBase, delayed_error: bool, *args: Any, **kwargs: Any) -> Any:
    """If autograd is enabled and any of the arguments require grad this will either
    raise an error or return a DelayedError depending on the value of delayed.

    Args:
        operator: The Operator to call with the *args and **kwargs with
        op_name: The name of the Operator
        delayed_error: If True, return a DelayedError instead of raising an error
        args: The flattened operands to the Operator
        kwargs: The keyword arguments to the Operator

    Raises:
        RuntimeError: If autograd is enabled and any of the arguments to the Operator
    """
    ...

def autograd_not_implemented(op: OperatorBase, deferred_error: bool) -> Callable:
    ...

def reenter_make_fx(fn): # -> _Wrapped[Callable[..., Any], Any, Callable[..., Any], GraphModule]:
    ...

def check_meta_consistency(lhs_list: list[Union[torch.Tensor, torch.SymInt, int]], rhs_list: list[Union[torch.Tensor, torch.SymInt, int]], lhs_name: str, rhs_name: str, include_contiguity: bool = ...) -> None:
    ...

def potential_input_alias_or_mutation(gm, inputs, pre_dispatch=...): # -> tuple[tuple[dict[int, int], dict[int, int], dict[int, int]], list[int]] | Literal[True]:
    ...

def analyze_potential_input_alias_or_mutation(name, aliases, input_mutations): # -> None:
    ...

def has_potential_input_alias_or_mutation(gm, inputs, pre_dispatch=...): # -> tuple[bool, bool]:
    ...

def unique_graph_id(proxy_mode, prefix): # -> tuple[int, str]:
    """Returns a unique name and id for a graph to be added to a proxy_mode tracer"""
    ...

def unique_graph_name_with_root(root: torch.fx.GraphModule, prefix: str) -> tuple[int, str]:
    ...

def clone_outputs_aliasing_inputs(args): # -> Callable[..., Any | Tensor]:
    ...

def prepare_fw_with_masks(fn): # -> Callable[..., tuple[Any, list[bool]]]:
    ...

def prepare_fw_with_masks_all_requires_grad(fn): # -> Callable[..., tuple[PyTree | Any, PyTree]]:
    ...

def unmask_none_gradients(grads, operands): # -> list[Any]:
    ...

def redirect_to_mode(hop: OperatorBase, mode): # -> Callable[..., Any]:
    """Utility for redispatching HOP to underlying mode

    Args:
        hop: The HOP to redispatch
        mode: The mode to redispatch to

    Returns:
        A decorated function that implements the HOP for the given mode
    """
    ...

def create_fw_bw_graph(fn, use_output_and_grad_bw, fw_inputs, fw_outputs): # -> tuple[GraphModule, GraphModule]:
    ...

def save_tensors_and_symints_for_backward(ctx, args): # -> None:
    ...

def saved_tensors_and_symints(ctx): # -> tuple[Any, ...]:
    ...

def get_dummy_aot_autograd_config(): # -> AOTConfig:
    ...

def first_slice_copy(t: torch.Tensor, dim: int = ...) -> torch.Tensor:
    ...

def diff_tensor_meta(meta1: TensorMetadata, meta2: TensorMetadata, check_grad=...) -> list[str]:
    ...

def validate_subgraph_args_types(lifted_args: Union[tuple[Any, ...], list[Any]]): # -> None:
    ...

def check_input_alias_and_mutation(gm: torch.fx.GraphModule, fake_args: list[FakeTensor]) -> tuple[dict[int, int], dict[int, int], dict[int, int], list[int]]:
    ...

def check_input_alias_and_mutation_return_outputs(gm: torch.fx.GraphModule, fake_args: Union[list[FakeTensor], tuple[FakeTensor, ...]]) -> tuple[dict[int, int], dict[int, int], dict[int, int], list[int], Union[tuple[Any, ...], list[Any]],]:
    ...

registered_hop_fake_fns: dict[torch._ops.OpOverload, Callable] = ...
F = TypeVar("F", bound=Callable)
@overload
def register_fake(hop, fn: None = ...) -> Callable[[F], F]:
    ...

@overload
def register_fake(hop, fn: F) -> F:
    ...

def register_fake(hop, fn=...): # -> Callable[..., F]:
    """
    Register a fake function for a HOP. This is conceptually equivalent of the
    register_fake utility for the custom ops. The registered function is called
    inside the fake_tensor _dispatch_impl.
    """
    ...

class FunctionalizeCtxWrapper:
    """
    This is a dummy wrapper to facilitate fake tensor caching.

    For AOT Dispatcher metadata collection pass, HOPs go from functionalization
    key to fake tensor key. The functionalization key wraps the subgraphs in a
    function, which changes from call to call even though the subgraph might
    still be same.

    To enable fake tensor caching, we just wrap the ctx and subgraph in this
    class and then use the subgraph as the hash.
    """
    @torch._disable_dynamo
    def __init__(self, ctx, subgraph) -> None:
        ...
    
    def __hash__(self) -> int:
        ...
    
    def __repr__(self): # -> str:
        ...
    
    def __call__(self, *args, **kwargs):
        ...
    


class HopInstance:
    def __init__(self, op: HigherOrderOperator, schema: HopSchema) -> None:
        ...
    
    def __call__(self, *args, **kwargs): # -> Any | None:
        ...
    
    @staticmethod
    def create(hop: HigherOrderOperator, *args, **kwargs): # -> HopInstance:
        ...
    


def call_op(op: Union[OpOverload, HopInstance], args, kwargs): # -> Any | None:
    ...

def materialize_as_graph(fn: Callable, args: tuple[Any], include_key_set: Optional[torch._C.DispatchKeySet] = ..., exclude_key_set: Optional[torch._C.DispatchKeySet] = ..., force_enable_grad=...) -> torch.fx.GraphModule:
    ...

def materialize_callable_in_args(op: HopInstance, args, kwargs): # -> PyTree:
    ...

def has_user_subclass(args, allowed_subclasses): # -> bool:
    """Check if any tensor arguments are user subclasses.

    This is used to determine if tensor subclasses should get a chance to run
    their own implementation first before falling back to the default implementation.

    Args:
        args: Arguments to check (will be flattened with pytree)
        allowed_subclasses: Tuple of allowed subclass types

    Returns:
        True if user tensor subclasses are found, False otherwise
    """
    ...

