"""
This type stub file was generated by pyright.
"""

import torch
import torch.utils._pytree as pytree
from typing import Callable, Union
from typing_extensions import TypeVarTuple
from torch._C import DispatchKey
from torch._ops import HigherOrderOperator
from torch._subclasses.fake_tensor import FakeTensorMode
from torch.fx.experimental.proxy_tensor import ProxyTorchDispatchMode

class MapImpl(HigherOrderOperator):
    def __init__(self) -> None:
        ...
    
    def __call__(self, *args, **kwargs): # -> Any | None:
        ...
    


map_impl = ...
def create_fw_bw_graph(f, num_mapped_args, *args): # -> tuple[GraphModule, GraphModule]:
    ...

def map(f: Callable[[pytree.PyTree, tuple[pytree.PyTree, ...]], pytree.PyTree], xs: Union[pytree.PyTree, torch.Tensor], *args: TypeVarTuple): # -> Any | None:
    r"""
    Perfoms a map of f with xs. Intuitively, you can think of the semantic being:

    out = []
    for idx in len(xs.size(0)):
        xs_sliced = xs.select(0, idx)
        out.append(f(xs_sliced, *args))
    torch.stack(out)

    .. warning::
        `torch._higher_order_ops.map` is a prototype feature in PyTorch. It currently
        does not support autograd and you may run into miscompiles.
        Read more about feature classification at:
        https://pytorch.org/blog/pytorch-feature-classification-changes/#prototype


    Args:
        f (Callable): a callable that takes an input x, that could either be a single Tensor
            or a nested dict, list of tensors and some additional inputs
        xs: the inputs that're to be mapped over. We'll iterate over the first dim of each x
            and perform f on each slice.

        *args: additional arguments provided to each step of f. They could also be omitted and
            map is able to automatically figure out the read dependency.

    Return:
        the stacked output for each step of f

    Example:

        def f(xs):
            return xs[0] + xs[1] + const1 + const2

        xs = [torch.randn(2, 3), torch.randn(2, 3)]
        const1 = torch.randn(2, 3)
        const2 = torch.randn(2, 3)
        # returns a tensor of shape [2, 2, 3]
        torch._higher_order_ops.map(f, xs)

    """
    ...

class MapAutogradOp(torch.autograd.Function):
    @staticmethod
    def forward(ctx, fw_graph, joint_graph, num_mapped_args, *flat_args): # -> tuple[Any, ...]:
        ...
    
    @staticmethod
    def backward(ctx, *flat_grads): # -> tuple[None, None, None, *tuple[Any, ...]]:
        ...
    


def trace_map(proxy_mode, func_overload, f, xs, pos_args): # -> Any | None:
    ...

@map_impl.py_impl(DispatchKey.CompositeExplicitAutograd)
def map_dense(f, xs, pos_args): # -> PyTree:
    ...

@map_impl.py_autograd_impl
def map_autograd(f, xs, pos_args): # -> Any | None:
    ...

@map_impl.py_impl(ProxyTorchDispatchMode)
def map_proxy_torch_dispatch_mode(mode, f, xs, args): # -> Any | None:
    ...

@map_impl.py_impl(FakeTensorMode)
def map_fake_tensor_mode(mode, f, xs, args): # -> PyTree:
    ...

@map_impl.py_functionalize_impl
def map_functionalize(ctx, f, xs, pos_args):
    ...

