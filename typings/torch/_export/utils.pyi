"""
This type stub file was generated by pyright.
"""

import torch
from inspect import Parameter
from typing import Any, Callable, Optional, TYPE_CHECKING, Union
from torch.export import ExportedProgram
from torch.export.graph_signature import ExportGraphSignature
from torch.utils._pytree import FlattenFunc, FromDumpableContextFn, KeyPath, ToDumpableContextFn, UnflattenFunc

if TYPE_CHECKING:
    ...
placeholder_prefixes = ...
_DISABLE_ATEN_TO_ASSERTION_PASS = ...
def get_keystr(key_path: KeyPath) -> str:
    """For a given index into the flat_args, return a human readable string
    describing how to access it, e.g. "*args["foo"][0].bar"
    """
    ...

def register_dataclass_as_pytree_node(cls: type[Any], flatten_fn: Optional[FlattenFunc] = ..., unflatten_fn: Optional[UnflattenFunc] = ..., *, serialized_type_name: Optional[str] = ..., to_dumpable_context: Optional[ToDumpableContextFn] = ..., from_dumpable_context: Optional[FromDumpableContextFn] = ..., return_none_fields: bool = ...) -> None:
    ...

def is_param(program: ExportedProgram, node: torch.fx.Node) -> bool:
    """
    Checks if the given node is a parameter within the exported program
    """
    ...

def get_param(program: ExportedProgram, node: torch.fx.Node) -> Optional[torch.nn.Parameter]:
    """
    Returns the parameter associated with the given node in the exported program.
    Returns None if the node is not a parameter within the exported program
    """
    ...

def is_buffer(program: ExportedProgram, node: torch.fx.Node) -> bool:
    """
    Checks if the given node is a buffer within the exported program
    """
    ...

def get_buffer(program: ExportedProgram, node: torch.fx.Node) -> Optional[torch.Tensor]:
    """
    Returns the buffer associated with the given node in the exported program.
    Returns None if the node is not a buffer within the exported program
    """
    ...

def is_lifted_tensor_constant(program: ExportedProgram, node: torch.fx.Node) -> bool:
    """
    Checks if the given node is a lifted tensor constant within the exported program
    """
    ...

def get_lifted_tensor_constant(program: ExportedProgram, node: torch.fx.Node) -> Optional[torch.Tensor]:
    """
    Returns the lifted tensor constant associated with the given node in the exported program.
    Returns None if the node is not a lifted tensor constant within the exported program
    """
    ...

def sequential_split(gm: torch.fx.GraphModule, node_call_back: Callable[[torch.fx.Node], Union[torch.fx.Node, bool]]) -> torch.fx.GraphModule:
    """
    sequential_split creates a new graph module that splits the input graph module into multiple submodules
    based on the node_call_back. It doesn't mutate the input graph module. The node_call_back should return
    True if the node is a delimiter.  Delimiter will be the first node in the next submodule.
    """
    ...

def nodes_filter(nodes: list[torch.fx.Node], node_call_back) -> list[torch.fx.Node]:
    """Returns the nodes that match the node_call_back as a list."""
    ...

def apply_runtime_assertion_pass(gm: torch.fx.GraphModule, graph_signature): # -> tuple[GraphModule, Any]:
    ...

def nodes_first(nodes: list[torch.fx.Node], node_call_back=...) -> Optional[torch.fx.Node]:
    """
    Returns the first node that matches the node_call_back. If no node matches, returns None.
    When node_call_back is None, returns the first node in the node list.
    """
    ...

def nodes_count(nodes: list[torch.fx.Node], node_call_back) -> int:
    """Returns the number of nodes that match the node_call_back."""
    ...

def nodes_map(nodes: list[torch.fx.Node], node_call_back) -> list[torch.fx.Node]:
    """
    Sequentially visit the nodes list and invoke node_call_back on each element.
    Returns the nodes list after the node_call_back is invoked on each element.
    """
    ...

def node_replace_(old_node: torch.fx.Node, new_node: torch.fx.Node) -> None:
    """
    Replace all uses of old_node with new_node.
    """
    ...

def node_inline_(call_mod_node: torch.fx.Node) -> Optional[torch.fx.GraphModule]:
    """
    Inline the submodule of the given node into the parent module.
    Note: we only support the case where submodule takes tensors inputs.
    """
    ...

def placeholder_naming_pass(gm: torch.fx.GraphModule, export_graph_signature: ExportGraphSignature, mod: torch.nn.Module, fake_args, fake_kwargs, fake_params_buffers, constants: dict[str, Any]) -> None:
    """
    This pass is run at the end of _export_non_strict() to assign better placeholder node names:
        - User inputs:
            These follow the signature of mod.forward(), e.g. forward(x, y) produces nodes x, y.
            For nested inputs from dictionaries, lists, tuples, or dataclasses,
            the names are a concatenation of the path to the tensor.
                e.g. x = {
                    'a': torch.randn(),
                    'b': [torch.randn(), torch.randn()]
                }
            produces nodes x_a, x_b_0, x_b_1.
        - Parameters/buffers/constants/custom objects:
            These follow the FQN of the object, prefixed by "p", "b", "c", "obj" respectively.
                e.g. self.bar.l0.weight produces "p_bar_l0_weight".
        - Effect tokens:
            These are named token, token_1, ...
    """
    ...

def remove_proxy_from_state_dict(state_dict: dict, in_place: bool) -> dict:
    """
    If `in_place` is false, return a new copy of `state_dict` with "proxy" removed from `v.__dict__`.
    `v` is the values in the dictionary.
    If `in_place` is true, modify `state_dict` in place.
    """
    ...

def register_module_as_pytree_input_node(cls: type[torch.nn.Module]) -> None:
    """
    Registers a module as a valid input type for :func:`torch.export.export`.

    Args:
        mod: the module instance
        serialized_type_name: The serialized name for the module. This is
        required if you want to serialize the pytree TreeSpec containing this
        module.

    Example::

        import torch


        class Module(torch.nn.Module):
            def __init__(self):
                super().__init__()
                self.linear = torch.nn.Linear(3, 3)

            def forward(self, x):
                return self.linear(x)


        torch._export.utils.register_module_as_pytree_node(InputDataClass)


        class Mod(torch.nn.Module):
            def forward(self, x, m):
                return m(x) + x


        ep = torch.export.export(Mod(), (torch.randn(3), Module()))
        print(ep)

    """
    class PrototypeModule(weakref.ref):
        ...
    
    

def deregister_module_as_pytree_input_node(cls: type[torch.nn.Module]) -> None:
    ...

def sync_state(*wrapped_method_modules): # -> None:
    """
    Sync state between exported modules corresponding to wrapped methods.
    This might be necessary after serializing/deserializing due to copying.
    """
    ...

class _WrappedMethod(torch.nn.Module):
    def __init__(self, method) -> None:
        ...
    


def wrap_method(method): # -> _WrappedMethod:
    """
    Wrap a method as a module so that it can be exported.
    The wrapped module's forward points to the method, and
    the method's original module state is shared.
    """
    ...

